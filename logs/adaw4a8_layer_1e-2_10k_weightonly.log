
- main_args params:
    - arch: ViT_B_16
    - batch_size: 128
    - num_samples: 1024

- weight params:
    - scheme: AbsMaxQuantizer
    - bit_width: 4
    - per_channel: True
    - AdaRound: PerLayer

- activation params:
    - scheme: MovAvgAbsMaxQuantizer
    - bit_width: 8
    - per_channel: False
    - momentum: 0.95
    - batches: 16
    - Identity addition : INT16 (The input of each LayerNorm)

- softmax params:
    - bit_width: 16
    - Activation of Softmax(Q@K/d_K) (attn_map) : UINT8

- layer_norm params:
    - bit_width: 8

- gelu params:
    - bit_width: 8

Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
    Initiated the V
Activation calibration is done.

[1/50] conv_proj
    INPUT_FP : torch.Size([1024, 3, 224, 224])
    OUTPUT_FP : torch.Size([1024, 768, 14, 14])
    V   : , torch.Size([768, 3, 16, 16])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 119101.3672 (MSE:0.0005, Reg:119101.3672) beta=20.00
Iter  1000 | Total loss: 16.0032 (MSE:0.0032, Reg:16.0000) beta=19.05
Iter  2000 | Total loss: 7.0030 (MSE:0.0030, Reg:7.0000) beta=17.16
Iter  3000 | Total loss: 5.9877 (MSE:0.0032, Reg:5.9844) beta=15.26
Iter  4000 | Total loss: 2.0037 (MSE:0.0037, Reg:2.0000) beta=13.37
Iter  4906 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=11.65
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0539, device='cuda:0', requires_grad=True)

[2/50] encoder.layers.0.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0109 (MSE:0.0109, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 259165.7188 (MSE:0.0023, Reg:259165.7188) beta=20.00
Iter  1000 | Total loss: 18580.5020 (MSE:0.0024, Reg:18580.5000) beta=19.05
Iter  2000 | Total loss: 8297.9756 (MSE:0.0022, Reg:8297.9736) beta=17.16
Iter  3000 | Total loss: 4888.5576 (MSE:0.0021, Reg:4888.5557) beta=15.26
Iter  4000 | Total loss: 2983.8943 (MSE:0.0023, Reg:2983.8921) beta=13.37
Iter  5000 | Total loss: 1578.2844 (MSE:0.0027, Reg:1578.2817) beta=11.47
Iter  6000 | Total loss: 728.3537 (MSE:0.0022, Reg:728.3514) beta=9.58
Iter  7000 | Total loss: 175.3537 (MSE:0.0023, Reg:175.3514) beta=7.68
Iter  8000 | Total loss: 8.8292 (MSE:0.0026, Reg:8.8266) beta=5.79
Iter  8548 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=4.75
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0258, device='cuda:0', requires_grad=True)

[3/50] encoder.layers.0.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 50016.1094 (MSE:0.0001, Reg:50016.1094) beta=20.00
Iter  1000 | Total loss: 181.0363 (MSE:0.0001, Reg:181.0362) beta=19.05
Iter  2000 | Total loss: 68.9944 (MSE:0.0001, Reg:68.9943) beta=17.16
Iter  3000 | Total loss: 28.0001 (MSE:0.0001, Reg:28.0000) beta=15.26
Iter  4000 | Total loss: 19.0001 (MSE:0.0001, Reg:19.0000) beta=13.37
Iter  5000 | Total loss: 8.0001 (MSE:0.0001, Reg:8.0000) beta=11.47
Iter  6000 | Total loss: 4.0001 (MSE:0.0001, Reg:4.0000) beta=9.58
Iter  7000 | Total loss: 1.0001 (MSE:0.0001, Reg:1.0000) beta=7.68
Iter  7400 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=6.93
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0125, device='cuda:0', requires_grad=True)

[4/50] encoder.layers.0.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0279 (MSE:0.0279, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 425035.4375 (MSE:0.0064, Reg:425035.4375) beta=20.00
Iter  1000 | Total loss: 15097.2471 (MSE:0.0067, Reg:15097.2402) beta=19.05
Iter  2000 | Total loss: 7254.8076 (MSE:0.0074, Reg:7254.8003) beta=17.16
Iter  3000 | Total loss: 5031.1807 (MSE:0.0070, Reg:5031.1738) beta=15.26
Iter  4000 | Total loss: 3470.8604 (MSE:0.0069, Reg:3470.8535) beta=13.37
Iter  5000 | Total loss: 2007.7109 (MSE:0.0078, Reg:2007.7031) beta=11.47
Iter  6000 | Total loss: 926.2044 (MSE:0.0066, Reg:926.1978) beta=9.58
Iter  7000 | Total loss: 239.8837 (MSE:0.0071, Reg:239.8767) beta=7.68
Iter  8000 | Total loss: 10.9462 (MSE:0.0071, Reg:10.9390) beta=5.79
Iter  8274 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=5.27
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0633, device='cuda:0', requires_grad=True)

[5/50] encoder.layers.0.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 418414.8750 (MSE:0.0020, Reg:418414.8750) beta=20.00
Iter  1000 | Total loss: 5139.8804 (MSE:0.0025, Reg:5139.8779) beta=19.05
Iter  2000 | Total loss: 2315.7642 (MSE:0.0024, Reg:2315.7617) beta=17.16
Iter  3000 | Total loss: 1568.6469 (MSE:0.0026, Reg:1568.6443) beta=15.26
Iter  4000 | Total loss: 1037.8785 (MSE:0.0026, Reg:1037.8759) beta=13.37
Iter  5000 | Total loss: 583.6442 (MSE:0.0026, Reg:583.6416) beta=11.47
Iter  6000 | Total loss: 260.6917 (MSE:0.0024, Reg:260.6892) beta=9.58
Iter  7000 | Total loss: 46.7805 (MSE:0.0025, Reg:46.7780) beta=7.68
Iter  7776 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=6.21
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0318, device='cuda:0', requires_grad=True)

[6/50] encoder.layers.1.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0076 (MSE:0.0076, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 355522.0000 (MSE:0.0045, Reg:355522.0000) beta=20.00
Iter  1000 | Total loss: 16352.2246 (MSE:0.0051, Reg:16352.2197) beta=19.05
Iter  2000 | Total loss: 6726.1353 (MSE:0.0050, Reg:6726.1304) beta=17.16
Iter  3000 | Total loss: 4390.0239 (MSE:0.0046, Reg:4390.0195) beta=15.26
Iter  4000 | Total loss: 2862.2681 (MSE:0.0045, Reg:2862.2637) beta=13.37
Iter  5000 | Total loss: 1622.7654 (MSE:0.0043, Reg:1622.7611) beta=11.47
Iter  6000 | Total loss: 685.7731 (MSE:0.0046, Reg:685.7686) beta=9.58
Iter  7000 | Total loss: 176.8817 (MSE:0.0048, Reg:176.8769) beta=7.68
Iter  7939 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=5.91
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0307, device='cuda:0', requires_grad=True)

[7/50] encoder.layers.1.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 71479.9688 (MSE:0.0003, Reg:71479.9688) beta=20.00
Iter  1000 | Total loss: 111.5258 (MSE:0.0003, Reg:111.5255) beta=19.05
Iter  2000 | Total loss: 24.7630 (MSE:0.0003, Reg:24.7627) beta=17.16
Iter  3000 | Total loss: 17.0003 (MSE:0.0003, Reg:17.0000) beta=15.26
Iter  4000 | Total loss: 10.0003 (MSE:0.0003, Reg:10.0000) beta=13.37
Iter  5000 | Total loss: 6.0003 (MSE:0.0003, Reg:6.0000) beta=11.47
Iter  6000 | Total loss: 1.0003 (MSE:0.0003, Reg:1.0000) beta=9.58
Iter  6623 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=8.40
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0084, device='cuda:0', requires_grad=True)

[8/50] encoder.layers.1.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0568 (MSE:0.0568, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 767255.1250 (MSE:0.0177, Reg:767255.1250) beta=20.00
Iter  1000 | Total loss: 39968.4648 (MSE:0.0201, Reg:39968.4453) beta=19.05
Iter  2000 | Total loss: 16258.1709 (MSE:0.0206, Reg:16258.1504) beta=17.16
Iter  3000 | Total loss: 10250.0215 (MSE:0.0206, Reg:10250.0010) beta=15.26
Iter  4000 | Total loss: 6500.9062 (MSE:0.0186, Reg:6500.8877) beta=13.37
Iter  5000 | Total loss: 3621.0176 (MSE:0.0192, Reg:3620.9985) beta=11.47
Iter  6000 | Total loss: 1522.0006 (MSE:0.0190, Reg:1521.9816) beta=9.58
Iter  7000 | Total loss: 207.8140 (MSE:0.0195, Reg:207.7945) beta=7.68
Iter  7898 | Total loss: 0.0203 (MSE:0.0203, Reg:0.0000) beta=5.98
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0666, device='cuda:0', requires_grad=True)

[9/50] encoder.layers.1.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 356692.0625 (MSE:0.0021, Reg:356692.0625) beta=20.00
Iter  1000 | Total loss: 540.1077 (MSE:0.0029, Reg:540.1049) beta=19.05
Iter  2000 | Total loss: 210.0027 (MSE:0.0030, Reg:209.9997) beta=17.16
Iter  3000 | Total loss: 124.9972 (MSE:0.0028, Reg:124.9943) beta=15.26
Iter  4000 | Total loss: 77.0039 (MSE:0.0039, Reg:77.0000) beta=13.37
Iter  5000 | Total loss: 41.0030 (MSE:0.0030, Reg:41.0000) beta=11.47
Iter  6000 | Total loss: 11.9199 (MSE:0.0031, Reg:11.9168) beta=9.58
Iter  7000 | Total loss: 3.0031 (MSE:0.0031, Reg:3.0000) beta=7.68
Iter  7438 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=6.85
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0215, device='cuda:0', requires_grad=True)

[10/50] encoder.layers.2.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0219 (MSE:0.0219, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 446376.6250 (MSE:0.0210, Reg:446376.5938) beta=20.00
Iter  1000 | Total loss: 30494.7129 (MSE:0.0126, Reg:30494.7012) beta=19.05
Iter  2000 | Total loss: 11591.8203 (MSE:0.0135, Reg:11591.8066) beta=17.16
Iter  3000 | Total loss: 6880.7852 (MSE:0.0127, Reg:6880.7725) beta=15.26
Iter  4000 | Total loss: 4314.3452 (MSE:0.0123, Reg:4314.3330) beta=13.37
Iter  5000 | Total loss: 2471.1123 (MSE:0.0127, Reg:2471.0996) beta=11.47
Iter  6000 | Total loss: 1055.6145 (MSE:0.0142, Reg:1055.6002) beta=9.58
Iter  7000 | Total loss: 181.9030 (MSE:0.0131, Reg:181.8899) beta=7.68
Iter  8000 | Total loss: 3.0144 (MSE:0.0144, Reg:3.0000) beta=5.79
Iter  8084 | Total loss: 0.0197 (MSE:0.0197, Reg:0.0000) beta=5.63
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0445, device='cuda:0', requires_grad=True)

[11/50] encoder.layers.2.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 103046.8516 (MSE:0.0010, Reg:103046.8516) beta=20.00
Iter  1000 | Total loss: 423.7682 (MSE:0.0011, Reg:423.7671) beta=19.05
Iter  2000 | Total loss: 178.6918 (MSE:0.0013, Reg:178.6905) beta=17.16
Iter  3000 | Total loss: 114.9922 (MSE:0.0011, Reg:114.9911) beta=15.26
Iter  4000 | Total loss: 81.0011 (MSE:0.0011, Reg:81.0000) beta=13.37
Iter  5000 | Total loss: 54.6741 (MSE:0.0014, Reg:54.6727) beta=11.47
Iter  6000 | Total loss: 25.7513 (MSE:0.0011, Reg:25.7503) beta=9.58
Iter  7000 | Total loss: 5.0002 (MSE:0.0012, Reg:4.9991) beta=7.68
Iter  7391 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=6.94
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0088, device='cuda:0', requires_grad=True)

[12/50] encoder.layers.2.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0497 (MSE:0.0497, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 758793.5000 (MSE:0.0237, Reg:758793.5000) beta=20.00
Iter  1000 | Total loss: 49780.6328 (MSE:0.0277, Reg:49780.6055) beta=19.05
Iter  2000 | Total loss: 28395.6094 (MSE:0.0267, Reg:28395.5820) beta=17.16
Iter  3000 | Total loss: 21189.6387 (MSE:0.0257, Reg:21189.6133) beta=15.26
Iter  4000 | Total loss: 15391.8467 (MSE:0.0285, Reg:15391.8184) beta=13.37
Iter  5000 | Total loss: 9886.3213 (MSE:0.0248, Reg:9886.2969) beta=11.47
Iter  6000 | Total loss: 5325.9233 (MSE:0.0279, Reg:5325.8955) beta=9.58
Iter  7000 | Total loss: 1782.0632 (MSE:0.0283, Reg:1782.0349) beta=7.68
Iter  8000 | Total loss: 1.0255 (MSE:0.0255, Reg:1.0000) beta=5.79
Iter  8178 | Total loss: 0.0283 (MSE:0.0283, Reg:0.0000) beta=5.45
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0790, device='cuda:0', requires_grad=True)

[13/50] encoder.layers.2.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 384964.6250 (MSE:0.0045, Reg:384964.6250) beta=20.00
Iter  1000 | Total loss: 695.2188 (MSE:0.0057, Reg:695.2131) beta=19.05
Iter  2000 | Total loss: 241.2854 (MSE:0.0053, Reg:241.2801) beta=17.16
Iter  3000 | Total loss: 148.4223 (MSE:0.0052, Reg:148.4172) beta=15.26
Iter  4000 | Total loss: 92.9403 (MSE:0.0049, Reg:92.9354) beta=13.37
Iter  5000 | Total loss: 52.5232 (MSE:0.0053, Reg:52.5178) beta=11.47
Iter  6000 | Total loss: 15.8780 (MSE:0.0050, Reg:15.8730) beta=9.58
Iter  7000 | Total loss: 4.0052 (MSE:0.0052, Reg:4.0000) beta=7.68
Iter  7356 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=7.01
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0234, device='cuda:0', requires_grad=True)

[14/50] encoder.layers.3.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0336 (MSE:0.0336, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 486058.8125 (MSE:0.0246, Reg:486058.7812) beta=20.00
Iter  1000 | Total loss: 41496.9961 (MSE:0.0196, Reg:41496.9766) beta=19.05
Iter  2000 | Total loss: 14866.7646 (MSE:0.0208, Reg:14866.7441) beta=17.16
Iter  3000 | Total loss: 8385.4678 (MSE:0.0221, Reg:8385.4453) beta=15.26
Iter  4000 | Total loss: 5004.0767 (MSE:0.0192, Reg:5004.0576) beta=13.37
Iter  5000 | Total loss: 2670.7673 (MSE:0.0210, Reg:2670.7463) beta=11.47
Iter  6000 | Total loss: 1077.0092 (MSE:0.0208, Reg:1076.9884) beta=9.58
Iter  7000 | Total loss: 179.6433 (MSE:0.0202, Reg:179.6231) beta=7.68
Iter  7864 | Total loss: 0.0208 (MSE:0.0208, Reg:0.0000) beta=6.05
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0453, device='cuda:0', requires_grad=True)

[15/50] encoder.layers.3.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 124883.2969 (MSE:0.0017, Reg:124883.2969) beta=20.00
Iter  1000 | Total loss: 420.2603 (MSE:0.0018, Reg:420.2585) beta=19.05
Iter  2000 | Total loss: 169.9687 (MSE:0.0016, Reg:169.9671) beta=17.16
Iter  3000 | Total loss: 125.0016 (MSE:0.0016, Reg:125.0000) beta=15.26
Iter  4000 | Total loss: 91.0017 (MSE:0.0017, Reg:91.0000) beta=13.37
Iter  5000 | Total loss: 58.8209 (MSE:0.0021, Reg:58.8187) beta=11.47
Iter  6000 | Total loss: 33.0012 (MSE:0.0017, Reg:32.9995) beta=9.58
Iter  7000 | Total loss: 2.0017 (MSE:0.0017, Reg:2.0000) beta=7.68
Iter  7337 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=7.05
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0113, device='cuda:0', requires_grad=True)

[16/50] encoder.layers.3.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0684 (MSE:0.0684, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 701957.3750 (MSE:0.0429, Reg:701957.3125) beta=20.00
Iter  1000 | Total loss: 45045.3164 (MSE:0.0449, Reg:45045.2734) beta=19.05
Iter  2000 | Total loss: 20119.8672 (MSE:0.0429, Reg:20119.8242) beta=17.16
Iter  3000 | Total loss: 12998.4277 (MSE:0.0411, Reg:12998.3867) beta=15.26
Iter  4000 | Total loss: 8488.0586 (MSE:0.0431, Reg:8488.0156) beta=13.37
Iter  5000 | Total loss: 4579.2837 (MSE:0.0433, Reg:4579.2402) beta=11.47
Iter  6000 | Total loss: 1650.5376 (MSE:0.0480, Reg:1650.4896) beta=9.58
Iter  7000 | Total loss: 138.6874 (MSE:0.0429, Reg:138.6445) beta=7.68
Iter  7628 | Total loss: 0.0408 (MSE:0.0408, Reg:0.0000) beta=6.49
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0613, device='cuda:0', requires_grad=True)

[17/50] encoder.layers.3.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 353202.8125 (MSE:0.0042, Reg:353202.8125) beta=20.00
Iter  1000 | Total loss: 442.8550 (MSE:0.0048, Reg:442.8502) beta=19.05
Iter  2000 | Total loss: 163.0043 (MSE:0.0044, Reg:162.9999) beta=17.16
Iter  3000 | Total loss: 107.0046 (MSE:0.0046, Reg:107.0000) beta=15.26
Iter  4000 | Total loss: 76.0040 (MSE:0.0040, Reg:76.0000) beta=13.37
Iter  5000 | Total loss: 49.0333 (MSE:0.0043, Reg:49.0290) beta=11.47
Iter  6000 | Total loss: 16.8963 (MSE:0.0044, Reg:16.8920) beta=9.58
Iter  6829 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=8.01
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0128, device='cuda:0', requires_grad=True)

[18/50] encoder.layers.4.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0546 (MSE:0.0546, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 495267.5312 (MSE:0.0346, Reg:495267.5000) beta=20.00
Iter  1000 | Total loss: 53537.8125 (MSE:0.0404, Reg:53537.7734) beta=19.05
Iter  2000 | Total loss: 21818.7480 (MSE:0.0342, Reg:21818.7129) beta=17.16
Iter  3000 | Total loss: 13049.9463 (MSE:0.0312, Reg:13049.9150) beta=15.26
Iter  4000 | Total loss: 7684.6333 (MSE:0.0338, Reg:7684.5996) beta=13.37
Iter  5000 | Total loss: 3987.3428 (MSE:0.0342, Reg:3987.3086) beta=11.47
Iter  6000 | Total loss: 1511.2850 (MSE:0.0395, Reg:1511.2456) beta=9.58
Iter  7000 | Total loss: 205.8819 (MSE:0.0353, Reg:205.8466) beta=7.68
Iter  7880 | Total loss: 0.0345 (MSE:0.0345, Reg:0.0000) beta=6.02
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0447, device='cuda:0', requires_grad=True)

[19/50] encoder.layers.4.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 128662.9531 (MSE:0.0024, Reg:128662.9531) beta=20.00
Iter  1000 | Total loss: 141.0220 (MSE:0.0024, Reg:141.0196) beta=19.05
Iter  2000 | Total loss: 54.9929 (MSE:0.0024, Reg:54.9905) beta=17.16
Iter  3000 | Total loss: 33.0022 (MSE:0.0022, Reg:33.0000) beta=15.26
Iter  4000 | Total loss: 18.0023 (MSE:0.0023, Reg:18.0000) beta=13.37
Iter  5000 | Total loss: 9.7698 (MSE:0.0027, Reg:9.7671) beta=11.47
Iter  6000 | Total loss: 5.0025 (MSE:0.0025, Reg:5.0000) beta=9.58
Iter  6756 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=8.15
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0112, device='cuda:0', requires_grad=True)

[20/50] encoder.layers.4.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0705 (MSE:0.0705, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 643861.1250 (MSE:0.0463, Reg:643861.0625) beta=20.00
Iter  1000 | Total loss: 48181.7852 (MSE:0.0533, Reg:48181.7305) beta=19.05
Iter  2000 | Total loss: 22392.7266 (MSE:0.0546, Reg:22392.6719) beta=17.16
Iter  3000 | Total loss: 14695.7314 (MSE:0.0552, Reg:14695.6758) beta=15.26
Iter  4000 | Total loss: 9432.6406 (MSE:0.0513, Reg:9432.5898) beta=13.37
Iter  5000 | Total loss: 5128.5781 (MSE:0.0473, Reg:5128.5308) beta=11.47
Iter  6000 | Total loss: 1594.5394 (MSE:0.0554, Reg:1594.4840) beta=9.58
Iter  7000 | Total loss: 108.9326 (MSE:0.0602, Reg:108.8724) beta=7.68
Iter  7534 | Total loss: 0.0502 (MSE:0.0502, Reg:0.0000) beta=6.67
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0533, device='cuda:0', requires_grad=True)

[21/50] encoder.layers.4.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0086 (MSE:0.0086, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 411172.9375 (MSE:0.0061, Reg:411172.9375) beta=20.00
Iter  1000 | Total loss: 235.7099 (MSE:0.0060, Reg:235.7040) beta=19.05
Iter  2000 | Total loss: 91.0071 (MSE:0.0083, Reg:90.9989) beta=17.16
Iter  3000 | Total loss: 56.7860 (MSE:0.0067, Reg:56.7794) beta=15.26
Iter  4000 | Total loss: 29.0064 (MSE:0.0064, Reg:29.0000) beta=13.37
Iter  5000 | Total loss: 16.9247 (MSE:0.0074, Reg:16.9173) beta=11.47
Iter  6000 | Total loss: 8.6790 (MSE:0.0082, Reg:8.6708) beta=9.58
Iter  6850 | Total loss: 0.0076 (MSE:0.0076, Reg:0.0000) beta=7.97
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0628, device='cuda:0', requires_grad=True)

[22/50] encoder.layers.5.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0541 (MSE:0.0541, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 517052.0312 (MSE:0.0365, Reg:517052.0000) beta=20.00
Iter  1000 | Total loss: 53816.4414 (MSE:0.0352, Reg:53816.4062) beta=19.05
Iter  2000 | Total loss: 22735.6191 (MSE:0.0416, Reg:22735.5781) beta=17.16
Iter  3000 | Total loss: 13699.4541 (MSE:0.0401, Reg:13699.4141) beta=15.26
Iter  4000 | Total loss: 8304.2402 (MSE:0.0394, Reg:8304.2012) beta=13.37
Iter  5000 | Total loss: 4144.3604 (MSE:0.0357, Reg:4144.3247) beta=11.47
Iter  6000 | Total loss: 1421.0227 (MSE:0.0342, Reg:1420.9885) beta=9.58
Iter  7000 | Total loss: 132.2391 (MSE:0.0384, Reg:132.2007) beta=7.68
Iter  7624 | Total loss: 0.0410 (MSE:0.0410, Reg:0.0000) beta=6.50
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0408, device='cuda:0', requires_grad=True)

[23/50] encoder.layers.5.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 151134.7500 (MSE:0.0027, Reg:151134.7500) beta=20.00
Iter  1000 | Total loss: 612.9380 (MSE:0.0030, Reg:612.9350) beta=19.05
Iter  2000 | Total loss: 172.1874 (MSE:0.0027, Reg:172.1846) beta=17.16
Iter  3000 | Total loss: 95.0014 (MSE:0.0029, Reg:94.9985) beta=15.26
Iter  4000 | Total loss: 57.0027 (MSE:0.0027, Reg:57.0000) beta=13.37
Iter  5000 | Total loss: 20.0029 (MSE:0.0029, Reg:20.0000) beta=11.47
Iter  6000 | Total loss: 6.3655 (MSE:0.0034, Reg:6.3621) beta=9.58
Iter  6751 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=8.16
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0117, device='cuda:0', requires_grad=True)

[24/50] encoder.layers.5.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1106 (MSE:0.1106, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 711298.1250 (MSE:0.0674, Reg:711298.0625) beta=20.00
Iter  1000 | Total loss: 49092.5000 (MSE:0.0702, Reg:49092.4297) beta=19.05
Iter  2000 | Total loss: 22380.8555 (MSE:0.0673, Reg:22380.7891) beta=17.16
Iter  3000 | Total loss: 14249.3740 (MSE:0.0614, Reg:14249.3125) beta=15.26
Iter  4000 | Total loss: 8874.8740 (MSE:0.0660, Reg:8874.8076) beta=13.37
Iter  5000 | Total loss: 4632.8584 (MSE:0.0671, Reg:4632.7910) beta=11.47
Iter  6000 | Total loss: 1407.2761 (MSE:0.0622, Reg:1407.2139) beta=9.58
Iter  7000 | Total loss: 56.6930 (MSE:0.0652, Reg:56.6279) beta=7.68
Iter  7461 | Total loss: 0.0722 (MSE:0.0722, Reg:0.0000) beta=6.81
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.1109, device='cuda:0', requires_grad=True)

[25/50] encoder.layers.5.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0374 (MSE:0.0374, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 465439.2812 (MSE:0.0386, Reg:465439.2500) beta=20.00
Iter  1000 | Total loss: 3755.6299 (MSE:0.0404, Reg:3755.5894) beta=19.05
Iter  2000 | Total loss: 1332.3916 (MSE:0.0440, Reg:1332.3477) beta=17.16
Iter  3000 | Total loss: 587.2563 (MSE:0.0490, Reg:587.2074) beta=15.26
Iter  4000 | Total loss: 299.7708 (MSE:0.0414, Reg:299.7294) beta=13.37
Iter  5000 | Total loss: 111.9832 (MSE:0.0379, Reg:111.9453) beta=11.47
Iter  6000 | Total loss: 35.0532 (MSE:0.0532, Reg:35.0000) beta=9.58
Iter  7000 | Total loss: 6.0443 (MSE:0.0443, Reg:6.0000) beta=7.68
Iter  7417 | Total loss: 0.0549 (MSE:0.0549, Reg:0.0000) beta=6.89
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.2151, device='cuda:0', requires_grad=True)

[26/50] encoder.layers.6.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0700 (MSE:0.0700, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 544418.1875 (MSE:0.0514, Reg:544418.1250) beta=20.00
Iter  1000 | Total loss: 51494.6289 (MSE:0.0516, Reg:51494.5781) beta=19.05
Iter  2000 | Total loss: 22451.7324 (MSE:0.0520, Reg:22451.6797) beta=17.16
Iter  3000 | Total loss: 13804.0518 (MSE:0.0539, Reg:13803.9980) beta=15.26
Iter  4000 | Total loss: 8272.1328 (MSE:0.0542, Reg:8272.0791) beta=13.37
Iter  5000 | Total loss: 4234.6558 (MSE:0.0470, Reg:4234.6089) beta=11.47
Iter  6000 | Total loss: 1287.2523 (MSE:0.0536, Reg:1287.1987) beta=9.58
Iter  7000 | Total loss: 86.6142 (MSE:0.0490, Reg:86.5653) beta=7.68
Iter  7908 | Total loss: 0.0536 (MSE:0.0536, Reg:0.0000) beta=5.96
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0434, device='cuda:0', requires_grad=True)

[27/50] encoder.layers.6.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 163857.5000 (MSE:0.0047, Reg:163857.5000) beta=20.00
Iter  1000 | Total loss: 951.9853 (MSE:0.0050, Reg:951.9803) beta=19.05
Iter  2000 | Total loss: 252.4742 (MSE:0.0048, Reg:252.4694) beta=17.16
Iter  3000 | Total loss: 131.9813 (MSE:0.0049, Reg:131.9764) beta=15.26
Iter  4000 | Total loss: 62.9384 (MSE:0.0049, Reg:62.9334) beta=13.37
Iter  5000 | Total loss: 25.0049 (MSE:0.0049, Reg:25.0000) beta=11.47
Iter  6000 | Total loss: 6.3147 (MSE:0.0050, Reg:6.3096) beta=9.58
Iter  7000 | Total loss: 1.0047 (MSE:0.0047, Reg:1.0000) beta=7.68
Iter  7048 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=7.59
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0118, device='cuda:0', requires_grad=True)

[28/50] encoder.layers.6.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1112 (MSE:0.1112, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 732631.5000 (MSE:0.0824, Reg:732631.4375) beta=20.00
Iter  1000 | Total loss: 38866.9102 (MSE:0.0754, Reg:38866.8359) beta=19.05
Iter  2000 | Total loss: 17542.8184 (MSE:0.0835, Reg:17542.7344) beta=17.16
Iter  3000 | Total loss: 10868.3965 (MSE:0.0701, Reg:10868.3262) beta=15.26
Iter  4000 | Total loss: 6682.0640 (MSE:0.0733, Reg:6681.9907) beta=13.37
Iter  5000 | Total loss: 3243.3872 (MSE:0.0740, Reg:3243.3132) beta=11.47
Iter  6000 | Total loss: 933.8486 (MSE:0.0769, Reg:933.7717) beta=9.58
Iter  7000 | Total loss: 19.2720 (MSE:0.0751, Reg:19.1969) beta=7.68
Iter  7610 | Total loss: 0.0753 (MSE:0.0753, Reg:0.0000) beta=6.53
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0579, device='cuda:0', requires_grad=True)

[29/50] encoder.layers.6.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0094 (MSE:0.0094, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 390988.0000 (MSE:0.0080, Reg:390988.0000) beta=20.00
Iter  1000 | Total loss: 197.0022 (MSE:0.0091, Reg:196.9931) beta=19.05
Iter  2000 | Total loss: 86.5582 (MSE:0.0090, Reg:86.5492) beta=17.16
Iter  3000 | Total loss: 60.0091 (MSE:0.0091, Reg:60.0000) beta=15.26
Iter  4000 | Total loss: 44.0093 (MSE:0.0093, Reg:44.0000) beta=13.37
Iter  5000 | Total loss: 28.0088 (MSE:0.0088, Reg:28.0000) beta=11.47
Iter  6000 | Total loss: 12.5221 (MSE:0.0103, Reg:12.5118) beta=9.58
Iter  6960 | Total loss: 0.0094 (MSE:0.0094, Reg:0.0000) beta=7.76
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0136, device='cuda:0', requires_grad=True)

[30/50] encoder.layers.7.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0612 (MSE:0.0612, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 601927.4375 (MSE:0.0490, Reg:601927.3750) beta=20.00
Iter  1000 | Total loss: 49970.2227 (MSE:0.0515, Reg:49970.1719) beta=19.05
Iter  2000 | Total loss: 21590.6934 (MSE:0.0536, Reg:21590.6406) beta=17.16
Iter  3000 | Total loss: 13264.9375 (MSE:0.0485, Reg:13264.8887) beta=15.26
Iter  4000 | Total loss: 8073.8164 (MSE:0.0461, Reg:8073.7705) beta=13.37
Iter  5000 | Total loss: 3962.4624 (MSE:0.0491, Reg:3962.4133) beta=11.47
Iter  6000 | Total loss: 1186.7974 (MSE:0.0468, Reg:1186.7506) beta=9.58
Iter  7000 | Total loss: 55.0006 (MSE:0.0511, Reg:54.9495) beta=7.68
Iter  7639 | Total loss: 0.0508 (MSE:0.0508, Reg:0.0000) beta=6.47
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0419, device='cuda:0', requires_grad=True)

[31/50] encoder.layers.7.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 153859.0312 (MSE:0.0038, Reg:153859.0312) beta=20.00
Iter  1000 | Total loss: 200.1656 (MSE:0.0041, Reg:200.1615) beta=19.05
Iter  2000 | Total loss: 68.9768 (MSE:0.0039, Reg:68.9730) beta=17.16
Iter  3000 | Total loss: 40.0038 (MSE:0.0038, Reg:40.0000) beta=15.26
Iter  4000 | Total loss: 23.0016 (MSE:0.0040, Reg:22.9976) beta=13.37
Iter  5000 | Total loss: 13.0041 (MSE:0.0041, Reg:13.0000) beta=11.47
Iter  6000 | Total loss: 6.0039 (MSE:0.0039, Reg:6.0000) beta=9.58
Iter  6858 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=7.95
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0110, device='cuda:0', requires_grad=True)

[32/50] encoder.layers.7.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1424 (MSE:0.1424, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 826040.6250 (MSE:0.0812, Reg:826040.5625) beta=20.00
Iter  1000 | Total loss: 51022.4883 (MSE:0.0868, Reg:51022.4023) beta=19.05
Iter  2000 | Total loss: 22401.7734 (MSE:0.0905, Reg:22401.6836) beta=17.16
Iter  3000 | Total loss: 12975.5752 (MSE:0.1004, Reg:12975.4746) beta=15.26
Iter  4000 | Total loss: 7407.1699 (MSE:0.0945, Reg:7407.0752) beta=13.37
Iter  5000 | Total loss: 3325.7043 (MSE:0.0916, Reg:3325.6128) beta=11.47
Iter  6000 | Total loss: 735.5378 (MSE:0.0880, Reg:735.4498) beta=9.58
Iter  7000 | Total loss: 18.7069 (MSE:0.0914, Reg:18.6154) beta=7.68
Iter  7571 | Total loss: 0.0868 (MSE:0.0868, Reg:0.0000) beta=6.60
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0593, device='cuda:0', requires_grad=True)

[33/50] encoder.layers.7.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0123 (MSE:0.0123, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 348967.6875 (MSE:0.0110, Reg:348967.6875) beta=20.00
Iter  1000 | Total loss: 456.1412 (MSE:0.0113, Reg:456.1299) beta=19.05
Iter  2000 | Total loss: 156.2423 (MSE:0.0119, Reg:156.2304) beta=17.16
Iter  3000 | Total loss: 95.0111 (MSE:0.0115, Reg:94.9996) beta=15.26
Iter  4000 | Total loss: 67.9382 (MSE:0.0115, Reg:67.9268) beta=13.37
Iter  5000 | Total loss: 38.0116 (MSE:0.0116, Reg:38.0000) beta=11.47
Iter  6000 | Total loss: 15.2093 (MSE:0.0108, Reg:15.1985) beta=9.58
Iter  6944 | Total loss: 0.0109 (MSE:0.0109, Reg:0.0000) beta=7.79
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0209, device='cuda:0', requires_grad=True)

[34/50] encoder.layers.8.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0622 (MSE:0.0622, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 608366.4375 (MSE:0.0478, Reg:608366.3750) beta=20.00
Iter  1000 | Total loss: 42646.6719 (MSE:0.0487, Reg:42646.6250) beta=19.05
Iter  2000 | Total loss: 18639.3301 (MSE:0.0535, Reg:18639.2773) beta=17.16
Iter  3000 | Total loss: 11476.4678 (MSE:0.0474, Reg:11476.4199) beta=15.26
Iter  4000 | Total loss: 7099.3896 (MSE:0.0511, Reg:7099.3384) beta=13.37
Iter  5000 | Total loss: 3690.0293 (MSE:0.0473, Reg:3689.9819) beta=11.47
Iter  6000 | Total loss: 1070.0945 (MSE:0.0479, Reg:1070.0466) beta=9.58
Iter  7000 | Total loss: 35.6722 (MSE:0.0466, Reg:35.6257) beta=7.68
Iter  7435 | Total loss: 0.0536 (MSE:0.0536, Reg:0.0000) beta=6.86
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0402, device='cuda:0', requires_grad=True)

[35/50] encoder.layers.8.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 142381.7031 (MSE:0.0034, Reg:142381.7031) beta=20.00
Iter  1000 | Total loss: 112.0125 (MSE:0.0033, Reg:112.0091) beta=19.05
Iter  2000 | Total loss: 35.0035 (MSE:0.0035, Reg:35.0000) beta=17.16
Iter  3000 | Total loss: 15.0036 (MSE:0.0036, Reg:15.0000) beta=15.26
Iter  4000 | Total loss: 9.0037 (MSE:0.0037, Reg:9.0000) beta=13.37
Iter  5000 | Total loss: 5.0234 (MSE:0.0035, Reg:5.0199) beta=11.47
Iter  6000 | Total loss: 2.0036 (MSE:0.0036, Reg:2.0000) beta=9.58
Iter  6144 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=9.31
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0157, device='cuda:0', requires_grad=True)

[36/50] encoder.layers.8.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1889 (MSE:0.1889, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 951013.8125 (MSE:0.1045, Reg:951013.6875) beta=20.00
Iter  1000 | Total loss: 75988.7578 (MSE:0.1035, Reg:75988.6562) beta=19.05
Iter  2000 | Total loss: 34006.3281 (MSE:0.0993, Reg:34006.2305) beta=17.16
Iter  3000 | Total loss: 18258.4043 (MSE:0.1073, Reg:18258.2969) beta=15.26
Iter  4000 | Total loss: 9611.2588 (MSE:0.1090, Reg:9611.1494) beta=13.37
Iter  5000 | Total loss: 3644.3840 (MSE:0.1025, Reg:3644.2815) beta=11.47
Iter  6000 | Total loss: 644.6947 (MSE:0.1015, Reg:644.5932) beta=9.58
Iter  7000 | Total loss: 12.7127 (MSE:0.1087, Reg:12.6039) beta=7.68
Iter  7300 | Total loss: 0.0991 (MSE:0.0991, Reg:0.0000) beta=7.12
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0585, device='cuda:0', requires_grad=True)

[37/50] encoder.layers.8.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0113 (MSE:0.0113, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 372950.9375 (MSE:0.0089, Reg:372950.9375) beta=20.00
Iter  1000 | Total loss: 333.8891 (MSE:0.0105, Reg:333.8786) beta=19.05
Iter  2000 | Total loss: 168.5877 (MSE:0.0111, Reg:168.5766) beta=17.16
Iter  3000 | Total loss: 124.0111 (MSE:0.0111, Reg:124.0000) beta=15.26
Iter  4000 | Total loss: 84.0115 (MSE:0.0115, Reg:84.0000) beta=13.37
Iter  5000 | Total loss: 53.2432 (MSE:0.0113, Reg:53.2319) beta=11.47
Iter  6000 | Total loss: 25.0112 (MSE:0.0112, Reg:25.0000) beta=9.58
Iter  6874 | Total loss: 0.0108 (MSE:0.0108, Reg:0.0000) beta=7.92
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0222, device='cuda:0', requires_grad=True)

[38/50] encoder.layers.9.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0445 (MSE:0.0445, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 601781.7500 (MSE:0.0279, Reg:601781.7500) beta=20.00
Iter  1000 | Total loss: 42082.0039 (MSE:0.0273, Reg:42081.9766) beta=19.05
Iter  2000 | Total loss: 17044.1387 (MSE:0.0280, Reg:17044.1113) beta=17.16
Iter  3000 | Total loss: 9481.7393 (MSE:0.0260, Reg:9481.7129) beta=15.26
Iter  4000 | Total loss: 5515.2939 (MSE:0.0322, Reg:5515.2617) beta=13.37
Iter  5000 | Total loss: 2717.8523 (MSE:0.0286, Reg:2717.8237) beta=11.47
Iter  6000 | Total loss: 782.8959 (MSE:0.0302, Reg:782.8658) beta=9.58
Iter  7000 | Total loss: 44.9084 (MSE:0.0301, Reg:44.8783) beta=7.68
Iter  7662 | Total loss: 0.0287 (MSE:0.0287, Reg:0.0000) beta=6.43
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0304, device='cuda:0', requires_grad=True)

[39/50] encoder.layers.9.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 128491.6797 (MSE:0.0014, Reg:128491.6797) beta=20.00
Iter  1000 | Total loss: 18.0016 (MSE:0.0016, Reg:18.0000) beta=19.05
Iter  2000 | Total loss: 6.0016 (MSE:0.0016, Reg:6.0000) beta=17.16
Iter  3000 | Total loss: 4.0016 (MSE:0.0016, Reg:4.0000) beta=15.26
Iter  4000 | Total loss: 1.8297 (MSE:0.0017, Reg:1.8280) beta=13.37
Iter  4210 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=12.97
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0126, device='cuda:0', requires_grad=True)

[40/50] encoder.layers.9.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1878 (MSE:0.1878, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 982346.3125 (MSE:0.0872, Reg:982346.2500) beta=20.00
Iter  1000 | Total loss: 84627.9062 (MSE:0.0913, Reg:84627.8125) beta=19.05
Iter  2000 | Total loss: 35696.9805 (MSE:0.0834, Reg:35696.8984) beta=17.16
Iter  3000 | Total loss: 18687.8418 (MSE:0.0858, Reg:18687.7559) beta=15.26
Iter  4000 | Total loss: 9612.7148 (MSE:0.0857, Reg:9612.6289) beta=13.37
Iter  5000 | Total loss: 3619.4756 (MSE:0.0868, Reg:3619.3887) beta=11.47
Iter  6000 | Total loss: 612.5295 (MSE:0.0879, Reg:612.4415) beta=9.58
Iter  7000 | Total loss: 13.0904 (MSE:0.0907, Reg:12.9997) beta=7.68
Iter  7386 | Total loss: 0.0833 (MSE:0.0833, Reg:0.0000) beta=6.95
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0538, device='cuda:0', requires_grad=True)

[41/50] encoder.layers.9.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0097 (MSE:0.0097, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 348590.8125 (MSE:0.0087, Reg:348590.8125) beta=20.00
Iter  1000 | Total loss: 457.5320 (MSE:0.0097, Reg:457.5222) beta=19.05
Iter  2000 | Total loss: 221.0111 (MSE:0.0111, Reg:221.0000) beta=17.16
Iter  3000 | Total loss: 149.5830 (MSE:0.0109, Reg:149.5721) beta=15.26
Iter  4000 | Total loss: 105.8695 (MSE:0.0099, Reg:105.8596) beta=13.37
Iter  5000 | Total loss: 63.4242 (MSE:0.0100, Reg:63.4142) beta=11.47
Iter  6000 | Total loss: 24.9536 (MSE:0.0105, Reg:24.9431) beta=9.58
Iter  7000 | Total loss: 1.0099 (MSE:0.0099, Reg:1.0000) beta=7.68
Iter  7283 | Total loss: 0.0095 (MSE:0.0095, Reg:0.0000) beta=7.15
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0298, device='cuda:0', requires_grad=True)

[42/50] encoder.layers.10.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0418 (MSE:0.0418, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 593865.8125 (MSE:0.0291, Reg:593865.8125) beta=20.00
Iter  1000 | Total loss: 22166.2207 (MSE:0.0300, Reg:22166.1914) beta=19.05
Iter  2000 | Total loss: 8360.9619 (MSE:0.0305, Reg:8360.9316) beta=17.16
Iter  3000 | Total loss: 4694.3579 (MSE:0.0309, Reg:4694.3271) beta=15.26
Iter  4000 | Total loss: 2804.7026 (MSE:0.0311, Reg:2804.6714) beta=13.37
Iter  5000 | Total loss: 1360.5798 (MSE:0.0293, Reg:1360.5505) beta=11.47
Iter  6000 | Total loss: 398.0903 (MSE:0.0319, Reg:398.0584) beta=9.58
Iter  7000 | Total loss: 24.1345 (MSE:0.0313, Reg:24.1032) beta=7.68
Iter  7627 | Total loss: 0.0297 (MSE:0.0297, Reg:0.0000) beta=6.50
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0283, device='cuda:0', requires_grad=True)

[43/50] encoder.layers.10.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 161040.3750 (MSE:0.0014, Reg:161040.3750) beta=20.00
Iter  1000 | Total loss: 603.2014 (MSE:0.0014, Reg:603.2000) beta=19.05
Iter  2000 | Total loss: 193.0927 (MSE:0.0016, Reg:193.0910) beta=17.16
Iter  3000 | Total loss: 70.8422 (MSE:0.0016, Reg:70.8406) beta=15.26
Iter  4000 | Total loss: 34.0017 (MSE:0.0017, Reg:34.0000) beta=13.37
Iter  5000 | Total loss: 10.9876 (MSE:0.0016, Reg:10.9860) beta=11.47
Iter  6000 | Total loss: 2.0018 (MSE:0.0018, Reg:2.0000) beta=9.58
Iter  6885 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=7.90
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0106, device='cuda:0', requires_grad=True)

[44/50] encoder.layers.10.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1213 (MSE:0.1213, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 850924.3125 (MSE:0.0791, Reg:850924.2500) beta=20.00
Iter  1000 | Total loss: 15768.1846 (MSE:0.0793, Reg:15768.1055) beta=19.05
Iter  2000 | Total loss: 6691.9580 (MSE:0.0861, Reg:6691.8721) beta=17.16
Iter  3000 | Total loss: 4226.3198 (MSE:0.0859, Reg:4226.2339) beta=15.26
Iter  4000 | Total loss: 2695.7290 (MSE:0.0780, Reg:2695.6509) beta=13.37
Iter  5000 | Total loss: 1409.2448 (MSE:0.0910, Reg:1409.1538) beta=11.47
Iter  6000 | Total loss: 404.9862 (MSE:0.0872, Reg:404.8990) beta=9.58
Iter  7000 | Total loss: 10.0844 (MSE:0.0851, Reg:9.9994) beta=7.68
Iter  7278 | Total loss: 0.0899 (MSE:0.0899, Reg:0.0000) beta=7.16
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0805, device='cuda:0', requires_grad=True)

[45/50] encoder.layers.10.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0548 (MSE:0.0548, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 348763.3125 (MSE:0.0496, Reg:348763.2500) beta=20.00
Iter  1000 | Total loss: 2414.0769 (MSE:0.0452, Reg:2414.0317) beta=19.05
Iter  2000 | Total loss: 955.0497 (MSE:0.0460, Reg:955.0037) beta=17.16
Iter  3000 | Total loss: 476.2356 (MSE:0.0424, Reg:476.1931) beta=15.26
Iter  4000 | Total loss: 241.0361 (MSE:0.0457, Reg:240.9905) beta=13.37
Iter  5000 | Total loss: 88.2332 (MSE:0.0446, Reg:88.1886) beta=11.47
Iter  6000 | Total loss: 19.8439 (MSE:0.0474, Reg:19.7965) beta=9.58
Iter  7000 | Total loss: 1.0552 (MSE:0.0552, Reg:1.0000) beta=7.68
Iter  7162 | Total loss: 0.0395 (MSE:0.0395, Reg:0.0000) beta=7.38
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0774, device='cuda:0', requires_grad=True)

[46/50] encoder.layers.11.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0624 (MSE:0.0624, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 557524.4375 (MSE:0.0474, Reg:557524.3750) beta=20.00
Iter  1000 | Total loss: 15590.8926 (MSE:0.0489, Reg:15590.8438) beta=19.05
Iter  2000 | Total loss: 5094.1260 (MSE:0.0492, Reg:5094.0767) beta=17.16
Iter  3000 | Total loss: 2608.0259 (MSE:0.0449, Reg:2607.9810) beta=15.26
Iter  4000 | Total loss: 1382.8247 (MSE:0.0447, Reg:1382.7800) beta=13.37
Iter  5000 | Total loss: 611.0136 (MSE:0.0474, Reg:610.9662) beta=11.47
Iter  6000 | Total loss: 169.3707 (MSE:0.0522, Reg:169.3185) beta=9.58
Iter  7000 | Total loss: 8.5388 (MSE:0.0438, Reg:8.4950) beta=7.68
Iter  7476 | Total loss: 0.0507 (MSE:0.0507, Reg:0.0000) beta=6.78
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0394, device='cuda:0', requires_grad=True)

[47/50] encoder.layers.11.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 154193.6406 (MSE:0.0044, Reg:154193.6406) beta=20.00
Iter  1000 | Total loss: 1244.6119 (MSE:0.0052, Reg:1244.6067) beta=19.05
Iter  2000 | Total loss: 422.2535 (MSE:0.0057, Reg:422.2478) beta=17.16
Iter  3000 | Total loss: 145.4272 (MSE:0.0053, Reg:145.4220) beta=15.26
Iter  4000 | Total loss: 49.9842 (MSE:0.0056, Reg:49.9786) beta=13.37
Iter  5000 | Total loss: 12.0051 (MSE:0.0051, Reg:12.0000) beta=11.47
Iter  6000 | Total loss: 2.9814 (MSE:0.0051, Reg:2.9763) beta=9.58
Iter  6412 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=8.80
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0347, device='cuda:0', requires_grad=True)

[48/50] encoder.layers.11.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1073 (MSE:0.1073, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 758079.5625 (MSE:0.0882, Reg:758079.5000) beta=20.00
Iter  1000 | Total loss: 5022.4663 (MSE:0.0883, Reg:5022.3779) beta=19.05
Iter  2000 | Total loss: 1716.0879 (MSE:0.0925, Reg:1715.9954) beta=17.16
Iter  3000 | Total loss: 1012.5134 (MSE:0.0928, Reg:1012.4207) beta=15.26
Iter  4000 | Total loss: 659.5474 (MSE:0.1011, Reg:659.4463) beta=13.37
Iter  5000 | Total loss: 351.9991 (MSE:0.1046, Reg:351.8945) beta=11.47
Iter  6000 | Total loss: 117.9283 (MSE:0.0903, Reg:117.8380) beta=9.58
Iter  7000 | Total loss: 4.0704 (MSE:0.0906, Reg:3.9798) beta=7.68
Iter  7219 | Total loss: 0.0898 (MSE:0.0898, Reg:0.0000) beta=7.27
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0596, device='cuda:0', requires_grad=True)

[49/50] encoder.layers.11.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0893 (MSE:0.0893, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 375912.6875 (MSE:0.0159, Reg:375912.6562) beta=20.00
Iter  1000 | Total loss: 80.0027 (MSE:0.0932, Reg:79.9095) beta=19.05
Iter  2000 | Total loss: 22.0918 (MSE:0.0918, Reg:22.0000) beta=17.16
Iter  3000 | Total loss: 13.0888 (MSE:0.0888, Reg:13.0000) beta=15.26
Iter  4000 | Total loss: 9.0881 (MSE:0.0881, Reg:9.0000) beta=13.37
Iter  5000 | Total loss: 6.0889 (MSE:0.0889, Reg:6.0000) beta=11.47
Iter  6000 | Total loss: 2.0860 (MSE:0.0860, Reg:2.0000) beta=9.58
Iter  7000 | Total loss: 1.0863 (MSE:0.0863, Reg:1.0000) beta=7.68
Iter  7150 | Total loss: 0.0917 (MSE:0.0917, Reg:0.0000) beta=7.40
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0686, device='cuda:0', requires_grad=True)

[50/50] heads.0
    INPUT_FP : torch.Size([1024, 768])
    OUTPUT_FP : torch.Size([1024, 1000])
    V   : , torch.Size([1000, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.2148 (MSE:0.2148, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 216800.5625 (MSE:0.1165, Reg:216800.4531) beta=20.00
Iter  1000 | Total loss: 25030.9023 (MSE:0.1177, Reg:25030.7852) beta=19.05
Iter  2000 | Total loss: 6675.5576 (MSE:0.1075, Reg:6675.4502) beta=17.16
Iter  3000 | Total loss: 2875.3372 (MSE:0.1223, Reg:2875.2148) beta=15.26
Iter  4000 | Total loss: 1125.5309 (MSE:0.1205, Reg:1125.4104) beta=13.37
Iter  5000 | Total loss: 347.1898 (MSE:0.1149, Reg:347.0750) beta=11.47
Iter  6000 | Total loss: 49.6881 (MSE:0.1187, Reg:49.5694) beta=9.58
Iter  6762 | Total loss: 0.1105 (MSE:0.1105, Reg:0.0000) beta=8.14
    Early stopped
    Set the rounding value

AdaRound for PerLayer weights is done.

    Quantized model Evaluation accuracy on 50000 images, 79.076%
Total time: 9195.40 sec
