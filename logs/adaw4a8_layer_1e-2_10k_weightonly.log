
- main_args params:
    - arch: ViT_B_16
    - batch_size: 128
    - num_samples: 1024

- weight params:
    - scheme: AbsMaxQuantizer
    - bit_width: 4
    - per_channel: True
    - AdaRound: PerLayer

- activation params:
    - scheme: MovAvgAbsMaxQuantizer
    - bit_width: 8
    - per_channel: False
    - momentum: 0.95
    - batches: 16
    - Identity addition : INT16 (The input of each LayerNorm)

- softmax params:
    - bit_width: 16
    - Activation of Softmax(Q@K/d_K) (attn_map) : UINT8

- layer_norm params:
    - bit_width: 8

- gelu params:
    - bit_width: 8

Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
    Initiated the V
Activation calibration is done.

[1/50] conv_proj
    INPUT_FP : torch.Size([1024, 3, 224, 224])
    OUTPUT_FP : torch.Size([1024, 768, 14, 14])
    V   : , torch.Size([768, 3, 16, 16])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 119754.2500 (MSE:0.0005, Reg:119754.2500) beta=20.00
Iter  1000 | Total loss: 3.5910 (MSE:0.0030, Reg:3.5880) beta=19.05
Iter  2000 | Total loss: 1.0035 (MSE:0.0035, Reg:1.0000) beta=17.16
Iter  3000 | Total loss: 1.0032 (MSE:0.0032, Reg:1.0000) beta=15.26
Iter  4000 | Total loss: 1.0027 (MSE:0.0027, Reg:1.0000) beta=13.37
Iter  5000 | Total loss: 1.0032 (MSE:0.0032, Reg:1.0000) beta=11.47
Iter  6000 | Total loss: 1.0031 (MSE:0.0031, Reg:1.0000) beta=9.58
Iter  6405 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=8.81
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0539, device='cuda:0', requires_grad=True)

[2/50] encoder.layers.0.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0099 (MSE:0.0099, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 258953.8438 (MSE:0.0021, Reg:258953.8438) beta=20.00
Iter  1000 | Total loss: 18625.0996 (MSE:0.0021, Reg:18625.0977) beta=19.05
Iter  2000 | Total loss: 8237.6992 (MSE:0.0023, Reg:8237.6973) beta=17.16
Iter  3000 | Total loss: 4913.8481 (MSE:0.0023, Reg:4913.8457) beta=15.26
Iter  4000 | Total loss: 3090.8464 (MSE:0.0022, Reg:3090.8442) beta=13.37
Iter  5000 | Total loss: 1657.7578 (MSE:0.0028, Reg:1657.7550) beta=11.47
Iter  6000 | Total loss: 656.2729 (MSE:0.0023, Reg:656.2706) beta=9.58
Iter  7000 | Total loss: 173.1125 (MSE:0.0024, Reg:173.1101) beta=7.68
Iter  8000 | Total loss: 9.8131 (MSE:0.0022, Reg:9.8109) beta=5.79
Iter  8574 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=4.70
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0258, device='cuda:0', requires_grad=True)

[3/50] encoder.layers.0.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 51294.7852 (MSE:0.0001, Reg:51294.7852) beta=20.00
Iter  1000 | Total loss: 196.5084 (MSE:0.0001, Reg:196.5083) beta=19.05
Iter  2000 | Total loss: 92.9999 (MSE:0.0001, Reg:92.9997) beta=17.16
Iter  3000 | Total loss: 51.0001 (MSE:0.0001, Reg:51.0000) beta=15.26
Iter  4000 | Total loss: 28.9920 (MSE:0.0001, Reg:28.9919) beta=13.37
Iter  5000 | Total loss: 12.6241 (MSE:0.0001, Reg:12.6240) beta=11.47
Iter  6000 | Total loss: 8.0001 (MSE:0.0001, Reg:8.0000) beta=9.58
Iter  7000 | Total loss: 3.0001 (MSE:0.0001, Reg:3.0000) beta=7.68
Iter  7368 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=6.99
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0125, device='cuda:0', requires_grad=True)

[4/50] encoder.layers.0.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0256 (MSE:0.0256, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 429743.8438 (MSE:0.0067, Reg:429743.8438) beta=20.00
Iter  1000 | Total loss: 14755.4492 (MSE:0.0067, Reg:14755.4424) beta=19.05
Iter  2000 | Total loss: 6898.0996 (MSE:0.0070, Reg:6898.0928) beta=17.16
Iter  3000 | Total loss: 4639.3740 (MSE:0.0069, Reg:4639.3672) beta=15.26
Iter  4000 | Total loss: 3014.1033 (MSE:0.0068, Reg:3014.0964) beta=13.37
Iter  5000 | Total loss: 1933.4209 (MSE:0.0070, Reg:1933.4138) beta=11.47
Iter  6000 | Total loss: 744.6448 (MSE:0.0076, Reg:744.6372) beta=9.58
Iter  7000 | Total loss: 115.8996 (MSE:0.0072, Reg:115.8924) beta=7.68
Iter  7973 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=5.84
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0633, device='cuda:0', requires_grad=True)

[5/50] encoder.layers.0.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 441887.8750 (MSE:0.0021, Reg:441887.8750) beta=20.00
Iter  1000 | Total loss: 5344.9551 (MSE:0.0024, Reg:5344.9526) beta=19.05
Iter  2000 | Total loss: 2395.4819 (MSE:0.0024, Reg:2395.4795) beta=17.16
Iter  3000 | Total loss: 1563.7148 (MSE:0.0027, Reg:1563.7122) beta=15.26
Iter  4000 | Total loss: 1026.6757 (MSE:0.0026, Reg:1026.6731) beta=13.37
Iter  5000 | Total loss: 593.6644 (MSE:0.0026, Reg:593.6618) beta=11.47
Iter  6000 | Total loss: 287.0106 (MSE:0.0025, Reg:287.0081) beta=9.58
Iter  7000 | Total loss: 46.9505 (MSE:0.0025, Reg:46.9479) beta=7.68
Iter  8000 | Total loss: 1.0027 (MSE:0.0027, Reg:1.0000) beta=5.79
Iter  8044 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=5.71
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0318, device='cuda:0', requires_grad=True)

[6/50] encoder.layers.1.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0079 (MSE:0.0079, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 354327.5625 (MSE:0.0081, Reg:354327.5625) beta=20.00
Iter  1000 | Total loss: 16697.1406 (MSE:0.0047, Reg:16697.1367) beta=19.05
Iter  2000 | Total loss: 6917.3574 (MSE:0.0044, Reg:6917.3530) beta=17.16
Iter  3000 | Total loss: 4289.8174 (MSE:0.0048, Reg:4289.8125) beta=15.26
Iter  4000 | Total loss: 2805.2002 (MSE:0.0047, Reg:2805.1956) beta=13.37
Iter  5000 | Total loss: 1646.3711 (MSE:0.0048, Reg:1646.3662) beta=11.47
Iter  6000 | Total loss: 757.3333 (MSE:0.0050, Reg:757.3283) beta=9.58
Iter  7000 | Total loss: 147.7374 (MSE:0.0049, Reg:147.7324) beta=7.68
Iter  8000 | Total loss: 2.9978 (MSE:0.0047, Reg:2.9931) beta=5.79
Iter  8106 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=5.59
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0307, device='cuda:0', requires_grad=True)

[7/50] encoder.layers.1.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 66345.5781 (MSE:0.0003, Reg:66345.5781) beta=20.00
Iter  1000 | Total loss: 146.9772 (MSE:0.0003, Reg:146.9770) beta=19.05
Iter  2000 | Total loss: 43.9089 (MSE:0.0002, Reg:43.9086) beta=17.16
Iter  3000 | Total loss: 22.0003 (MSE:0.0003, Reg:22.0000) beta=15.26
Iter  4000 | Total loss: 10.0003 (MSE:0.0003, Reg:10.0000) beta=13.37
Iter  5000 | Total loss: 5.9998 (MSE:0.0003, Reg:5.9995) beta=11.47
Iter  6000 | Total loss: 2.0003 (MSE:0.0003, Reg:2.0000) beta=9.58
Iter  6325 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=8.96
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0084, device='cuda:0', requires_grad=True)

[8/50] encoder.layers.1.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0577 (MSE:0.0577, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 778375.5000 (MSE:0.0164, Reg:778375.5000) beta=20.00
Iter  1000 | Total loss: 39007.9805 (MSE:0.0201, Reg:39007.9609) beta=19.05
Iter  2000 | Total loss: 15901.4756 (MSE:0.0213, Reg:15901.4541) beta=17.16
Iter  3000 | Total loss: 10320.4863 (MSE:0.0199, Reg:10320.4668) beta=15.26
Iter  4000 | Total loss: 6576.6841 (MSE:0.0202, Reg:6576.6641) beta=13.37
Iter  5000 | Total loss: 3575.4746 (MSE:0.0204, Reg:3575.4541) beta=11.47
Iter  6000 | Total loss: 1436.2469 (MSE:0.0200, Reg:1436.2269) beta=9.58
Iter  7000 | Total loss: 287.4764 (MSE:0.0205, Reg:287.4559) beta=7.68
Iter  7829 | Total loss: 0.0200 (MSE:0.0200, Reg:0.0000) beta=6.11
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0666, device='cuda:0', requires_grad=True)

[9/50] encoder.layers.1.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 356408.3750 (MSE:0.0023, Reg:356408.3750) beta=20.00
Iter  1000 | Total loss: 422.2944 (MSE:0.0029, Reg:422.2914) beta=19.05
Iter  2000 | Total loss: 169.0029 (MSE:0.0029, Reg:169.0000) beta=17.16
Iter  3000 | Total loss: 100.5691 (MSE:0.0029, Reg:100.5662) beta=15.26
Iter  4000 | Total loss: 62.5177 (MSE:0.0030, Reg:62.5147) beta=13.37
Iter  5000 | Total loss: 42.0031 (MSE:0.0031, Reg:42.0000) beta=11.47
Iter  6000 | Total loss: 15.0031 (MSE:0.0031, Reg:15.0000) beta=9.58
Iter  6962 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=7.76
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0215, device='cuda:0', requires_grad=True)

[10/50] encoder.layers.2.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0214 (MSE:0.0214, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 444687.2500 (MSE:0.0133, Reg:444687.2500) beta=20.00
Iter  1000 | Total loss: 30242.9668 (MSE:0.0132, Reg:30242.9531) beta=19.05
Iter  2000 | Total loss: 11535.3389 (MSE:0.0142, Reg:11535.3242) beta=17.16
Iter  3000 | Total loss: 6986.3174 (MSE:0.0135, Reg:6986.3037) beta=15.26
Iter  4000 | Total loss: 4479.2583 (MSE:0.0132, Reg:4479.2451) beta=13.37
Iter  5000 | Total loss: 2520.8733 (MSE:0.0148, Reg:2520.8584) beta=11.47
Iter  6000 | Total loss: 1099.2916 (MSE:0.0142, Reg:1099.2775) beta=9.58
Iter  7000 | Total loss: 213.1572 (MSE:0.0130, Reg:213.1441) beta=7.68
Iter  8000 | Total loss: 1.0131 (MSE:0.0131, Reg:1.0000) beta=5.79
Iter  8173 | Total loss: 0.0137 (MSE:0.0137, Reg:0.0000) beta=5.46
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0445, device='cuda:0', requires_grad=True)

[11/50] encoder.layers.2.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 102371.7812 (MSE:0.0009, Reg:102371.7812) beta=20.00
Iter  1000 | Total loss: 370.0424 (MSE:0.0011, Reg:370.0413) beta=19.05
Iter  2000 | Total loss: 147.8200 (MSE:0.0011, Reg:147.8189) beta=17.16
Iter  3000 | Total loss: 99.0010 (MSE:0.0010, Reg:99.0000) beta=15.26
Iter  4000 | Total loss: 71.0010 (MSE:0.0010, Reg:71.0000) beta=13.37
Iter  5000 | Total loss: 42.7652 (MSE:0.0011, Reg:42.7641) beta=11.47
Iter  6000 | Total loss: 19.9619 (MSE:0.0011, Reg:19.9608) beta=9.58
Iter  7000 | Total loss: 8.0012 (MSE:0.0012, Reg:8.0000) beta=7.68
Iter  7403 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=6.92
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0088, device='cuda:0', requires_grad=True)

[12/50] encoder.layers.2.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0529 (MSE:0.0529, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 757463.7500 (MSE:0.0240, Reg:757463.7500) beta=20.00
Iter  1000 | Total loss: 49425.6875 (MSE:0.0285, Reg:49425.6602) beta=19.05
Iter  2000 | Total loss: 28842.7480 (MSE:0.0293, Reg:28842.7188) beta=17.16
Iter  3000 | Total loss: 20664.7891 (MSE:0.0281, Reg:20664.7617) beta=15.26
Iter  4000 | Total loss: 15535.7607 (MSE:0.0284, Reg:15535.7324) beta=13.37
Iter  5000 | Total loss: 9523.4238 (MSE:0.0469, Reg:9523.3770) beta=11.47
Iter  6000 | Total loss: 5239.6011 (MSE:0.0283, Reg:5239.5728) beta=9.58
Iter  7000 | Total loss: 1943.8857 (MSE:0.0268, Reg:1943.8589) beta=7.68
Iter  7985 | Total loss: 0.0267 (MSE:0.0267, Reg:0.0000) beta=5.82
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0790, device='cuda:0', requires_grad=True)

[13/50] encoder.layers.2.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 385993.6250 (MSE:0.0043, Reg:385993.6250) beta=20.00
Iter  1000 | Total loss: 642.5689 (MSE:0.0055, Reg:642.5634) beta=19.05
Iter  2000 | Total loss: 228.1832 (MSE:0.0055, Reg:228.1777) beta=17.16
Iter  3000 | Total loss: 137.9736 (MSE:0.0054, Reg:137.9682) beta=15.26
Iter  4000 | Total loss: 80.0050 (MSE:0.0051, Reg:80.0000) beta=13.37
Iter  5000 | Total loss: 46.0050 (MSE:0.0050, Reg:46.0000) beta=11.47
Iter  6000 | Total loss: 15.0051 (MSE:0.0051, Reg:15.0000) beta=9.58
Iter  7000 | Total loss: 5.0051 (MSE:0.0053, Reg:4.9999) beta=7.68
Iter  7346 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=7.03
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0234, device='cuda:0', requires_grad=True)

[14/50] encoder.layers.3.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0330 (MSE:0.0330, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 486836.0938 (MSE:0.0254, Reg:486836.0625) beta=20.00
Iter  1000 | Total loss: 41164.6328 (MSE:0.0219, Reg:41164.6094) beta=19.05
Iter  2000 | Total loss: 14917.8750 (MSE:0.0228, Reg:14917.8525) beta=17.16
Iter  3000 | Total loss: 8430.2393 (MSE:0.0220, Reg:8430.2168) beta=15.26
Iter  4000 | Total loss: 4937.4917 (MSE:0.0209, Reg:4937.4707) beta=13.37
Iter  5000 | Total loss: 2646.7373 (MSE:0.0204, Reg:2646.7170) beta=11.47
Iter  6000 | Total loss: 1069.0945 (MSE:0.0213, Reg:1069.0732) beta=9.58
Iter  7000 | Total loss: 196.2515 (MSE:0.0198, Reg:196.2317) beta=7.68
Iter  7868 | Total loss: 0.0197 (MSE:0.0197, Reg:0.0000) beta=6.04
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0453, device='cuda:0', requires_grad=True)

[15/50] encoder.layers.3.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 126780.5469 (MSE:0.0016, Reg:126780.5469) beta=20.00
Iter  1000 | Total loss: 448.2885 (MSE:0.0015, Reg:448.2870) beta=19.05
Iter  2000 | Total loss: 177.1153 (MSE:0.0016, Reg:177.1136) beta=17.16
Iter  3000 | Total loss: 126.9730 (MSE:0.0016, Reg:126.9715) beta=15.26
Iter  4000 | Total loss: 86.0020 (MSE:0.0020, Reg:86.0000) beta=13.37
Iter  5000 | Total loss: 52.9990 (MSE:0.0016, Reg:52.9974) beta=11.47
Iter  6000 | Total loss: 19.0016 (MSE:0.0016, Reg:19.0000) beta=9.58
Iter  6877 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=7.92
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0113, device='cuda:0', requires_grad=True)

[16/50] encoder.layers.3.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0692 (MSE:0.0692, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 695276.5625 (MSE:0.0460, Reg:695276.5000) beta=20.00
Iter  1000 | Total loss: 43370.1719 (MSE:0.0451, Reg:43370.1250) beta=19.05
Iter  2000 | Total loss: 19266.2188 (MSE:0.0476, Reg:19266.1719) beta=17.16
Iter  3000 | Total loss: 12408.2607 (MSE:0.0460, Reg:12408.2148) beta=15.26
Iter  4000 | Total loss: 8038.5776 (MSE:0.0461, Reg:8038.5317) beta=13.37
Iter  5000 | Total loss: 4378.2783 (MSE:0.0441, Reg:4378.2344) beta=11.47
Iter  6000 | Total loss: 1619.9847 (MSE:0.0378, Reg:1619.9469) beta=9.58
Iter  7000 | Total loss: 125.1015 (MSE:0.0428, Reg:125.0588) beta=7.68
Iter  7631 | Total loss: 0.0436 (MSE:0.0436, Reg:0.0000) beta=6.49
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0613, device='cuda:0', requires_grad=True)

[17/50] encoder.layers.3.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 359487.0000 (MSE:0.0040, Reg:359487.0000) beta=20.00
Iter  1000 | Total loss: 409.5426 (MSE:0.0044, Reg:409.5383) beta=19.05
Iter  2000 | Total loss: 167.6445 (MSE:0.0048, Reg:167.6398) beta=17.16
Iter  3000 | Total loss: 89.2811 (MSE:0.0044, Reg:89.2767) beta=15.26
Iter  4000 | Total loss: 52.0041 (MSE:0.0041, Reg:52.0000) beta=13.37
Iter  5000 | Total loss: 22.0043 (MSE:0.0043, Reg:22.0000) beta=11.47
Iter  6000 | Total loss: 12.0011 (MSE:0.0041, Reg:11.9970) beta=9.58
Iter  7000 | Total loss: 4.0040 (MSE:0.0040, Reg:4.0000) beta=7.68
Iter  7624 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=6.50
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0128, device='cuda:0', requires_grad=True)

[18/50] encoder.layers.4.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0485 (MSE:0.0485, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 496971.1875 (MSE:0.0369, Reg:496971.1562) beta=20.00
Iter  1000 | Total loss: 53540.4648 (MSE:0.0374, Reg:53540.4258) beta=19.05
Iter  2000 | Total loss: 21762.2949 (MSE:0.0348, Reg:21762.2598) beta=17.16
Iter  3000 | Total loss: 12849.6475 (MSE:0.0350, Reg:12849.6123) beta=15.26
Iter  4000 | Total loss: 7677.6436 (MSE:0.0334, Reg:7677.6104) beta=13.37
Iter  5000 | Total loss: 4037.9851 (MSE:0.0336, Reg:4037.9514) beta=11.47
Iter  6000 | Total loss: 1525.9421 (MSE:0.0357, Reg:1525.9065) beta=9.58
Iter  7000 | Total loss: 203.1895 (MSE:0.0364, Reg:203.1531) beta=7.68
Iter  7747 | Total loss: 0.0402 (MSE:0.0402, Reg:0.0000) beta=6.27
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0447, device='cuda:0', requires_grad=True)

[19/50] encoder.layers.4.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 131806.7500 (MSE:0.0023, Reg:131806.7500) beta=20.00
Iter  1000 | Total loss: 124.6707 (MSE:0.0024, Reg:124.6682) beta=19.05
Iter  2000 | Total loss: 53.0018 (MSE:0.0023, Reg:52.9995) beta=17.16
Iter  3000 | Total loss: 27.8398 (MSE:0.0024, Reg:27.8375) beta=15.26
Iter  4000 | Total loss: 16.0023 (MSE:0.0023, Reg:16.0000) beta=13.37
Iter  5000 | Total loss: 7.0024 (MSE:0.0024, Reg:7.0000) beta=11.47
Iter  6000 | Total loss: 2.0023 (MSE:0.0023, Reg:2.0000) beta=9.58
Iter  6222 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=9.16
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0112, device='cuda:0', requires_grad=True)

[20/50] encoder.layers.4.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0686 (MSE:0.0686, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 636269.7500 (MSE:0.0533, Reg:636269.6875) beta=20.00
Iter  1000 | Total loss: 48291.5781 (MSE:0.0513, Reg:48291.5273) beta=19.05
Iter  2000 | Total loss: 22567.8086 (MSE:0.0494, Reg:22567.7598) beta=17.16
Iter  3000 | Total loss: 14868.2998 (MSE:0.0526, Reg:14868.2471) beta=15.26
Iter  4000 | Total loss: 9378.9004 (MSE:0.0567, Reg:9378.8438) beta=13.37
Iter  5000 | Total loss: 5007.4292 (MSE:0.0657, Reg:5007.3638) beta=11.47
Iter  6000 | Total loss: 1569.2938 (MSE:0.0508, Reg:1569.2430) beta=9.58
Iter  7000 | Total loss: 77.9706 (MSE:0.0544, Reg:77.9162) beta=7.68
Iter  7648 | Total loss: 0.0530 (MSE:0.0530, Reg:0.0000) beta=6.46
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0533, device='cuda:0', requires_grad=True)

[21/50] encoder.layers.4.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 415457.2188 (MSE:0.0058, Reg:415457.2188) beta=20.00
Iter  1000 | Total loss: 197.3177 (MSE:0.0082, Reg:197.3094) beta=19.05
Iter  2000 | Total loss: 75.0066 (MSE:0.0066, Reg:75.0000) beta=17.16
Iter  3000 | Total loss: 51.0082 (MSE:0.0082, Reg:51.0000) beta=15.26
Iter  4000 | Total loss: 39.0062 (MSE:0.0077, Reg:38.9985) beta=13.37
Iter  5000 | Total loss: 23.9530 (MSE:0.0065, Reg:23.9465) beta=11.47
Iter  6000 | Total loss: 7.0068 (MSE:0.0068, Reg:7.0000) beta=9.58
Iter  6784 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=8.09
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0628, device='cuda:0', requires_grad=True)

[22/50] encoder.layers.5.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0479 (MSE:0.0479, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 511420.5938 (MSE:0.0328, Reg:511420.5625) beta=20.00
Iter  1000 | Total loss: 52612.6367 (MSE:0.0321, Reg:52612.6055) beta=19.05
Iter  2000 | Total loss: 22232.6309 (MSE:0.0354, Reg:22232.5957) beta=17.16
Iter  3000 | Total loss: 13278.3418 (MSE:0.0390, Reg:13278.3027) beta=15.26
Iter  4000 | Total loss: 7887.6069 (MSE:0.0345, Reg:7887.5723) beta=13.37
Iter  5000 | Total loss: 3847.0583 (MSE:0.0393, Reg:3847.0190) beta=11.47
Iter  6000 | Total loss: 1266.1581 (MSE:0.0391, Reg:1266.1189) beta=9.58
Iter  7000 | Total loss: 117.2327 (MSE:0.0408, Reg:117.1919) beta=7.68
Iter  7705 | Total loss: 0.0359 (MSE:0.0359, Reg:0.0000) beta=6.35
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0408, device='cuda:0', requires_grad=True)

[23/50] encoder.layers.5.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 151362.6562 (MSE:0.0029, Reg:151362.6562) beta=20.00
Iter  1000 | Total loss: 485.3240 (MSE:0.0029, Reg:485.3210) beta=19.05
Iter  2000 | Total loss: 163.4357 (MSE:0.0026, Reg:163.4331) beta=17.16
Iter  3000 | Total loss: 82.0029 (MSE:0.0029, Reg:82.0000) beta=15.26
Iter  4000 | Total loss: 43.9386 (MSE:0.0028, Reg:43.9358) beta=13.37
Iter  5000 | Total loss: 13.0029 (MSE:0.0029, Reg:13.0000) beta=11.47
Iter  6000 | Total loss: 4.0028 (MSE:0.0028, Reg:4.0000) beta=9.58
Iter  6781 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=8.10
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0117, device='cuda:0', requires_grad=True)

[24/50] encoder.layers.5.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1086 (MSE:0.1086, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 701792.6875 (MSE:0.0631, Reg:701792.6250) beta=20.00
Iter  1000 | Total loss: 46450.9609 (MSE:0.0607, Reg:46450.8984) beta=19.05
Iter  2000 | Total loss: 21198.4199 (MSE:0.0765, Reg:21198.3438) beta=17.16
Iter  3000 | Total loss: 13539.1836 (MSE:0.0643, Reg:13539.1191) beta=15.26
Iter  4000 | Total loss: 8383.2715 (MSE:0.0651, Reg:8383.2061) beta=13.37
Iter  5000 | Total loss: 4181.8667 (MSE:0.0748, Reg:4181.7920) beta=11.47
Iter  6000 | Total loss: 1279.3613 (MSE:0.0672, Reg:1279.2942) beta=9.58
Iter  7000 | Total loss: 48.7632 (MSE:0.0645, Reg:48.6988) beta=7.68
Iter  7638 | Total loss: 0.0670 (MSE:0.0670, Reg:0.0000) beta=6.48
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.1109, device='cuda:0', requires_grad=True)

[25/50] encoder.layers.5.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0557 (MSE:0.0557, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 517297.3125 (MSE:0.0435, Reg:517297.2812) beta=20.00
Iter  1000 | Total loss: 4585.8110 (MSE:0.0442, Reg:4585.7666) beta=19.05
Iter  2000 | Total loss: 1735.7321 (MSE:0.0436, Reg:1735.6885) beta=17.16
Iter  3000 | Total loss: 799.4742 (MSE:0.0526, Reg:799.4216) beta=15.26
Iter  4000 | Total loss: 388.9523 (MSE:0.0396, Reg:388.9127) beta=13.37
Iter  5000 | Total loss: 159.5040 (MSE:0.0448, Reg:159.4593) beta=11.47
Iter  6000 | Total loss: 38.3002 (MSE:0.0436, Reg:38.2566) beta=9.58
Iter  7000 | Total loss: 7.9873 (MSE:0.0468, Reg:7.9405) beta=7.68
Iter  8000 | Total loss: 1.0406 (MSE:0.0406, Reg:1.0000) beta=5.79
Iter  8039 | Total loss: 0.0462 (MSE:0.0462, Reg:0.0000) beta=5.72
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.2151, device='cuda:0', requires_grad=True)

[26/50] encoder.layers.6.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0650 (MSE:0.0650, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 548138.8125 (MSE:0.0497, Reg:548138.7500) beta=20.00
Iter  1000 | Total loss: 51779.8203 (MSE:0.0516, Reg:51779.7695) beta=19.05
Iter  2000 | Total loss: 22510.7383 (MSE:0.0552, Reg:22510.6836) beta=17.16
Iter  3000 | Total loss: 13811.6611 (MSE:0.0500, Reg:13811.6113) beta=15.26
Iter  4000 | Total loss: 8194.1992 (MSE:0.0559, Reg:8194.1436) beta=13.37
Iter  5000 | Total loss: 4116.6406 (MSE:0.0534, Reg:4116.5874) beta=11.47
Iter  6000 | Total loss: 1315.0593 (MSE:0.0528, Reg:1315.0066) beta=9.58
Iter  7000 | Total loss: 72.9638 (MSE:0.0504, Reg:72.9134) beta=7.68
Iter  7569 | Total loss: 0.0556 (MSE:0.0556, Reg:0.0000) beta=6.61
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0434, device='cuda:0', requires_grad=True)

[27/50] encoder.layers.6.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 162886.5312 (MSE:0.0051, Reg:162886.5312) beta=20.00
Iter  1000 | Total loss: 915.4276 (MSE:0.0051, Reg:915.4225) beta=19.05
Iter  2000 | Total loss: 274.1297 (MSE:0.0048, Reg:274.1249) beta=17.16
Iter  3000 | Total loss: 141.7446 (MSE:0.0050, Reg:141.7396) beta=15.26
Iter  4000 | Total loss: 81.9622 (MSE:0.0048, Reg:81.9574) beta=13.37
Iter  5000 | Total loss: 37.0047 (MSE:0.0047, Reg:37.0000) beta=11.47
Iter  6000 | Total loss: 9.0049 (MSE:0.0049, Reg:9.0000) beta=9.58
Iter  6906 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=7.86
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0118, device='cuda:0', requires_grad=True)

[28/50] encoder.layers.6.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1088 (MSE:0.1088, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 730096.1875 (MSE:0.0772, Reg:730096.1250) beta=20.00
Iter  1000 | Total loss: 38102.4180 (MSE:0.0745, Reg:38102.3438) beta=19.05
Iter  2000 | Total loss: 16788.3301 (MSE:0.0755, Reg:16788.2539) beta=17.16
Iter  3000 | Total loss: 10353.6133 (MSE:0.0714, Reg:10353.5420) beta=15.26
Iter  4000 | Total loss: 6347.2188 (MSE:0.0713, Reg:6347.1475) beta=13.37
Iter  5000 | Total loss: 3141.5730 (MSE:0.0758, Reg:3141.4971) beta=11.47
Iter  6000 | Total loss: 885.3179 (MSE:0.0790, Reg:885.2389) beta=9.58
Iter  7000 | Total loss: 29.6664 (MSE:0.0729, Reg:29.5935) beta=7.68
Iter  7550 | Total loss: 0.0722 (MSE:0.0722, Reg:0.0000) beta=6.64
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0579, device='cuda:0', requires_grad=True)

[29/50] encoder.layers.6.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0098 (MSE:0.0098, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 388585.4375 (MSE:0.0080, Reg:388585.4375) beta=20.00
Iter  1000 | Total loss: 211.7991 (MSE:0.0085, Reg:211.7906) beta=19.05
Iter  2000 | Total loss: 81.9294 (MSE:0.0087, Reg:81.9207) beta=17.16
Iter  3000 | Total loss: 62.4335 (MSE:0.0093, Reg:62.4242) beta=15.26
Iter  4000 | Total loss: 39.0094 (MSE:0.0094, Reg:39.0000) beta=13.37
Iter  5000 | Total loss: 23.0088 (MSE:0.0088, Reg:23.0000) beta=11.47
Iter  6000 | Total loss: 9.0093 (MSE:0.0093, Reg:9.0000) beta=9.58
Iter  6760 | Total loss: 0.0090 (MSE:0.0090, Reg:0.0000) beta=8.14
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0136, device='cuda:0', requires_grad=True)

[30/50] encoder.layers.7.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0687 (MSE:0.0687, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 598424.1250 (MSE:0.0523, Reg:598424.0625) beta=20.00
Iter  1000 | Total loss: 50241.3516 (MSE:0.0546, Reg:50241.2969) beta=19.05
Iter  2000 | Total loss: 21322.8496 (MSE:0.0519, Reg:21322.7969) beta=17.16
Iter  3000 | Total loss: 12924.0928 (MSE:0.0499, Reg:12924.0430) beta=15.26
Iter  4000 | Total loss: 7615.5659 (MSE:0.0492, Reg:7615.5166) beta=13.37
Iter  5000 | Total loss: 3848.1277 (MSE:0.0464, Reg:3848.0813) beta=11.47
Iter  6000 | Total loss: 1139.6704 (MSE:0.0496, Reg:1139.6208) beta=9.58
Iter  7000 | Total loss: 53.3855 (MSE:0.0491, Reg:53.3364) beta=7.68
Iter  7712 | Total loss: 0.0508 (MSE:0.0508, Reg:0.0000) beta=6.34
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0419, device='cuda:0', requires_grad=True)

[31/50] encoder.layers.7.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 154090.6250 (MSE:0.0037, Reg:154090.6250) beta=20.00
Iter  1000 | Total loss: 166.1372 (MSE:0.0039, Reg:166.1334) beta=19.05
Iter  2000 | Total loss: 46.0040 (MSE:0.0040, Reg:46.0000) beta=17.16
Iter  3000 | Total loss: 29.8195 (MSE:0.0041, Reg:29.8154) beta=15.26
Iter  4000 | Total loss: 18.0040 (MSE:0.0040, Reg:18.0000) beta=13.37
Iter  5000 | Total loss: 12.0041 (MSE:0.0041, Reg:12.0000) beta=11.47
Iter  6000 | Total loss: 2.0039 (MSE:0.0039, Reg:2.0000) beta=9.58
Iter  6940 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=7.80
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0110, device='cuda:0', requires_grad=True)

[32/50] encoder.layers.7.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1344 (MSE:0.1344, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 826142.6875 (MSE:0.0895, Reg:826142.6250) beta=20.00
Iter  1000 | Total loss: 53104.5352 (MSE:0.0886, Reg:53104.4453) beta=19.05
Iter  2000 | Total loss: 23076.9707 (MSE:0.0873, Reg:23076.8828) beta=17.16
Iter  3000 | Total loss: 13308.6377 (MSE:0.0995, Reg:13308.5381) beta=15.26
Iter  4000 | Total loss: 7480.2847 (MSE:0.0941, Reg:7480.1904) beta=13.37
Iter  5000 | Total loss: 3397.6565 (MSE:0.0875, Reg:3397.5691) beta=11.47
Iter  6000 | Total loss: 793.6613 (MSE:0.0891, Reg:793.5721) beta=9.58
Iter  7000 | Total loss: 11.4519 (MSE:0.0902, Reg:11.3617) beta=7.68
Iter  7175 | Total loss: 0.0852 (MSE:0.0852, Reg:0.0000) beta=7.35
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0593, device='cuda:0', requires_grad=True)

[33/50] encoder.layers.7.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0117 (MSE:0.0117, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 348468.5000 (MSE:0.0110, Reg:348468.5000) beta=20.00
Iter  1000 | Total loss: 465.3080 (MSE:0.0119, Reg:465.2961) beta=19.05
Iter  2000 | Total loss: 175.9292 (MSE:0.0118, Reg:175.9174) beta=17.16
Iter  3000 | Total loss: 111.9972 (MSE:0.0120, Reg:111.9852) beta=15.26
Iter  4000 | Total loss: 78.0114 (MSE:0.0114, Reg:78.0000) beta=13.37
Iter  5000 | Total loss: 54.0117 (MSE:0.0117, Reg:54.0000) beta=11.47
Iter  6000 | Total loss: 16.0114 (MSE:0.0114, Reg:16.0000) beta=9.58
Iter  6754 | Total loss: 0.0119 (MSE:0.0119, Reg:0.0000) beta=8.15
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0209, device='cuda:0', requires_grad=True)

[34/50] encoder.layers.8.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0634 (MSE:0.0634, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 607176.3125 (MSE:0.0441, Reg:607176.2500) beta=20.00
Iter  1000 | Total loss: 41946.2422 (MSE:0.0485, Reg:41946.1953) beta=19.05
Iter  2000 | Total loss: 18128.8555 (MSE:0.0478, Reg:18128.8086) beta=17.16
Iter  3000 | Total loss: 11046.9736 (MSE:0.0537, Reg:11046.9199) beta=15.26
Iter  4000 | Total loss: 6709.8179 (MSE:0.0522, Reg:6709.7656) beta=13.37
Iter  5000 | Total loss: 3341.1497 (MSE:0.0509, Reg:3341.0986) beta=11.47
Iter  6000 | Total loss: 917.9372 (MSE:0.0555, Reg:917.8817) beta=9.58
Iter  7000 | Total loss: 32.9385 (MSE:0.0461, Reg:32.8924) beta=7.68
Iter  7732 | Total loss: 0.0448 (MSE:0.0448, Reg:0.0000) beta=6.30
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0402, device='cuda:0', requires_grad=True)

[35/50] encoder.layers.8.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 143683.8750 (MSE:0.0033, Reg:143683.8750) beta=20.00
Iter  1000 | Total loss: 119.0765 (MSE:0.0035, Reg:119.0729) beta=19.05
Iter  2000 | Total loss: 27.0035 (MSE:0.0035, Reg:27.0000) beta=17.16
Iter  3000 | Total loss: 16.0036 (MSE:0.0036, Reg:16.0000) beta=15.26
Iter  4000 | Total loss: 11.0039 (MSE:0.0039, Reg:11.0000) beta=13.37
Iter  5000 | Total loss: 6.0032 (MSE:0.0037, Reg:5.9996) beta=11.47
Iter  5816 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=9.93
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0157, device='cuda:0', requires_grad=True)

[36/50] encoder.layers.8.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.2000 (MSE:0.2000, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 952900.3125 (MSE:0.1061, Reg:952900.1875) beta=20.00
Iter  1000 | Total loss: 79044.5000 (MSE:0.1143, Reg:79044.3828) beta=19.05
Iter  2000 | Total loss: 35086.5039 (MSE:0.1127, Reg:35086.3906) beta=17.16
Iter  3000 | Total loss: 18640.9902 (MSE:0.1122, Reg:18640.8789) beta=15.26
Iter  4000 | Total loss: 9941.1006 (MSE:0.1085, Reg:9940.9922) beta=13.37
Iter  5000 | Total loss: 3851.1055 (MSE:0.1090, Reg:3850.9963) beta=11.47
Iter  6000 | Total loss: 644.1921 (MSE:0.1032, Reg:644.0889) beta=9.58
Iter  7000 | Total loss: 8.1059 (MSE:0.1059, Reg:8.0000) beta=7.68
Iter  7414 | Total loss: 0.0999 (MSE:0.0999, Reg:0.0000) beta=6.90
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0585, device='cuda:0', requires_grad=True)

[37/50] encoder.layers.8.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0110 (MSE:0.0110, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 373778.5000 (MSE:0.0088, Reg:373778.5000) beta=20.00
Iter  1000 | Total loss: 347.5302 (MSE:0.0111, Reg:347.5191) beta=19.05
Iter  2000 | Total loss: 152.2762 (MSE:0.0114, Reg:152.2648) beta=17.16
Iter  3000 | Total loss: 109.0107 (MSE:0.0108, Reg:109.0000) beta=15.26
Iter  4000 | Total loss: 78.0111 (MSE:0.0111, Reg:78.0000) beta=13.37
Iter  5000 | Total loss: 48.6841 (MSE:0.0110, Reg:48.6731) beta=11.47
Iter  6000 | Total loss: 17.0112 (MSE:0.0112, Reg:17.0000) beta=9.58
Iter  7000 | Total loss: 3.0112 (MSE:0.0112, Reg:3.0000) beta=7.68
Iter  7320 | Total loss: 0.0105 (MSE:0.0105, Reg:0.0000) beta=7.08
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0222, device='cuda:0', requires_grad=True)

[38/50] encoder.layers.9.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0444 (MSE:0.0444, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 600072.8750 (MSE:0.0276, Reg:600072.8750) beta=20.00
Iter  1000 | Total loss: 41792.8359 (MSE:0.0306, Reg:41792.8047) beta=19.05
Iter  2000 | Total loss: 16842.9902 (MSE:0.0300, Reg:16842.9609) beta=17.16
Iter  3000 | Total loss: 9451.3994 (MSE:0.0299, Reg:9451.3691) beta=15.26
Iter  4000 | Total loss: 5381.2075 (MSE:0.0335, Reg:5381.1738) beta=13.37
Iter  5000 | Total loss: 2536.4026 (MSE:0.0296, Reg:2536.3730) beta=11.47
Iter  6000 | Total loss: 725.0531 (MSE:0.0293, Reg:725.0238) beta=9.58
Iter  7000 | Total loss: 32.1304 (MSE:0.0286, Reg:32.1019) beta=7.68
Iter  7667 | Total loss: 0.0277 (MSE:0.0277, Reg:0.0000) beta=6.42
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0304, device='cuda:0', requires_grad=True)

[39/50] encoder.layers.9.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 126907.0156 (MSE:0.0015, Reg:126907.0156) beta=20.00
Iter  1000 | Total loss: 30.9947 (MSE:0.0018, Reg:30.9928) beta=19.05
Iter  2000 | Total loss: 11.9976 (MSE:0.0016, Reg:11.9960) beta=17.16
Iter  3000 | Total loss: 5.0016 (MSE:0.0016, Reg:5.0000) beta=15.26
Iter  4000 | Total loss: 2.0016 (MSE:0.0016, Reg:2.0000) beta=13.37
Iter  5000 | Total loss: 1.0017 (MSE:0.0017, Reg:1.0000) beta=11.47
Iter  5677 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=10.19
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0126, device='cuda:0', requires_grad=True)

[40/50] encoder.layers.9.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1811 (MSE:0.1811, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 983610.0000 (MSE:0.0831, Reg:983609.9375) beta=20.00
Iter  1000 | Total loss: 89119.9141 (MSE:0.0824, Reg:89119.8281) beta=19.05
Iter  2000 | Total loss: 37548.1836 (MSE:0.0826, Reg:37548.1016) beta=17.16
Iter  3000 | Total loss: 19413.7441 (MSE:0.0826, Reg:19413.6621) beta=15.26
Iter  4000 | Total loss: 9984.5654 (MSE:0.0829, Reg:9984.4824) beta=13.37
Iter  5000 | Total loss: 3836.6401 (MSE:0.0867, Reg:3836.5535) beta=11.47
Iter  6000 | Total loss: 653.4670 (MSE:0.0790, Reg:653.3881) beta=9.58
Iter  7000 | Total loss: 17.0715 (MSE:0.0922, Reg:16.9793) beta=7.68
Iter  7289 | Total loss: 0.0816 (MSE:0.0816, Reg:0.0000) beta=7.14
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0538, device='cuda:0', requires_grad=True)

[41/50] encoder.layers.9.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0128 (MSE:0.0128, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 347006.9062 (MSE:0.0083, Reg:347006.9062) beta=20.00
Iter  1000 | Total loss: 515.1119 (MSE:0.0104, Reg:515.1015) beta=19.05
Iter  2000 | Total loss: 248.1625 (MSE:0.0096, Reg:248.1529) beta=17.16
Iter  3000 | Total loss: 176.6785 (MSE:0.0101, Reg:176.6684) beta=15.26
Iter  4000 | Total loss: 109.8176 (MSE:0.0101, Reg:109.8075) beta=13.37
Iter  5000 | Total loss: 58.2854 (MSE:0.0097, Reg:58.2757) beta=11.47
Iter  6000 | Total loss: 23.9884 (MSE:0.0101, Reg:23.9783) beta=9.58
Iter  7000 | Total loss: 1.0096 (MSE:0.0096, Reg:1.0000) beta=7.68
Iter  7200 | Total loss: 0.0100 (MSE:0.0100, Reg:0.0000) beta=7.31
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0298, device='cuda:0', requires_grad=True)

[42/50] encoder.layers.10.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0395 (MSE:0.0395, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 594584.3125 (MSE:0.0319, Reg:594584.2500) beta=20.00
Iter  1000 | Total loss: 21292.0977 (MSE:0.0311, Reg:21292.0664) beta=19.05
Iter  2000 | Total loss: 8095.5796 (MSE:0.0300, Reg:8095.5498) beta=17.16
Iter  3000 | Total loss: 4593.4150 (MSE:0.0294, Reg:4593.3857) beta=15.26
Iter  4000 | Total loss: 2803.3013 (MSE:0.0317, Reg:2803.2695) beta=13.37
Iter  5000 | Total loss: 1355.2695 (MSE:0.0304, Reg:1355.2391) beta=11.47
Iter  6000 | Total loss: 408.0711 (MSE:0.0352, Reg:408.0359) beta=9.58
Iter  7000 | Total loss: 24.9233 (MSE:0.0308, Reg:24.8925) beta=7.68
Iter  7756 | Total loss: 0.0309 (MSE:0.0309, Reg:0.0000) beta=6.25
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0283, device='cuda:0', requires_grad=True)

[43/50] encoder.layers.10.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 162275.3594 (MSE:0.0013, Reg:162275.3594) beta=20.00
Iter  1000 | Total loss: 491.6037 (MSE:0.0017, Reg:491.6021) beta=19.05
Iter  2000 | Total loss: 178.4429 (MSE:0.0019, Reg:178.4410) beta=17.16
Iter  3000 | Total loss: 78.7634 (MSE:0.0015, Reg:78.7619) beta=15.26
Iter  4000 | Total loss: 36.0017 (MSE:0.0017, Reg:36.0000) beta=13.37
Iter  5000 | Total loss: 14.8541 (MSE:0.0017, Reg:14.8523) beta=11.47
Iter  6000 | Total loss: 2.0016 (MSE:0.0016, Reg:2.0000) beta=9.58
Iter  6654 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=8.34
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0106, device='cuda:0', requires_grad=True)

[44/50] encoder.layers.10.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1165 (MSE:0.1165, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 853670.2500 (MSE:0.0716, Reg:853670.1875) beta=20.00
Iter  1000 | Total loss: 15490.2354 (MSE:0.0848, Reg:15490.1504) beta=19.05
Iter  2000 | Total loss: 6346.9502 (MSE:0.0850, Reg:6346.8652) beta=17.16
Iter  3000 | Total loss: 3956.1204 (MSE:0.0836, Reg:3956.0366) beta=15.26
Iter  4000 | Total loss: 2587.7356 (MSE:0.0847, Reg:2587.6509) beta=13.37
Iter  5000 | Total loss: 1369.4557 (MSE:0.0897, Reg:1369.3660) beta=11.47
Iter  6000 | Total loss: 387.7079 (MSE:0.0906, Reg:387.6172) beta=9.58
Iter  7000 | Total loss: 7.9610 (MSE:0.0845, Reg:7.8766) beta=7.68
Iter  7296 | Total loss: 0.0922 (MSE:0.0922, Reg:0.0000) beta=7.12
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0805, device='cuda:0', requires_grad=True)

[45/50] encoder.layers.10.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0588 (MSE:0.0588, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 358731.4062 (MSE:0.0423, Reg:358731.3750) beta=20.00
Iter  1000 | Total loss: 2018.8831 (MSE:0.0484, Reg:2018.8346) beta=19.05
Iter  2000 | Total loss: 880.7940 (MSE:0.0463, Reg:880.7477) beta=17.16
Iter  3000 | Total loss: 447.9151 (MSE:0.0451, Reg:447.8701) beta=15.26
Iter  4000 | Total loss: 232.6216 (MSE:0.0491, Reg:232.5725) beta=13.37
Iter  5000 | Total loss: 81.3816 (MSE:0.0540, Reg:81.3277) beta=11.47
Iter  6000 | Total loss: 21.6211 (MSE:0.0493, Reg:21.5718) beta=9.58
Iter  7000 | Total loss: 1.0105 (MSE:0.0446, Reg:0.9659) beta=7.68
Iter  7018 | Total loss: 0.0450 (MSE:0.0450, Reg:0.0000) beta=7.65
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0774, device='cuda:0', requires_grad=True)

[46/50] encoder.layers.11.self_attention.in_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0636 (MSE:0.0636, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 553840.5000 (MSE:0.0464, Reg:553840.4375) beta=20.00
Iter  1000 | Total loss: 15663.5518 (MSE:0.0464, Reg:15663.5049) beta=19.05
Iter  2000 | Total loss: 4926.6074 (MSE:0.0471, Reg:4926.5605) beta=17.16
Iter  3000 | Total loss: 2480.4731 (MSE:0.0532, Reg:2480.4199) beta=15.26
Iter  4000 | Total loss: 1334.2869 (MSE:0.0469, Reg:1334.2400) beta=13.37
Iter  5000 | Total loss: 605.3267 (MSE:0.0461, Reg:605.2805) beta=11.47
Iter  6000 | Total loss: 170.6954 (MSE:0.0495, Reg:170.6458) beta=9.58
Iter  7000 | Total loss: 12.9253 (MSE:0.0517, Reg:12.8736) beta=7.68
Iter  7474 | Total loss: 0.0508 (MSE:0.0508, Reg:0.0000) beta=6.79
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0394, device='cuda:0', requires_grad=True)

[47/50] encoder.layers.11.self_attention.out_proj
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 154767.6719 (MSE:0.0040, Reg:154767.6719) beta=20.00
Iter  1000 | Total loss: 1377.8455 (MSE:0.0047, Reg:1377.8408) beta=19.05
Iter  2000 | Total loss: 532.8834 (MSE:0.0047, Reg:532.8787) beta=17.16
Iter  3000 | Total loss: 193.2207 (MSE:0.0060, Reg:193.2147) beta=15.26
Iter  4000 | Total loss: 78.9474 (MSE:0.0050, Reg:78.9424) beta=13.37
Iter  5000 | Total loss: 17.0055 (MSE:0.0055, Reg:17.0000) beta=11.47
Iter  5900 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=9.77
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0347, device='cuda:0', requires_grad=True)

[48/50] encoder.layers.11.mlp.linear_1
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1078 (MSE:0.1078, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 755400.6250 (MSE:0.0946, Reg:755400.5000) beta=20.00
Iter  1000 | Total loss: 5659.3433 (MSE:0.0996, Reg:5659.2437) beta=19.05
Iter  2000 | Total loss: 1908.3156 (MSE:0.0954, Reg:1908.2202) beta=17.16
Iter  3000 | Total loss: 1075.9417 (MSE:0.1010, Reg:1075.8407) beta=15.26
Iter  4000 | Total loss: 665.9568 (MSE:0.0978, Reg:665.8591) beta=13.37
Iter  5000 | Total loss: 344.8365 (MSE:0.0966, Reg:344.7398) beta=11.47
Iter  6000 | Total loss: 115.9471 (MSE:0.1040, Reg:115.8431) beta=9.58
Iter  7000 | Total loss: 4.2347 (MSE:0.0872, Reg:4.1475) beta=7.68
Iter  7273 | Total loss: 0.0905 (MSE:0.0905, Reg:0.0000) beta=7.17
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0596, device='cuda:0', requires_grad=True)

[49/50] encoder.layers.11.mlp.linear_2
    INPUT_FP : torch.Size([1024, 197, 3072])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0977 (MSE:0.0977, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 370830.4688 (MSE:0.0173, Reg:370830.4375) beta=20.00
Iter  1000 | Total loss: 100.9004 (MSE:0.0923, Reg:100.8082) beta=19.05
Iter  2000 | Total loss: 37.0913 (MSE:0.0913, Reg:37.0000) beta=17.16
Iter  3000 | Total loss: 25.0899 (MSE:0.0899, Reg:25.0000) beta=15.26
Iter  4000 | Total loss: 17.0928 (MSE:0.0928, Reg:17.0000) beta=13.37
Iter  5000 | Total loss: 8.0875 (MSE:0.0875, Reg:8.0000) beta=11.47
Iter  6000 | Total loss: 5.0916 (MSE:0.0916, Reg:5.0000) beta=9.58
Iter  6915 | Total loss: 0.0873 (MSE:0.0873, Reg:0.0000) beta=7.85
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0686, device='cuda:0', requires_grad=True)

[50/50] heads.0
    INPUT_FP : torch.Size([1024, 768])
    OUTPUT_FP : torch.Size([1024, 1000])
    V   : , torch.Size([1000, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.2097 (MSE:0.2097, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 203283.0938 (MSE:0.1630, Reg:203282.9375) beta=20.00
Iter  1000 | Total loss: 23862.3184 (MSE:0.1243, Reg:23862.1934) beta=19.05
Iter  2000 | Total loss: 6278.9209 (MSE:0.1307, Reg:6278.7900) beta=17.16
Iter  3000 | Total loss: 2693.1829 (MSE:0.1355, Reg:2693.0474) beta=15.26
Iter  4000 | Total loss: 1068.7307 (MSE:0.1462, Reg:1068.5845) beta=13.37
Iter  5000 | Total loss: 329.5901 (MSE:0.1457, Reg:329.4444) beta=11.47
Iter  6000 | Total loss: 35.1259 (MSE:0.1286, Reg:34.9973) beta=9.58
Iter  6961 | Total loss: 0.1276 (MSE:0.1276, Reg:0.0000) beta=7.76
    Early stopped
    Set the rounding value

AdaRound for PerLayer weights is done.

    Quantized model Evaluation accuracy on 50000 images, 79.134%
Total time: 3871.28 sec
