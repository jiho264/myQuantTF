
- main_args params:
    - arch: ViT_B_16
    - batch_size: 128
    - num_samples: 1024

- weight params:
    - scheme: AbsMaxQuantizer
    - bit_width: 4
    - per_channel: True
    - AdaRound: PerLayer

- activation params:
    - scheme: MovAvgAbsMaxQuantizer
    - bit_width: 8
    - per_channel: False
    - momentum: 0.95
    - batches: 16

- softmax params:
    - bit_width: 16

- Activation of Softmax(Q@K/d_K) (attn_map) : UINT8


- layer_norm params:
    - bit_width: 8

- Identity addition : INT16 (The input of each LayerNorm)

- gelu params:
    - bit_width: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
    Initiated the V
Activation calibration is done.

[1/50] conv_proj
 INPUT_FP : torch.Size([1024, 3, 224, 224])
 OUTPUT_FP : torch.Size([1024, 768, 14, 14])
    V   : , torch.Size([768, 3, 16, 16])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 54518.8789 (MSE:54518.8789, Reg:54518.8789) beta=20.00

Iter  2974 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=17.81
    Early stopped

    Set the rounding value

[2/50] encoder.layers.0.self_attention.in_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0089 (MSE:0.0089, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 58179.9570 (MSE:58179.9570, Reg:58179.9492) beta=20.00

Iter  2966 | Total loss: 0.0096 (MSE:0.0096, Reg:0.0000) beta=17.83
    Early stopped

    Set the rounding value

[3/50] encoder.layers.0.self_attention.out_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 23431.6133 (MSE:23431.6133, Reg:23431.6113) beta=20.00

Iter  2925 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=17.92
    Early stopped

    Set the rounding value

[4/50] encoder.layers.0.mlp.linear_1
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0180 (MSE:0.0180, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0166 (MSE:0.0166, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 83935.1484 (MSE:83935.1484, Reg:83935.0938) beta=20.00

Iter  2989 | Total loss: 0.0173 (MSE:0.0173, Reg:0.0000) beta=17.77
    Early stopped

    Set the rounding value

[5/50] encoder.layers.0.mlp.linear_2
 INPUT_FP : torch.Size([1024, 197, 3072])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0093 (MSE:0.0093, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0088 (MSE:0.0088, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 118641.6641 (MSE:118641.6641, Reg:118641.6562) beta=20.00

Iter  2955 | Total loss: 0.0094 (MSE:0.0094, Reg:0.0000) beta=17.85
    Early stopped

    Set the rounding value

[6/50] encoder.layers.1.self_attention.in_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0328 (MSE:0.0328, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0368 (MSE:0.0368, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 107993.8516 (MSE:107993.8516, Reg:107993.8203) beta=20.00

Iter  2975 | Total loss: 0.0309 (MSE:0.0309, Reg:0.0000) beta=17.81
    Early stopped

    Set the rounding value

[7/50] encoder.layers.1.self_attention.out_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 23270.7148 (MSE:23270.7148, Reg:23270.7129) beta=20.00

Iter  2950 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=17.86
    Early stopped

    Set the rounding value

[8/50] encoder.layers.1.mlp.linear_1
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1161 (MSE:0.1161, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0976 (MSE:0.0976, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 334353.5312 (MSE:334353.5312, Reg:334353.4375) beta=20.00

Iter  2976 | Total loss: 0.1079 (MSE:0.1079, Reg:0.0000) beta=17.80
    Early stopped

    Set the rounding value

[9/50] encoder.layers.1.mlp.linear_2
 INPUT_FP : torch.Size([1024, 197, 3072])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0105 (MSE:0.0105, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0097 (MSE:0.0097, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 204655.2812 (MSE:204655.2812, Reg:204655.2656) beta=20.00

Iter  3000 | Total loss: 0.0180 (MSE:0.0180, Reg:0.0064) beta=17.75

Iter  3015 | Total loss: 0.0138 (MSE:0.0138, Reg:0.0000) beta=17.72
    Early stopped

    Set the rounding value

[10/50] encoder.layers.2.self_attention.in_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0658 (MSE:0.0658, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0621 (MSE:0.0621, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 136889.1094 (MSE:136889.1094, Reg:136889.0625) beta=20.00

Iter  2973 | Total loss: 0.0536 (MSE:0.0536, Reg:0.0000) beta=17.81
    Early stopped

    Set the rounding value

[11/50] encoder.layers.2.self_attention.out_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 39011.9844 (MSE:39011.9844, Reg:39011.9766) beta=20.00

Iter  2901 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=17.97
    Early stopped

    Set the rounding value

[12/50] encoder.layers.2.mlp.linear_1
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0803 (MSE:0.0803, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0786 (MSE:0.0786, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 288334.0312 (MSE:288334.0312, Reg:288333.9688) beta=20.00

Iter  2961 | Total loss: 0.0789 (MSE:0.0789, Reg:0.0000) beta=17.84
    Early stopped

    Set the rounding value

[13/50] encoder.layers.2.mlp.linear_2
 INPUT_FP : torch.Size([1024, 197, 3072])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0132 (MSE:0.0132, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0116 (MSE:0.0116, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 172851.4531 (MSE:172851.4531, Reg:172851.4375) beta=20.00

Iter  2932 | Total loss: 0.0119 (MSE:0.0119, Reg:0.0000) beta=17.90
    Early stopped

    Set the rounding value

[14/50] encoder.layers.3.self_attention.in_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0610 (MSE:0.0610, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0676 (MSE:0.0676, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 150547.8281 (MSE:150547.8281, Reg:150547.7656) beta=20.00

Iter  2974 | Total loss: 0.0614 (MSE:0.0614, Reg:0.0000) beta=17.81
    Early stopped

    Set the rounding value

[15/50] encoder.layers.3.self_attention.out_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0070 (MSE:0.0070, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0073 (MSE:0.0073, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 40631.0000 (MSE:40631.0000, Reg:40630.9922) beta=20.00

Iter  2944 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=17.88
    Early stopped

    Set the rounding value

[16/50] encoder.layers.3.mlp.linear_1
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1306 (MSE:0.1306, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.1377 (MSE:0.1377, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 313480.3438 (MSE:313480.3438, Reg:313480.2188) beta=20.00

Iter  2981 | Total loss: 0.1392 (MSE:0.1392, Reg:0.0000) beta=17.79
    Early stopped

    Set the rounding value

[17/50] encoder.layers.3.mlp.linear_2
 INPUT_FP : torch.Size([1024, 197, 3072])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0121 (MSE:0.0121, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0104 (MSE:0.0104, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 175393.0938 (MSE:175393.0938, Reg:175393.0781) beta=20.00

Iter  2888 | Total loss: 0.0113 (MSE:0.0113, Reg:0.0000) beta=18.00
    Early stopped

    Set the rounding value

[18/50] encoder.layers.4.self_attention.in_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0901 (MSE:0.0901, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0875 (MSE:0.0875, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 162042.1719 (MSE:162042.1719, Reg:162042.0938) beta=20.00

Iter  2974 | Total loss: 0.0872 (MSE:0.0872, Reg:0.0000) beta=17.81
    Early stopped

    Set the rounding value

[19/50] encoder.layers.4.self_attention.out_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0090 (MSE:0.0090, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0086 (MSE:0.0086, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 41972.8281 (MSE:41972.8281, Reg:41972.8203) beta=20.00

Iter  2954 | Total loss: 0.0096 (MSE:0.0096, Reg:0.0000) beta=17.85
    Early stopped

    Set the rounding value

[20/50] encoder.layers.4.mlp.linear_1
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1667 (MSE:0.1667, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.1667 (MSE:0.1667, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 301826.1875 (MSE:301826.1875, Reg:301826.0312) beta=20.00

Iter  2978 | Total loss: 0.1701 (MSE:0.1701, Reg:0.0000) beta=17.80
    Early stopped

    Set the rounding value

[21/50] encoder.layers.4.mlp.linear_2
 INPUT_FP : torch.Size([1024, 197, 3072])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0135 (MSE:0.0135, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0138 (MSE:0.0138, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 207467.0156 (MSE:207467.0156, Reg:207467.0000) beta=20.00

Iter  2928 | Total loss: 0.0139 (MSE:0.0139, Reg:0.0000) beta=17.91
    Early stopped

    Set the rounding value

[22/50] encoder.layers.5.self_attention.in_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0980 (MSE:0.0980, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.1040 (MSE:0.1040, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 169334.2812 (MSE:169334.2812, Reg:169334.1875) beta=20.00

Iter  2979 | Total loss: 0.1006 (MSE:0.1006, Reg:0.0000) beta=17.80
    Early stopped

    Set the rounding value

[23/50] encoder.layers.5.self_attention.out_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0114 (MSE:0.0114, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0118 (MSE:0.0118, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 56940.6133 (MSE:56940.6133, Reg:56940.6016) beta=20.00

Iter  2944 | Total loss: 0.0115 (MSE:0.0115, Reg:0.0000) beta=17.88
    Early stopped

    Set the rounding value

[24/50] encoder.layers.5.mlp.linear_1
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1985 (MSE:0.1985, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.1914 (MSE:0.1914, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 322040.4688 (MSE:322040.4688, Reg:322040.2812) beta=20.00

Iter  2980 | Total loss: 0.1873 (MSE:0.1873, Reg:0.0000) beta=17.80
    Early stopped

    Set the rounding value

[25/50] encoder.layers.5.mlp.linear_2
 INPUT_FP : torch.Size([1024, 197, 3072])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0785 (MSE:0.0785, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0797 (MSE:0.0797, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 320767.9375 (MSE:320767.9375, Reg:320767.8750) beta=20.00

Iter  3000 | Total loss: 0.6743 (MSE:0.6743, Reg:0.5915) beta=17.75

Iter  3169 | Total loss: 0.0926 (MSE:0.0926, Reg:0.0000) beta=17.37
    Early stopped

    Set the rounding value

[26/50] encoder.layers.6.self_attention.in_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1392 (MSE:0.1392, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.1447 (MSE:0.1447, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 181970.5312 (MSE:181970.5312, Reg:181970.3906) beta=20.00

Iter  2978 | Total loss: 0.1431 (MSE:0.1431, Reg:0.0000) beta=17.80
    Early stopped

    Set the rounding value

[27/50] encoder.layers.6.self_attention.out_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0150 (MSE:0.0150, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0155 (MSE:0.0155, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 52081.1641 (MSE:52081.1641, Reg:52081.1484) beta=20.00

Iter  2936 | Total loss: 0.0167 (MSE:0.0167, Reg:0.0000) beta=17.89
    Early stopped

    Set the rounding value

[28/50] encoder.layers.6.mlp.linear_1
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.2081 (MSE:0.2081, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.2030 (MSE:0.2030, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 306013.3438 (MSE:306013.3438, Reg:306013.1250) beta=20.00

Iter  2974 | Total loss: 0.2058 (MSE:0.2058, Reg:0.0000) beta=17.81
    Early stopped

    Set the rounding value

[29/50] encoder.layers.6.mlp.linear_2
 INPUT_FP : torch.Size([1024, 197, 3072])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0209 (MSE:0.0209, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0215 (MSE:0.0215, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 245771.2812 (MSE:245771.2812, Reg:245771.2656) beta=20.00

Iter  3000 | Total loss: 0.0826 (MSE:0.0826, Reg:0.0622) beta=17.75

Iter  3074 | Total loss: 0.0214 (MSE:0.0214, Reg:0.0000) beta=17.58
    Early stopped

    Set the rounding value

[30/50] encoder.layers.7.self_attention.in_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1347 (MSE:0.1347, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.1338 (MSE:0.1338, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 185323.9531 (MSE:185323.9531, Reg:185323.8125) beta=20.00

Iter  2976 | Total loss: 0.1363 (MSE:0.1363, Reg:0.0000) beta=17.80
    Early stopped

    Set the rounding value

[31/50] encoder.layers.7.self_attention.out_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0120 (MSE:0.0120, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0118 (MSE:0.0118, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 39802.7773 (MSE:39802.7773, Reg:39802.7656) beta=20.00

Iter  2837 | Total loss: 0.0114 (MSE:0.0114, Reg:0.0000) beta=18.12
    Early stopped

    Set the rounding value

[32/50] encoder.layers.7.mlp.linear_1
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.2347 (MSE:0.2347, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.2367 (MSE:0.2367, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 315884.0938 (MSE:315884.0938, Reg:315883.8438) beta=20.00

Iter  2974 | Total loss: 0.2245 (MSE:0.2245, Reg:0.0000) beta=17.81
    Early stopped

    Set the rounding value

[33/50] encoder.layers.7.mlp.linear_2
 INPUT_FP : torch.Size([1024, 197, 3072])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0262 (MSE:0.0262, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0255 (MSE:0.0255, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 213819.8125 (MSE:213819.8125, Reg:213819.7812) beta=20.00

Iter  2944 | Total loss: 0.0262 (MSE:0.0262, Reg:0.0000) beta=17.88
    Early stopped

    Set the rounding value

[34/50] encoder.layers.8.self_attention.in_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1275 (MSE:0.1275, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.1208 (MSE:0.1208, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 172931.3125 (MSE:172931.3125, Reg:172931.1875) beta=20.00

Iter  2974 | Total loss: 0.1312 (MSE:0.1312, Reg:0.0000) beta=17.81
    Early stopped

    Set the rounding value

[35/50] encoder.layers.8.self_attention.out_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0110 (MSE:0.0110, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0109 (MSE:0.0109, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 31020.8242 (MSE:31020.8242, Reg:31020.8125) beta=20.00

Iter  2785 | Total loss: 0.0104 (MSE:0.0104, Reg:0.0000) beta=18.23
    Early stopped

    Set the rounding value

[36/50] encoder.layers.8.mlp.linear_1
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.2337 (MSE:0.2337, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.2548 (MSE:0.2548, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 318761.1875 (MSE:318761.1875, Reg:318760.9688) beta=20.00

Iter  2962 | Total loss: 0.2367 (MSE:0.2367, Reg:0.0000) beta=17.84
    Early stopped

    Set the rounding value

[37/50] encoder.layers.8.mlp.linear_2
 INPUT_FP : torch.Size([1024, 197, 3072])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0208 (MSE:0.0208, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0216 (MSE:0.0216, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 286401.8125 (MSE:286401.8125, Reg:286401.7812) beta=20.00

Iter  2940 | Total loss: 0.0210 (MSE:0.0210, Reg:0.0000) beta=17.88
    Early stopped

    Set the rounding value

[38/50] encoder.layers.9.self_attention.in_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0762 (MSE:0.0762, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0750 (MSE:0.0750, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 157504.6719 (MSE:157504.6719, Reg:157504.5938) beta=20.00

Iter  2975 | Total loss: 0.0804 (MSE:0.0804, Reg:0.0000) beta=17.81
    Early stopped

    Set the rounding value

[39/50] encoder.layers.9.self_attention.out_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0080 (MSE:0.0080, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 44429.2070 (MSE:44429.2070, Reg:44429.1992) beta=20.00

Iter  2937 | Total loss: 0.0076 (MSE:0.0076, Reg:0.0000) beta=17.89
    Early stopped

    Set the rounding value

[40/50] encoder.layers.9.mlp.linear_1
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.2108 (MSE:0.2108, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.2113 (MSE:0.2113, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 369340.6562 (MSE:369340.6562, Reg:369340.4375) beta=20.00

Iter  2985 | Total loss: 0.2092 (MSE:0.2092, Reg:0.0000) beta=17.78
    Early stopped

    Set the rounding value

[41/50] encoder.layers.9.mlp.linear_2
 INPUT_FP : torch.Size([1024, 197, 3072])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0229 (MSE:0.0229, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0222 (MSE:0.0222, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 317165.7812 (MSE:317165.7812, Reg:317165.7500) beta=20.00

Iter  2996 | Total loss: 0.0214 (MSE:0.0214, Reg:0.0000) beta=17.76
    Early stopped

    Set the rounding value

[42/50] encoder.layers.10.self_attention.in_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0815 (MSE:0.0815, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0798 (MSE:0.0798, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 169284.8750 (MSE:169284.8750, Reg:169284.7969) beta=20.00

Iter  2979 | Total loss: 0.0802 (MSE:0.0802, Reg:0.0000) beta=17.80
    Early stopped

    Set the rounding value

[43/50] encoder.layers.10.self_attention.out_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 34034.9258 (MSE:34034.9258, Reg:34034.9219) beta=20.00

Iter  2877 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=18.03
    Early stopped

    Set the rounding value

[44/50] encoder.layers.10.mlp.linear_1
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1907 (MSE:0.1907, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.1897 (MSE:0.1897, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 344173.5938 (MSE:344173.5938, Reg:344173.4062) beta=20.00

Iter  2979 | Total loss: 0.1839 (MSE:0.1839, Reg:0.0000) beta=17.80
    Early stopped

    Set the rounding value

[45/50] encoder.layers.10.mlp.linear_2
 INPUT_FP : torch.Size([1024, 197, 3072])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1227 (MSE:0.1227, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.1138 (MSE:0.1138, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 390600.5625 (MSE:390600.5625, Reg:390600.4375) beta=20.00

Iter  3000 | Total loss: 0.3825 (MSE:0.3825, Reg:0.2603) beta=17.75

Iter  3070 | Total loss: 0.1102 (MSE:0.1102, Reg:0.0000) beta=17.59
    Early stopped

    Set the rounding value

[46/50] encoder.layers.11.self_attention.in_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 2304])
    V   : , torch.Size([2304, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1260 (MSE:0.1260, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.1243 (MSE:0.1243, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 189363.8750 (MSE:189363.8750, Reg:189363.7500) beta=20.00

Iter  2946 | Total loss: 0.1262 (MSE:0.1262, Reg:0.0000) beta=17.87
    Early stopped

    Set the rounding value

[47/50] encoder.layers.11.self_attention.out_proj
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0158 (MSE:0.0158, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0134 (MSE:0.0134, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 59283.8047 (MSE:59283.8047, Reg:59283.7891) beta=20.00

Iter  2968 | Total loss: 0.0169 (MSE:0.0169, Reg:0.0000) beta=17.82
    Early stopped

    Set the rounding value

[48/50] encoder.layers.11.mlp.linear_1
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 3072])
    V   : , torch.Size([3072, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.2113 (MSE:0.2113, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.2125 (MSE:0.2125, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 372820.9688 (MSE:372820.9688, Reg:372820.7812) beta=20.00

Iter  2955 | Total loss: 0.2189 (MSE:0.2189, Reg:0.0000) beta=17.85
    Early stopped

    Set the rounding value

[49/50] encoder.layers.11.mlp.linear_2
 INPUT_FP : torch.Size([1024, 197, 3072])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : , torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.1063 (MSE:0.1063, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0912 (MSE:0.0912, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 240907.9531 (MSE:240907.9531, Reg:240907.8594) beta=20.00

Iter  2988 | Total loss: 0.1026 (MSE:0.1026, Reg:0.0000) beta=17.78
    Early stopped

    Set the rounding value

[50/50] heads.0
 INPUT_FP : torch.Size([1024, 768])
 OUTPUT_FP : torch.Size([1024, 1000])
    V   : , torch.Size([1000, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.3952 (MSE:0.3952, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.4013 (MSE:0.4013, Reg:0.0000) beta=0.00

Iter  2000 | Total loss: 72899.2734 (MSE:72899.2734, Reg:72898.8125) beta=20.00

Iter  2969 | Total loss: 0.4512 (MSE:0.4512, Reg:0.0000) beta=17.82
    Early stopped

    Set the rounding value

50
AdaRound for PerLayer weights is done.

    Quantized model Evaluation accuracy on 50000 images, 72.964%
Total time: 1930.38 sec
