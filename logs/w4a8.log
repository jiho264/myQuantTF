
- main_args params:
    - arch: ViT_B_16
    - batch_size: 128
    - num_samples: 1024

- weight params:
    - scheme: AbsMaxQuantizer
    - bit_width: 4
    - per_channel: True

- activation params:
    - scheme: MovAvgAbsMaxQuantizer
    - bit_width: 8
    - per_channel: False
    - momentum: 0.95
    - batches: 16

- softmax params:

- Activation of Softmax(Q@K/d_K) (attn_map) : UINT8


- layer_norm params:

- Identity addition : INT16 (The input of each LayerNorm)

- gelu params:
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Activation calibration is done.


    Quantized model Evaluation accuracy on 50000 images, 76.894%
Total time: 595.66 sec
