
- main_args params:
    - arch: ViT_B_16
    - batch_size: 128
    - num_samples: 1024

- weight params:
    - scheme: AbsMaxQuantizer
    - bit_width: 4
    - per_channel: True
    - AdaRound: PerBlock

- activation params:
    - scheme: MovAvgAbsMaxQuantizer
    - bit_width: 8
    - per_channel: False
    - momentum: 0.95
    - batches: 16
    - Identity addition : INT16 (The input of each LayerNorm)

- softmax params:
    - bit_width: 16
    - Activation of Softmax(Q@K/d_K) (attn_map) : UINT8

- layer_norm params:
    - bit_width: 8

- gelu params:
    - bit_width: 8

Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
    Initiated the V
Activation calibration is done.

[1/26] conv_proj
    INPUT_FP : torch.Size([1024, 3, 224, 224])
    OUTPUT_FP : torch.Size([1024, 768, 14, 14])
    V   : , torch.Size([768, 3, 16, 16])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 119101.3672 (MSE:0.0005, Reg:119101.3672) beta=20.00
Iter  1000 | Total loss: 16.0032 (MSE:0.0032, Reg:16.0000) beta=19.05
Iter  2000 | Total loss: 7.0030 (MSE:0.0030, Reg:7.0000) beta=17.16
Iter  3000 | Total loss: 5.9877 (MSE:0.0032, Reg:5.9844) beta=15.26
Iter  4000 | Total loss: 2.0037 (MSE:0.0037, Reg:2.0000) beta=13.37
Iter  4906 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=11.65
    Early stopped
    Set the rounding value
    s_a : act_quant, torch.Size([]), Parameter containing:
tensor(0.0539, device='cuda:0', requires_grad=True)

[2/26] encoder.layers.0.self_attention
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : in_proj, torch.Size([2304, 768])
    V   : out_proj, torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 212175.8438 (MSE:0.0001, Reg:212175.8438) beta=20.00
Iter  1000 | Total loss: 4885.9414 (MSE:0.0001, Reg:4885.9414) beta=19.05
Iter  2000 | Total loss: 2378.4814 (MSE:0.0001, Reg:2378.4814) beta=17.16
Iter  3000 | Total loss: 1260.4618 (MSE:0.0001, Reg:1260.4617) beta=15.26
Iter  4000 | Total loss: 650.8584 (MSE:0.0001, Reg:650.8583) beta=13.37
Iter  5000 | Total loss: 260.6978 (MSE:0.0001, Reg:260.6977) beta=11.47
Iter  6000 | Total loss: 59.8191 (MSE:0.0001, Reg:59.8190) beta=9.58
Iter  7000 | Total loss: 13.7671 (MSE:0.0001, Reg:13.7670) beta=7.68
Iter  8000 | Total loss: 2.0001 (MSE:0.0001, Reg:2.0000) beta=5.79
Iter  8535 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=4.78
    Early stopped
    Set the rounding value
    s_a : in_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0258, device='cuda:0', requires_grad=True)
    s_a : qk_act, torch.Size([]), Parameter containing:
tensor(0.1385, device='cuda:0', requires_grad=True)
    s_a : softmax_act, torch.Size([]), Parameter containing:
tensor(0.0039, device='cuda:0', requires_grad=True)
    s_a : attnout_act, torch.Size([]), Parameter containing:
tensor(0.0117, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : out_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0125, device='cuda:0', requires_grad=True)

[3/26] encoder.layers.0.mlp
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : linear_1, torch.Size([3072, 768])
    V   : linear_2, torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0216 (MSE:0.0216, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 766733.9375 (MSE:0.0025, Reg:766733.9375) beta=20.00
Iter  1000 | Total loss: 16789.5430 (MSE:0.0027, Reg:16789.5391) beta=19.05
Iter  2000 | Total loss: 5895.7461 (MSE:0.0029, Reg:5895.7432) beta=17.16
Iter  3000 | Total loss: 3020.2637 (MSE:0.0028, Reg:3020.2607) beta=15.26
Iter  4000 | Total loss: 1630.2756 (MSE:0.0033, Reg:1630.2725) beta=13.37
Iter  5000 | Total loss: 797.5153 (MSE:0.0033, Reg:797.5119) beta=11.47
Iter  6000 | Total loss: 277.7504 (MSE:0.0029, Reg:277.7476) beta=9.58
Iter  7000 | Total loss: 35.4673 (MSE:0.0028, Reg:35.4644) beta=7.68
Iter  7722 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=6.32
    Early stopped
    Set the rounding value
    s_a : linear_1.act_quant, torch.Size([]), Parameter containing:
tensor(0.0633, device='cuda:0', requires_grad=True)
    s_a : gelu_act, torch.Size([]), Parameter containing:
tensor(0.0606, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : linear_2.act_quant, torch.Size([]), Parameter containing:
tensor(0.0318, device='cuda:0', requires_grad=True)

[4/26] encoder.layers.1.self_attention
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : in_proj, torch.Size([2304, 768])
    V   : out_proj, torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 192298.4531 (MSE:0.0003, Reg:192298.4531) beta=20.00
Iter  1000 | Total loss: 1268.0837 (MSE:0.0003, Reg:1268.0834) beta=19.05
Iter  2000 | Total loss: 359.2510 (MSE:0.0002, Reg:359.2508) beta=17.16
Iter  3000 | Total loss: 184.6551 (MSE:0.0002, Reg:184.6548) beta=15.26
Iter  4000 | Total loss: 92.7437 (MSE:0.0003, Reg:92.7434) beta=13.37
Iter  5000 | Total loss: 41.0003 (MSE:0.0003, Reg:41.0000) beta=11.47
Iter  6000 | Total loss: 18.0003 (MSE:0.0003, Reg:18.0000) beta=9.58
Iter  7000 | Total loss: 1.0003 (MSE:0.0003, Reg:1.0000) beta=7.68
Iter  7046 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=7.60
    Early stopped
    Set the rounding value
    s_a : in_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0307, device='cuda:0', requires_grad=True)
    s_a : qk_act, torch.Size([]), Parameter containing:
tensor(0.1225, device='cuda:0', requires_grad=True)
    s_a : softmax_act, torch.Size([]), Parameter containing:
tensor(0.0039, device='cuda:0', requires_grad=True)
    s_a : attnout_act, torch.Size([]), Parameter containing:
tensor(0.0117, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : out_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0084, device='cuda:0', requires_grad=True)

[5/26] encoder.layers.1.mlp
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : linear_1, torch.Size([3072, 768])
    V   : linear_2, torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0070 (MSE:0.0070, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 896393.5000 (MSE:0.0024, Reg:896393.5000) beta=20.00
Iter  1000 | Total loss: 2045.0820 (MSE:0.0040, Reg:2045.0780) beta=19.05
Iter  2000 | Total loss: 580.5795 (MSE:0.0039, Reg:580.5757) beta=17.16
Iter  3000 | Total loss: 317.1646 (MSE:0.0040, Reg:317.1607) beta=15.26
Iter  4000 | Total loss: 196.0032 (MSE:0.0038, Reg:195.9995) beta=13.37
Iter  5000 | Total loss: 101.8471 (MSE:0.0047, Reg:101.8424) beta=11.47
Iter  6000 | Total loss: 26.2584 (MSE:0.0039, Reg:26.2545) beta=9.58
Iter  7000 | Total loss: 2.8584 (MSE:0.0046, Reg:2.8538) beta=7.68
Iter  7264 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=7.18
    Early stopped
    Set the rounding value
    s_a : linear_1.act_quant, torch.Size([]), Parameter containing:
tensor(0.0666, device='cuda:0', requires_grad=True)
    s_a : gelu_act, torch.Size([]), Parameter containing:
tensor(0.0377, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : linear_2.act_quant, torch.Size([]), Parameter containing:
tensor(0.0215, device='cuda:0', requires_grad=True)

[6/26] encoder.layers.2.self_attention
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : in_proj, torch.Size([2304, 768])
    V   : out_proj, torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 328450.2812 (MSE:0.0010, Reg:328450.2812) beta=20.00
Iter  1000 | Total loss: 1833.9055 (MSE:0.0013, Reg:1833.9042) beta=19.05
Iter  2000 | Total loss: 613.9628 (MSE:0.0012, Reg:613.9615) beta=17.16
Iter  3000 | Total loss: 332.9427 (MSE:0.0011, Reg:332.9416) beta=15.26
Iter  4000 | Total loss: 178.7720 (MSE:0.0013, Reg:178.7707) beta=13.37
Iter  5000 | Total loss: 95.6567 (MSE:0.0011, Reg:95.6556) beta=11.47
Iter  6000 | Total loss: 37.8330 (MSE:0.0012, Reg:37.8318) beta=9.58
Iter  7000 | Total loss: 5.0012 (MSE:0.0012, Reg:5.0000) beta=7.68
Iter  7702 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=6.35
    Early stopped
    Set the rounding value
    s_a : in_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0445, device='cuda:0', requires_grad=True)
    s_a : qk_act, torch.Size([]), Parameter containing:
tensor(0.1450, device='cuda:0', requires_grad=True)
    s_a : softmax_act, torch.Size([]), Parameter containing:
tensor(0.0039, device='cuda:0', requires_grad=True)
    s_a : attnout_act, torch.Size([]), Parameter containing:
tensor(0.0131, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : out_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0088, device='cuda:0', requires_grad=True)

[7/26] encoder.layers.2.mlp
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : linear_1, torch.Size([3072, 768])
    V   : linear_2, torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0121 (MSE:0.0121, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 976486.1250 (MSE:0.0053, Reg:976486.1250) beta=20.00
Iter  1000 | Total loss: 4340.6519 (MSE:0.0074, Reg:4340.6445) beta=19.05
Iter  2000 | Total loss: 1412.8962 (MSE:0.0077, Reg:1412.8885) beta=17.16
Iter  3000 | Total loss: 833.4022 (MSE:0.0068, Reg:833.3954) beta=15.26
Iter  4000 | Total loss: 550.2197 (MSE:0.0072, Reg:550.2125) beta=13.37
Iter  5000 | Total loss: 255.2027 (MSE:0.0073, Reg:255.1954) beta=11.47
Iter  6000 | Total loss: 66.0080 (MSE:0.0081, Reg:65.9999) beta=9.58
Iter  7000 | Total loss: 9.7873 (MSE:0.0076, Reg:9.7797) beta=7.68
Iter  7658 | Total loss: 0.0074 (MSE:0.0074, Reg:0.0000) beta=6.44
    Early stopped
    Set the rounding value
    s_a : linear_1.act_quant, torch.Size([]), Parameter containing:
tensor(0.0790, device='cuda:0', requires_grad=True)
    s_a : gelu_act, torch.Size([]), Parameter containing:
tensor(0.0249, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : linear_2.act_quant, torch.Size([]), Parameter containing:
tensor(0.0234, device='cuda:0', requires_grad=True)

[8/26] encoder.layers.3.self_attention
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : in_proj, torch.Size([2304, 768])
    V   : out_proj, torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 425343.9375 (MSE:0.0024, Reg:425343.9375) beta=20.00
Iter  1000 | Total loss: 3938.7739 (MSE:0.0023, Reg:3938.7717) beta=19.05
Iter  2000 | Total loss: 1235.4844 (MSE:0.0022, Reg:1235.4822) beta=17.16
Iter  3000 | Total loss: 633.7007 (MSE:0.0022, Reg:633.6985) beta=15.26
Iter  4000 | Total loss: 364.3306 (MSE:0.0022, Reg:364.3284) beta=13.37
Iter  5000 | Total loss: 165.1053 (MSE:0.0022, Reg:165.1030) beta=11.47
Iter  6000 | Total loss: 59.0013 (MSE:0.0019, Reg:58.9994) beta=9.58
Iter  7000 | Total loss: 6.0023 (MSE:0.0023, Reg:6.0000) beta=7.68
Iter  7495 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=6.75
    Early stopped
    Set the rounding value
    s_a : in_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0453, device='cuda:0', requires_grad=True)
    s_a : qk_act, torch.Size([]), Parameter containing:
tensor(0.1476, device='cuda:0', requires_grad=True)
    s_a : softmax_act, torch.Size([]), Parameter containing:
tensor(0.0039, device='cuda:0', requires_grad=True)
    s_a : attnout_act, torch.Size([]), Parameter containing:
tensor(0.0129, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : out_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0113, device='cuda:0', requires_grad=True)

[9/26] encoder.layers.3.mlp
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : linear_1, torch.Size([3072, 768])
    V   : linear_2, torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0074 (MSE:0.0074, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 787195.2500 (MSE:0.0054, Reg:787195.2500) beta=20.00
Iter  1000 | Total loss: 2190.5281 (MSE:0.0063, Reg:2190.5217) beta=19.05
Iter  2000 | Total loss: 620.1400 (MSE:0.0058, Reg:620.1342) beta=17.16
Iter  3000 | Total loss: 353.4868 (MSE:0.0061, Reg:353.4807) beta=15.26
Iter  4000 | Total loss: 221.8713 (MSE:0.0064, Reg:221.8649) beta=13.37
Iter  5000 | Total loss: 101.0039 (MSE:0.0062, Reg:100.9977) beta=11.47
Iter  6000 | Total loss: 49.0060 (MSE:0.0061, Reg:48.9999) beta=9.58
Iter  7000 | Total loss: 2.0062 (MSE:0.0062, Reg:2.0000) beta=7.68
Iter  7376 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=6.97
    Early stopped
    Set the rounding value
    s_a : linear_1.act_quant, torch.Size([]), Parameter containing:
tensor(0.0613, device='cuda:0', requires_grad=True)
    s_a : gelu_act, torch.Size([]), Parameter containing:
tensor(0.0272, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : linear_2.act_quant, torch.Size([]), Parameter containing:
tensor(0.0128, device='cuda:0', requires_grad=True)

[10/26] encoder.layers.4.self_attention
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : in_proj, torch.Size([2304, 768])
    V   : out_proj, torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 464464.5625 (MSE:0.0032, Reg:464464.5625) beta=20.00
Iter  1000 | Total loss: 2963.5273 (MSE:0.0033, Reg:2963.5239) beta=19.05
Iter  2000 | Total loss: 926.3737 (MSE:0.0035, Reg:926.3701) beta=17.16
Iter  3000 | Total loss: 434.6064 (MSE:0.0035, Reg:434.6029) beta=15.26
Iter  4000 | Total loss: 210.9893 (MSE:0.0034, Reg:210.9859) beta=13.37
Iter  5000 | Total loss: 95.0409 (MSE:0.0036, Reg:95.0373) beta=11.47
Iter  6000 | Total loss: 28.8176 (MSE:0.0034, Reg:28.8142) beta=9.58
Iter  7000 | Total loss: 2.0036 (MSE:0.0036, Reg:2.0000) beta=7.68
Iter  7412 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=6.90
    Early stopped
    Set the rounding value
    s_a : in_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0447, device='cuda:0', requires_grad=True)
    s_a : qk_act, torch.Size([]), Parameter containing:
tensor(0.0924, device='cuda:0', requires_grad=True)
    s_a : softmax_act, torch.Size([]), Parameter containing:
tensor(0.0037, device='cuda:0', requires_grad=True)
    s_a : attnout_act, torch.Size([]), Parameter containing:
tensor(0.0121, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : out_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0112, device='cuda:0', requires_grad=True)

[11/26] encoder.layers.4.mlp
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : linear_1, torch.Size([3072, 768])
    V   : linear_2, torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0100 (MSE:0.0100, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 761085.7500 (MSE:0.0076, Reg:761085.7500) beta=20.00
Iter  1000 | Total loss: 2454.5542 (MSE:0.0076, Reg:2454.5466) beta=19.05
Iter  2000 | Total loss: 744.6153 (MSE:0.0084, Reg:744.6069) beta=17.16
Iter  3000 | Total loss: 336.4024 (MSE:0.0075, Reg:336.3950) beta=15.26
Iter  4000 | Total loss: 185.5559 (MSE:0.0086, Reg:185.5473) beta=13.37
Iter  5000 | Total loss: 104.5785 (MSE:0.0080, Reg:104.5705) beta=11.47
Iter  6000 | Total loss: 44.0080 (MSE:0.0081, Reg:43.9999) beta=9.58
Iter  7000 | Total loss: 8.7612 (MSE:0.0076, Reg:8.7536) beta=7.68
Iter  7378 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=6.97
    Early stopped
    Set the rounding value
    s_a : linear_1.act_quant, torch.Size([]), Parameter containing:
tensor(0.0533, device='cuda:0', requires_grad=True)
    s_a : gelu_act, torch.Size([]), Parameter containing:
tensor(0.0333, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : linear_2.act_quant, torch.Size([]), Parameter containing:
tensor(0.0628, device='cuda:0', requires_grad=True)

[12/26] encoder.layers.5.self_attention
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : in_proj, torch.Size([2304, 768])
    V   : out_proj, torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0079 (MSE:0.0079, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 461308.5625 (MSE:0.0034, Reg:461308.5625) beta=20.00
Iter  1000 | Total loss: 1357.4786 (MSE:0.0044, Reg:1357.4742) beta=19.05
Iter  2000 | Total loss: 387.6193 (MSE:0.0048, Reg:387.6146) beta=17.16
Iter  3000 | Total loss: 191.7762 (MSE:0.0046, Reg:191.7716) beta=15.26
Iter  4000 | Total loss: 96.0039 (MSE:0.0044, Reg:95.9995) beta=13.37
Iter  5000 | Total loss: 34.0042 (MSE:0.0042, Reg:34.0000) beta=11.47
Iter  6000 | Total loss: 12.6410 (MSE:0.0048, Reg:12.6363) beta=9.58
Iter  6995 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=7.69
    Early stopped
    Set the rounding value
    s_a : in_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0408, device='cuda:0', requires_grad=True)
    s_a : qk_act, torch.Size([]), Parameter containing:
tensor(0.0877, device='cuda:0', requires_grad=True)
    s_a : softmax_act, torch.Size([]), Parameter containing:
tensor(0.0036, device='cuda:0', requires_grad=True)
    s_a : attnout_act, torch.Size([]), Parameter containing:
tensor(0.0098, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : out_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0117, device='cuda:0', requires_grad=True)

[13/26] encoder.layers.5.mlp
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : linear_1, torch.Size([3072, 768])
    V   : linear_2, torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0585 (MSE:0.0585, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 1019386.6250 (MSE:0.0415, Reg:1019386.6250) beta=20.00
Iter  1000 | Total loss: 3493.4180 (MSE:0.0446, Reg:3493.3733) beta=19.05
Iter  2000 | Total loss: 1002.4001 (MSE:0.0438, Reg:1002.3563) beta=17.16
Iter  3000 | Total loss: 485.4160 (MSE:0.0468, Reg:485.3692) beta=15.26
Iter  4000 | Total loss: 243.7858 (MSE:0.0468, Reg:243.7389) beta=13.37
Iter  5000 | Total loss: 107.2456 (MSE:0.0480, Reg:107.1976) beta=11.47
Iter  6000 | Total loss: 27.7336 (MSE:0.0493, Reg:27.6844) beta=9.58
Iter  6715 | Total loss: 0.0528 (MSE:0.0528, Reg:0.0000) beta=8.22
    Early stopped
    Set the rounding value
    s_a : linear_1.act_quant, torch.Size([]), Parameter containing:
tensor(0.1109, device='cuda:0', requires_grad=True)
    s_a : gelu_act, torch.Size([]), Parameter containing:
tensor(0.1080, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : linear_2.act_quant, torch.Size([]), Parameter containing:
tensor(0.2151, device='cuda:0', requires_grad=True)

[14/26] encoder.layers.6.self_attention
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : in_proj, torch.Size([2304, 768])
    V   : out_proj, torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0093 (MSE:0.0093, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 474235.2188 (MSE:0.0060, Reg:474235.2188) beta=20.00
Iter  1000 | Total loss: 1749.3479 (MSE:0.0064, Reg:1749.3416) beta=19.05
Iter  2000 | Total loss: 453.1930 (MSE:0.0065, Reg:453.1865) beta=17.16
Iter  3000 | Total loss: 218.9100 (MSE:0.0066, Reg:218.9034) beta=15.26
Iter  4000 | Total loss: 114.2210 (MSE:0.0064, Reg:114.2146) beta=13.37
Iter  5000 | Total loss: 51.0060 (MSE:0.0060, Reg:51.0000) beta=11.47
Iter  6000 | Total loss: 13.0067 (MSE:0.0067, Reg:13.0000) beta=9.58
Iter  6958 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=7.76
    Early stopped
    Set the rounding value
    s_a : in_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0434, device='cuda:0', requires_grad=True)
    s_a : qk_act, torch.Size([]), Parameter containing:
tensor(0.0895, device='cuda:0', requires_grad=True)
    s_a : softmax_act, torch.Size([]), Parameter containing:
tensor(0.0036, device='cuda:0', requires_grad=True)
    s_a : attnout_act, torch.Size([]), Parameter containing:
tensor(0.0140, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : out_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0118, device='cuda:0', requires_grad=True)

[15/26] encoder.layers.6.mlp
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : linear_1, torch.Size([3072, 768])
    V   : linear_2, torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0130 (MSE:0.0130, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 736159.0000 (MSE:0.0094, Reg:736159.0000) beta=20.00
Iter  1000 | Total loss: 995.7029 (MSE:0.0114, Reg:995.6915) beta=19.05
Iter  2000 | Total loss: 275.3390 (MSE:0.0106, Reg:275.3284) beta=17.16
Iter  3000 | Total loss: 158.0151 (MSE:0.0112, Reg:158.0039) beta=15.26
Iter  4000 | Total loss: 84.9931 (MSE:0.0114, Reg:84.9817) beta=13.37
Iter  5000 | Total loss: 37.0113 (MSE:0.0113, Reg:37.0000) beta=11.47
Iter  6000 | Total loss: 8.0105 (MSE:0.0105, Reg:8.0000) beta=9.58
Iter  6894 | Total loss: 0.0109 (MSE:0.0109, Reg:0.0000) beta=7.89
    Early stopped
    Set the rounding value
    s_a : linear_1.act_quant, torch.Size([]), Parameter containing:
tensor(0.0579, device='cuda:0', requires_grad=True)
    s_a : gelu_act, torch.Size([]), Parameter containing:
tensor(0.0364, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : linear_2.act_quant, torch.Size([]), Parameter containing:
tensor(0.0136, device='cuda:0', requires_grad=True)

[16/26] encoder.layers.7.self_attention
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : in_proj, torch.Size([2304, 768])
    V   : out_proj, torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 458075.0000 (MSE:0.0045, Reg:458075.0000) beta=20.00
Iter  1000 | Total loss: 623.3011 (MSE:0.0051, Reg:623.2960) beta=19.05
Iter  2000 | Total loss: 161.9145 (MSE:0.0053, Reg:161.9092) beta=17.16
Iter  3000 | Total loss: 77.0053 (MSE:0.0053, Reg:77.0000) beta=15.26
Iter  4000 | Total loss: 40.0054 (MSE:0.0054, Reg:40.0000) beta=13.37
Iter  5000 | Total loss: 17.9357 (MSE:0.0058, Reg:17.9299) beta=11.47
Iter  6000 | Total loss: 5.0051 (MSE:0.0051, Reg:5.0000) beta=9.58
Iter  6664 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=8.32
    Early stopped
    Set the rounding value
    s_a : in_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0419, device='cuda:0', requires_grad=True)
    s_a : qk_act, torch.Size([]), Parameter containing:
tensor(0.0946, device='cuda:0', requires_grad=True)
    s_a : softmax_act, torch.Size([]), Parameter containing:
tensor(0.0037, device='cuda:0', requires_grad=True)
    s_a : attnout_act, torch.Size([]), Parameter containing:
tensor(0.0137, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : out_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0110, device='cuda:0', requires_grad=True)

[17/26] encoder.layers.7.mlp
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : linear_1, torch.Size([3072, 768])
    V   : linear_2, torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0132 (MSE:0.0132, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 708855.1250 (MSE:0.0132, Reg:708855.1250) beta=20.00
Iter  1000 | Total loss: 1097.8438 (MSE:0.0136, Reg:1097.8301) beta=19.05
Iter  2000 | Total loss: 345.1622 (MSE:0.0143, Reg:345.1479) beta=17.16
Iter  3000 | Total loss: 173.0104 (MSE:0.0123, Reg:172.9981) beta=15.26
Iter  4000 | Total loss: 90.9505 (MSE:0.0134, Reg:90.9370) beta=13.37
Iter  5000 | Total loss: 44.0124 (MSE:0.0124, Reg:44.0000) beta=11.47
Iter  6000 | Total loss: 11.5328 (MSE:0.0136, Reg:11.5192) beta=9.58
Iter  6808 | Total loss: 0.0137 (MSE:0.0137, Reg:0.0000) beta=8.05
    Early stopped
    Set the rounding value
    s_a : linear_1.act_quant, torch.Size([]), Parameter containing:
tensor(0.0593, device='cuda:0', requires_grad=True)
    s_a : gelu_act, torch.Size([]), Parameter containing:
tensor(0.0393, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : linear_2.act_quant, torch.Size([]), Parameter containing:
tensor(0.0209, device='cuda:0', requires_grad=True)

[18/26] encoder.layers.8.self_attention
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : in_proj, torch.Size([2304, 768])
    V   : out_proj, torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 412677.9375 (MSE:0.0043, Reg:412677.9375) beta=20.00
Iter  1000 | Total loss: 310.2472 (MSE:0.0043, Reg:310.2429) beta=19.05
Iter  2000 | Total loss: 71.4539 (MSE:0.0049, Reg:71.4490) beta=17.16
Iter  3000 | Total loss: 32.0047 (MSE:0.0047, Reg:32.0000) beta=15.26
Iter  4000 | Total loss: 12.2520 (MSE:0.0051, Reg:12.2469) beta=13.37
Iter  5000 | Total loss: 9.0046 (MSE:0.0046, Reg:9.0000) beta=11.47
Iter  6000 | Total loss: 2.0049 (MSE:0.0049, Reg:2.0000) beta=9.58
Iter  6517 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=8.60
    Early stopped
    Set the rounding value
    s_a : in_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0402, device='cuda:0', requires_grad=True)
    s_a : qk_act, torch.Size([]), Parameter containing:
tensor(0.0915, device='cuda:0', requires_grad=True)
    s_a : softmax_act, torch.Size([]), Parameter containing:
tensor(0.0037, device='cuda:0', requires_grad=True)
    s_a : attnout_act, torch.Size([]), Parameter containing:
tensor(0.0128, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : out_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0157, device='cuda:0', requires_grad=True)

[19/26] encoder.layers.8.mlp
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : linear_1, torch.Size([3072, 768])
    V   : linear_2, torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0127 (MSE:0.0127, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 735834.6250 (MSE:0.0094, Reg:735834.6250) beta=20.00
Iter  1000 | Total loss: 608.0978 (MSE:0.0119, Reg:608.0859) beta=19.05
Iter  2000 | Total loss: 201.0118 (MSE:0.0126, Reg:200.9992) beta=17.16
Iter  3000 | Total loss: 121.0624 (MSE:0.0117, Reg:121.0507) beta=15.26
Iter  4000 | Total loss: 78.2987 (MSE:0.0119, Reg:78.2868) beta=13.37
Iter  5000 | Total loss: 44.8521 (MSE:0.0119, Reg:44.8402) beta=11.47
Iter  6000 | Total loss: 12.9980 (MSE:0.0128, Reg:12.9852) beta=9.58
Iter  7000 | Total loss: 1.0125 (MSE:0.0128, Reg:0.9998) beta=7.68
Iter  7029 | Total loss: 0.0121 (MSE:0.0121, Reg:0.0000) beta=7.63
    Early stopped
    Set the rounding value
    s_a : linear_1.act_quant, torch.Size([]), Parameter containing:
tensor(0.0585, device='cuda:0', requires_grad=True)
    s_a : gelu_act, torch.Size([]), Parameter containing:
tensor(0.0365, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : linear_2.act_quant, torch.Size([]), Parameter containing:
tensor(0.0222, device='cuda:0', requires_grad=True)

[20/26] encoder.layers.9.self_attention
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : in_proj, torch.Size([2304, 768])
    V   : out_proj, torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 364666.6562 (MSE:0.0016, Reg:364666.6562) beta=20.00
Iter  1000 | Total loss: 639.0830 (MSE:0.0024, Reg:639.0806) beta=19.05
Iter  2000 | Total loss: 199.0023 (MSE:0.0024, Reg:198.9999) beta=17.16
Iter  3000 | Total loss: 100.0025 (MSE:0.0025, Reg:100.0000) beta=15.26
Iter  4000 | Total loss: 50.7352 (MSE:0.0022, Reg:50.7329) beta=13.37
Iter  5000 | Total loss: 16.0023 (MSE:0.0023, Reg:16.0000) beta=11.47
Iter  6000 | Total loss: 4.0026 (MSE:0.0026, Reg:4.0000) beta=9.58
Iter  6497 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=8.64
    Early stopped
    Set the rounding value
    s_a : in_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0304, device='cuda:0', requires_grad=True)
    s_a : qk_act, torch.Size([]), Parameter containing:
tensor(0.0843, device='cuda:0', requires_grad=True)
    s_a : softmax_act, torch.Size([]), Parameter containing:
tensor(0.0032, device='cuda:0', requires_grad=True)
    s_a : attnout_act, torch.Size([]), Parameter containing:
tensor(0.0095, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : out_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0126, device='cuda:0', requires_grad=True)

[21/26] encoder.layers.9.mlp
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : linear_1, torch.Size([3072, 768])
    V   : linear_2, torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0115 (MSE:0.0115, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 654890.2500 (MSE:0.0082, Reg:654890.2500) beta=20.00
Iter  1000 | Total loss: 907.0297 (MSE:0.0107, Reg:907.0190) beta=19.05
Iter  2000 | Total loss: 346.7956 (MSE:0.0101, Reg:346.7855) beta=17.16
Iter  3000 | Total loss: 212.0109 (MSE:0.0111, Reg:211.9999) beta=15.26
Iter  4000 | Total loss: 133.4607 (MSE:0.0112, Reg:133.4495) beta=13.37
Iter  5000 | Total loss: 79.0089 (MSE:0.0110, Reg:78.9979) beta=11.47
Iter  6000 | Total loss: 22.0109 (MSE:0.0109, Reg:22.0000) beta=9.58
Iter  7000 | Total loss: 3.0093 (MSE:0.0099, Reg:2.9994) beta=7.68
Iter  7157 | Total loss: 0.0105 (MSE:0.0105, Reg:0.0000) beta=7.39
    Early stopped
    Set the rounding value
    s_a : linear_1.act_quant, torch.Size([]), Parameter containing:
tensor(0.0538, device='cuda:0', requires_grad=True)
    s_a : gelu_act, torch.Size([]), Parameter containing:
tensor(0.0463, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : linear_2.act_quant, torch.Size([]), Parameter containing:
tensor(0.0298, device='cuda:0', requires_grad=True)

[22/26] encoder.layers.10.self_attention
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : in_proj, torch.Size([2304, 768])
    V   : out_proj, torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 384007.0000 (MSE:0.0025, Reg:384007.0000) beta=20.00
Iter  1000 | Total loss: 127.6485 (MSE:0.0022, Reg:127.6463) beta=19.05
Iter  2000 | Total loss: 36.1501 (MSE:0.0020, Reg:36.1481) beta=17.16
Iter  3000 | Total loss: 23.0021 (MSE:0.0021, Reg:23.0000) beta=15.26
Iter  4000 | Total loss: 11.0023 (MSE:0.0023, Reg:11.0000) beta=13.37
Iter  5000 | Total loss: 10.0022 (MSE:0.0022, Reg:10.0000) beta=11.47
Iter  5840 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=9.88
    Early stopped
    Set the rounding value
    s_a : in_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0283, device='cuda:0', requires_grad=True)
    s_a : qk_act, torch.Size([]), Parameter containing:
tensor(0.0716, device='cuda:0', requires_grad=True)
    s_a : softmax_act, torch.Size([]), Parameter containing:
tensor(0.0024, device='cuda:0', requires_grad=True)
    s_a : attnout_act, torch.Size([]), Parameter containing:
tensor(0.0075, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : out_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0106, device='cuda:0', requires_grad=True)

[23/26] encoder.layers.10.mlp
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : linear_1, torch.Size([3072, 768])
    V   : linear_2, torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0755 (MSE:0.0755, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 1061952.8750 (MSE:0.0580, Reg:1061952.7500) beta=20.00
Iter  1000 | Total loss: 4632.0703 (MSE:0.0460, Reg:4632.0244) beta=19.05
Iter  2000 | Total loss: 1290.9924 (MSE:0.0513, Reg:1290.9412) beta=17.16
Iter  3000 | Total loss: 536.6309 (MSE:0.0470, Reg:536.5839) beta=15.26
Iter  4000 | Total loss: 226.3147 (MSE:0.0551, Reg:226.2596) beta=13.37
Iter  5000 | Total loss: 88.0038 (MSE:0.0491, Reg:87.9547) beta=11.47
Iter  6000 | Total loss: 27.0343 (MSE:0.0549, Reg:26.9794) beta=9.58
Iter  6876 | Total loss: 0.0502 (MSE:0.0502, Reg:0.0000) beta=7.92
    Early stopped
    Set the rounding value
    s_a : linear_1.act_quant, torch.Size([]), Parameter containing:
tensor(0.0805, device='cuda:0', requires_grad=True)
    s_a : gelu_act, torch.Size([]), Parameter containing:
tensor(0.0796, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : linear_2.act_quant, torch.Size([]), Parameter containing:
tensor(0.0774, device='cuda:0', requires_grad=True)

[24/26] encoder.layers.11.self_attention
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : in_proj, torch.Size([2304, 768])
    V   : out_proj, torch.Size([768, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 465031.5625 (MSE:0.0037, Reg:465031.5625) beta=20.00
Iter  1000 | Total loss: 254.8754 (MSE:0.0058, Reg:254.8696) beta=19.05
Iter  2000 | Total loss: 50.7271 (MSE:0.0056, Reg:50.7215) beta=17.16
Iter  3000 | Total loss: 19.0012 (MSE:0.0051, Reg:18.9961) beta=15.26
Iter  4000 | Total loss: 4.0065 (MSE:0.0065, Reg:4.0000) beta=13.37
Iter  5000 | Total loss: 1.0055 (MSE:0.0056, Reg:0.9999) beta=11.47
Iter  5026 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=11.42
    Early stopped
    Set the rounding value
    s_a : in_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0394, device='cuda:0', requires_grad=True)
    s_a : qk_act, torch.Size([]), Parameter containing:
tensor(0.0662, device='cuda:0', requires_grad=True)
    s_a : softmax_act, torch.Size([]), Parameter containing:
tensor(0.0032, device='cuda:0', requires_grad=True)
    s_a : attnout_act, torch.Size([]), Parameter containing:
tensor(0.0123, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : out_proj.act_quant, torch.Size([]), Parameter containing:
tensor(0.0347, device='cuda:0', requires_grad=True)

[25/26] encoder.layers.11.mlp
    INPUT_FP : torch.Size([1024, 197, 768])
    OUTPUT_FP : torch.Size([1024, 197, 768])
    V   : linear_1, torch.Size([3072, 768])
    V   : linear_2, torch.Size([768, 3072])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.0953 (MSE:0.0953, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 950367.2500 (MSE:0.0179, Reg:950367.2500) beta=20.00
Iter  1000 | Total loss: 226.0907 (MSE:0.0911, Reg:225.9996) beta=19.05
Iter  2000 | Total loss: 69.7736 (MSE:0.0960, Reg:69.6775) beta=17.16
Iter  3000 | Total loss: 38.7989 (MSE:0.0936, Reg:38.7053) beta=15.26
Iter  4000 | Total loss: 28.0883 (MSE:0.0883, Reg:28.0000) beta=13.37
Iter  5000 | Total loss: 11.0916 (MSE:0.0916, Reg:11.0000) beta=11.47
Iter  6000 | Total loss: 4.0918 (MSE:0.0918, Reg:4.0000) beta=9.58
Iter  7000 | Total loss: 2.0922 (MSE:0.0922, Reg:2.0000) beta=7.68
Iter  7646 | Total loss: 0.0926 (MSE:0.0926, Reg:0.0000) beta=6.46
    Early stopped
    Set the rounding value
    s_a : linear_1.act_quant, torch.Size([]), Parameter containing:
tensor(0.0596, device='cuda:0', requires_grad=True)
    s_a : gelu_act, torch.Size([]), Parameter containing:
tensor(0.0312, device='cuda:0', requires_grad=True)
    Set the rounding value
    s_a : linear_2.act_quant, torch.Size([]), Parameter containing:
tensor(0.0686, device='cuda:0', requires_grad=True)

[26/26] heads.0
    INPUT_FP : torch.Size([1024, 768])
    OUTPUT_FP : torch.Size([1024, 1000])
    V   : , torch.Size([1000, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 10000
Iter     1 | Total loss: 0.2142 (MSE:0.2142, Reg:0.0000) beta=0.00
Iter   500 | Total loss: 210395.3906 (MSE:0.1390, Reg:210395.2500) beta=20.00
Iter  1000 | Total loss: 23759.8359 (MSE:0.1720, Reg:23759.6641) beta=19.05
Iter  2000 | Total loss: 5819.8530 (MSE:0.1560, Reg:5819.6973) beta=17.16
Iter  3000 | Total loss: 2521.9509 (MSE:0.1558, Reg:2521.7952) beta=15.26
Iter  4000 | Total loss: 1011.0977 (MSE:0.1434, Reg:1010.9542) beta=13.37
Iter  5000 | Total loss: 300.6649 (MSE:0.1534, Reg:300.5114) beta=11.47
Iter  6000 | Total loss: 43.7274 (MSE:0.1316, Reg:43.5959) beta=9.58
Iter  7000 | Total loss: 1.1487 (MSE:0.1487, Reg:1.0000) beta=7.68
Iter  7163 | Total loss: 0.1502 (MSE:0.1502, Reg:0.0000) beta=7.38
    Early stopped
    Set the rounding value

AdaRound for PerBlock weights is done.

    Quantized model Evaluation accuracy on 50000 images, 78.484%
Total time: 9130.46 sec
