{'arch': 'ViT_B_16', 'batch_size': 128, 'num_samples': 1024}
2D search with INT4 - [W]+AdaRound[W] conv_proj
2D search with INT8 - [A] conv_proj
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_0.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_0.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_0.self_attention.attnMapActivationQuantizer
2D search with INT8 - [A] encoder.layers.encoder_layer_0.self_attention.attnOutActivationQuantizer
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_0.self_attention.out_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_0.self_attention.out_proj
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_0.mlp.linear_1
2D search with INT8 - [A] encoder.layers.encoder_layer_0.mlp.linear_1
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_0.mlp.linear_2
2D search with INT8 - [A] encoder.layers.encoder_layer_0.mlp.linear_2
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_1.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_1.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_1.self_attention.attnMapActivationQuantizer
2D search with INT8 - [A] encoder.layers.encoder_layer_1.self_attention.attnOutActivationQuantizer
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_1.self_attention.out_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_1.self_attention.out_proj
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_1.mlp.linear_1
2D search with INT8 - [A] encoder.layers.encoder_layer_1.mlp.linear_1
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_1.mlp.linear_2
2D search with INT8 - [A] encoder.layers.encoder_layer_1.mlp.linear_2
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_2.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_2.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_2.self_attention.attnMapActivationQuantizer
2D search with INT8 - [A] encoder.layers.encoder_layer_2.self_attention.attnOutActivationQuantizer
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_2.self_attention.out_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_2.self_attention.out_proj
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_2.mlp.linear_1
2D search with INT8 - [A] encoder.layers.encoder_layer_2.mlp.linear_1
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_2.mlp.linear_2
2D search with INT8 - [A] encoder.layers.encoder_layer_2.mlp.linear_2
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_3.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_3.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_3.self_attention.attnMapActivationQuantizer
2D search with INT8 - [A] encoder.layers.encoder_layer_3.self_attention.attnOutActivationQuantizer
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_3.self_attention.out_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_3.self_attention.out_proj
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_3.mlp.linear_1
2D search with INT8 - [A] encoder.layers.encoder_layer_3.mlp.linear_1
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_3.mlp.linear_2
2D search with INT8 - [A] encoder.layers.encoder_layer_3.mlp.linear_2
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_4.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_4.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_4.self_attention.attnMapActivationQuantizer
2D search with INT8 - [A] encoder.layers.encoder_layer_4.self_attention.attnOutActivationQuantizer
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_4.self_attention.out_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_4.self_attention.out_proj
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_4.mlp.linear_1
2D search with INT8 - [A] encoder.layers.encoder_layer_4.mlp.linear_1
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_4.mlp.linear_2
2D search with INT8 - [A] encoder.layers.encoder_layer_4.mlp.linear_2
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_5.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_5.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_5.self_attention.attnMapActivationQuantizer
2D search with INT8 - [A] encoder.layers.encoder_layer_5.self_attention.attnOutActivationQuantizer
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_5.self_attention.out_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_5.self_attention.out_proj
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_5.mlp.linear_1
2D search with INT8 - [A] encoder.layers.encoder_layer_5.mlp.linear_1
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_5.mlp.linear_2
2D search with INT8 - [A] encoder.layers.encoder_layer_5.mlp.linear_2
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_6.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_6.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_6.self_attention.attnMapActivationQuantizer
2D search with INT8 - [A] encoder.layers.encoder_layer_6.self_attention.attnOutActivationQuantizer
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_6.self_attention.out_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_6.self_attention.out_proj
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_6.mlp.linear_1
2D search with INT8 - [A] encoder.layers.encoder_layer_6.mlp.linear_1
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_6.mlp.linear_2
2D search with INT8 - [A] encoder.layers.encoder_layer_6.mlp.linear_2
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_7.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_7.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_7.self_attention.attnMapActivationQuantizer
2D search with INT8 - [A] encoder.layers.encoder_layer_7.self_attention.attnOutActivationQuantizer
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_7.self_attention.out_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_7.self_attention.out_proj
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_7.mlp.linear_1
2D search with INT8 - [A] encoder.layers.encoder_layer_7.mlp.linear_1
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_7.mlp.linear_2
2D search with INT8 - [A] encoder.layers.encoder_layer_7.mlp.linear_2
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_8.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_8.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_8.self_attention.attnMapActivationQuantizer
2D search with INT8 - [A] encoder.layers.encoder_layer_8.self_attention.attnOutActivationQuantizer
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_8.self_attention.out_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_8.self_attention.out_proj
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_8.mlp.linear_1
2D search with INT8 - [A] encoder.layers.encoder_layer_8.mlp.linear_1
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_8.mlp.linear_2
2D search with INT8 - [A] encoder.layers.encoder_layer_8.mlp.linear_2
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_9.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_9.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_9.self_attention.attnMapActivationQuantizer
2D search with INT8 - [A] encoder.layers.encoder_layer_9.self_attention.attnOutActivationQuantizer
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_9.self_attention.out_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_9.self_attention.out_proj
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_9.mlp.linear_1
2D search with INT8 - [A] encoder.layers.encoder_layer_9.mlp.linear_1
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_9.mlp.linear_2
2D search with INT8 - [A] encoder.layers.encoder_layer_9.mlp.linear_2
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_10.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_10.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_10.self_attention.attnMapActivationQuantizer
2D search with INT8 - [A] encoder.layers.encoder_layer_10.self_attention.attnOutActivationQuantizer
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_10.self_attention.out_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_10.self_attention.out_proj
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_10.mlp.linear_1
2D search with INT8 - [A] encoder.layers.encoder_layer_10.mlp.linear_1
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_10.mlp.linear_2
2D search with INT8 - [A] encoder.layers.encoder_layer_10.mlp.linear_2
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_11.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_11.self_attention.in_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_11.self_attention.attnMapActivationQuantizer
2D search with INT8 - [A] encoder.layers.encoder_layer_11.self_attention.attnOutActivationQuantizer
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_11.self_attention.out_proj
2D search with INT8 - [A] encoder.layers.encoder_layer_11.self_attention.out_proj
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_11.mlp.linear_1
2D search with INT8 - [A] encoder.layers.encoder_layer_11.mlp.linear_1
2D search with INT4 - [W]+AdaRound[W] encoder.layers.encoder_layer_11.mlp.linear_2
2D search with INT8 - [A] encoder.layers.encoder_layer_11.mlp.linear_2
2D search with INT4 - [W]+AdaRound[W] heads.head
Weight quantization parameter : {'scheme': 'AdaRoundQuantizer', 'bit_width': 4, 'per_channel': True}
Activation quantization parameter : {'scheme': 'MovingAvgMinMaxQuantizer', 'bit_width': 8}
Attention quantization parameter : {}
LayerNorm quantization parameter : {}
Calibration done 

conv_proj
   FP_OUTPUT shape torch.Size([1024, 768, 14, 14])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 3, 16, 16])
Iter     1 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 86257.3516 (MSE:0.0003, Reg:86257.3516) beta=20.00
Iter  6000 | Total loss: 93.9998 (MSE:0.0007, Reg:93.9991) beta=19.10
Iter  7000 | Total loss: 55.0007 (MSE:0.0007, Reg:55.0000) beta=18.20
Iter  8000 | Total loss: 36.0006 (MSE:0.0006, Reg:36.0000) beta=17.30
Iter  9000 | Total loss: 33.0001 (MSE:0.0006, Reg:32.9995) beta=16.40
Iter 10000 | Total loss: 26.0006 (MSE:0.0006, Reg:26.0000) beta=15.50
Iter 11000 | Total loss: 26.0006 (MSE:0.0006, Reg:26.0000) beta=14.60
Iter 12000 | Total loss: 21.0006 (MSE:0.0006, Reg:21.0000) beta=13.70
Iter 13000 | Total loss: 16.0006 (MSE:0.0006, Reg:16.0000) beta=12.80
Iter 14000 | Total loss: 13.0006 (MSE:0.0006, Reg:13.0000) beta=11.90
Iter 15000 | Total loss: 11.0006 (MSE:0.0006, Reg:11.0000) beta=11.00
Iter 16000 | Total loss: 8.0007 (MSE:0.0007, Reg:8.0000) beta=10.10
Iter 17000 | Total loss: 7.0007 (MSE:0.0007, Reg:7.0000) beta=9.20
Iter 18000 | Total loss: 5.0006 (MSE:0.0006, Reg:5.0000) beta=8.30
Iter 19000 | Total loss: 5.0006 (MSE:0.0006, Reg:5.0000) beta=7.40
Iter 20000 | Total loss: 4.0007 (MSE:0.0007, Reg:4.0000) beta=6.50
Iter 20629 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=5.93 ... Early stopping

encoder.layers.encoder_layer_0.self_attention.in_proj
   FP_OUTPUT shape torch.Size([1024, 197, 2304])
,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([2304, 768])
Iter     1 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 320792.3125 (MSE:0.0003, Reg:320792.3125) beta=20.00
Iter  6000 | Total loss: 27290.4297 (MSE:0.0005, Reg:27290.4297) beta=19.10
Iter  7000 | Total loss: 20585.1309 (MSE:0.0005, Reg:20585.1309) beta=18.20
Iter  8000 | Total loss: 17130.5078 (MSE:0.0005, Reg:17130.5078) beta=17.30
Iter  9000 | Total loss: 14789.2139 (MSE:0.0005, Reg:14789.2129) beta=16.40
Iter 10000 | Total loss: 12816.6992 (MSE:0.0005, Reg:12816.6982) beta=15.50
Iter 11000 | Total loss: 11101.1885 (MSE:0.0005, Reg:11101.1875) beta=14.60
Iter 12000 | Total loss: 9325.6738 (MSE:0.0005, Reg:9325.6729) beta=13.70
Iter 13000 | Total loss: 7671.7681 (MSE:0.0006, Reg:7671.7676) beta=12.80
Iter 14000 | Total loss: 6130.6216 (MSE:0.0006, Reg:6130.6211) beta=11.90
Iter 15000 | Total loss: 4560.5483 (MSE:0.0006, Reg:4560.5479) beta=11.00
Iter 16000 | Total loss: 3177.9070 (MSE:0.0006, Reg:3177.9065) beta=10.10
Iter 17000 | Total loss: 2046.8043 (MSE:0.0006, Reg:2046.8037) beta=9.20
Iter 18000 | Total loss: 1192.5699 (MSE:0.0006, Reg:1192.5693) beta=8.30
Iter 19000 | Total loss: 578.4279 (MSE:0.0006, Reg:578.4272) beta=7.40
Iter 20000 | Total loss: 243.0383 (MSE:0.0006, Reg:243.0377) beta=6.50
Iter 21000 | Total loss: 47.4063 (MSE:0.0006, Reg:47.4057) beta=5.60
Iter 22000 | Total loss: 2.0006 (MSE:0.0006, Reg:2.0000) beta=4.70
Iter 22450 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.29 ... Early stopping

encoder.layers.encoder_layer_0.self_attention.out_proj
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 768])
Iter     1 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0000 (MSE:0.0000, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0000 (MSE:0.0000, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0000 (MSE:0.0000, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0000 (MSE:0.0000, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 57576.3203 (MSE:0.0000, Reg:57576.3203) beta=20.00
Iter  6000 | Total loss: 1069.8560 (MSE:0.0000, Reg:1069.8560) beta=19.10
Iter  7000 | Total loss: 737.9331 (MSE:0.0000, Reg:737.9331) beta=18.20
Iter  8000 | Total loss: 547.0530 (MSE:0.0000, Reg:547.0530) beta=17.30
Iter  9000 | Total loss: 429.6925 (MSE:0.0000, Reg:429.6925) beta=16.40
Iter 10000 | Total loss: 347.0000 (MSE:0.0000, Reg:347.0000) beta=15.50
Iter 11000 | Total loss: 278.5526 (MSE:0.0000, Reg:278.5526) beta=14.60
Iter 12000 | Total loss: 230.0471 (MSE:0.0000, Reg:230.0471) beta=13.70
Iter 13000 | Total loss: 179.0000 (MSE:0.0000, Reg:179.0000) beta=12.80
Iter 14000 | Total loss: 135.0000 (MSE:0.0000, Reg:135.0000) beta=11.90
Iter 15000 | Total loss: 109.0000 (MSE:0.0000, Reg:109.0000) beta=11.00
Iter 16000 | Total loss: 83.7387 (MSE:0.0000, Reg:83.7387) beta=10.10
Iter 17000 | Total loss: 57.0000 (MSE:0.0000, Reg:56.9999) beta=9.20
Iter 18000 | Total loss: 32.0000 (MSE:0.0000, Reg:32.0000) beta=8.30
Iter 19000 | Total loss: 18.9863 (MSE:0.0000, Reg:18.9863) beta=7.40
Iter 20000 | Total loss: 3.8056 (MSE:0.0000, Reg:3.8056) beta=6.50
Iter 20560 | Total loss: 0.0000 (MSE:0.0000, Reg:0.0000) beta=6.00 ... Early stopping

encoder.layers.encoder_layer_0.mlp.linear_1
   FP_OUTPUT shape torch.Size([1024, 197, 3072])
,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([3072, 768])
Iter     1 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 618268.0000 (MSE:0.0010, Reg:618268.0000) beta=20.00
Iter  6000 | Total loss: 34700.0625 (MSE:0.0017, Reg:34700.0625) beta=19.10
Iter  7000 | Total loss: 25897.7715 (MSE:0.0018, Reg:25897.7695) beta=18.20
Iter  8000 | Total loss: 22110.5918 (MSE:0.0018, Reg:22110.5898) beta=17.30
Iter  9000 | Total loss: 19443.8203 (MSE:0.0018, Reg:19443.8184) beta=16.40
Iter 10000 | Total loss: 17006.9043 (MSE:0.0019, Reg:17006.9023) beta=15.50
Iter 11000 | Total loss: 14646.5410 (MSE:0.0021, Reg:14646.5391) beta=14.60
Iter 12000 | Total loss: 12359.7676 (MSE:0.0019, Reg:12359.7656) beta=13.70
Iter 13000 | Total loss: 10148.0781 (MSE:0.0019, Reg:10148.0762) beta=12.80
Iter 14000 | Total loss: 7934.7930 (MSE:0.0020, Reg:7934.7910) beta=11.90
Iter 15000 | Total loss: 6038.5596 (MSE:0.0019, Reg:6038.5576) beta=11.00
Iter 16000 | Total loss: 4172.1685 (MSE:0.0021, Reg:4172.1665) beta=10.10
Iter 17000 | Total loss: 2688.0430 (MSE:0.0020, Reg:2688.0410) beta=9.20
Iter 18000 | Total loss: 1510.2589 (MSE:0.0019, Reg:1510.2570) beta=8.30
Iter 19000 | Total loss: 688.2161 (MSE:0.0019, Reg:688.2141) beta=7.40
Iter 20000 | Total loss: 174.7128 (MSE:0.0021, Reg:174.7108) beta=6.50
Iter 21000 | Total loss: 3.0022 (MSE:0.0022, Reg:3.0000) beta=5.60
Iter 21168 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=5.45 ... Early stopping

encoder.layers.encoder_layer_0.mlp.linear_2
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 3072])
Iter     1 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 385280.1875 (MSE:0.0005, Reg:385280.1875) beta=20.00
Iter  6000 | Total loss: 12524.3174 (MSE:0.0007, Reg:12524.3164) beta=19.10
Iter  7000 | Total loss: 7988.2012 (MSE:0.0007, Reg:7988.2002) beta=18.20
Iter  8000 | Total loss: 6162.7607 (MSE:0.0008, Reg:6162.7598) beta=17.30
Iter  9000 | Total loss: 5085.6846 (MSE:0.0008, Reg:5085.6836) beta=16.40
Iter 10000 | Total loss: 4274.2988 (MSE:0.0007, Reg:4274.2979) beta=15.50
Iter 11000 | Total loss: 3591.8350 (MSE:0.0007, Reg:3591.8342) beta=14.60
Iter 12000 | Total loss: 2947.3494 (MSE:0.0007, Reg:2947.3486) beta=13.70
Iter 13000 | Total loss: 2351.9265 (MSE:0.0008, Reg:2351.9258) beta=12.80
Iter 14000 | Total loss: 1779.4326 (MSE:0.0008, Reg:1779.4319) beta=11.90
Iter 15000 | Total loss: 1270.2043 (MSE:0.0007, Reg:1270.2036) beta=11.00
Iter 16000 | Total loss: 875.5803 (MSE:0.0007, Reg:875.5796) beta=10.10
Iter 17000 | Total loss: 539.3889 (MSE:0.0007, Reg:539.3881) beta=9.20
Iter 18000 | Total loss: 304.9833 (MSE:0.0007, Reg:304.9826) beta=8.30
Iter 19000 | Total loss: 135.3761 (MSE:0.0007, Reg:135.3754) beta=7.40
Iter 20000 | Total loss: 45.2066 (MSE:0.0008, Reg:45.2059) beta=6.50
Iter 21000 | Total loss: 8.0008 (MSE:0.0008, Reg:8.0000) beta=5.60
Iter 21484 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=5.16 ... Early stopping

encoder.layers.encoder_layer_1.self_attention.in_proj
   FP_OUTPUT shape torch.Size([1024, 197, 2304])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([2304, 768])
Iter     1 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 357727.8750 (MSE:0.0010, Reg:357727.8750) beta=20.00
Iter  6000 | Total loss: 28145.3672 (MSE:0.0012, Reg:28145.3652) beta=19.10
Iter  7000 | Total loss: 18555.5156 (MSE:0.0012, Reg:18555.5137) beta=18.20
Iter  8000 | Total loss: 14585.5117 (MSE:0.0013, Reg:14585.5107) beta=17.30
Iter  9000 | Total loss: 12081.8027 (MSE:0.0013, Reg:12081.8018) beta=16.40
Iter 10000 | Total loss: 10127.2100 (MSE:0.0012, Reg:10127.2090) beta=15.50
Iter 11000 | Total loss: 8461.2979 (MSE:0.0013, Reg:8461.2969) beta=14.60
Iter 12000 | Total loss: 6942.3384 (MSE:0.0012, Reg:6942.3369) beta=13.70
Iter 13000 | Total loss: 5622.3945 (MSE:0.0013, Reg:5622.3931) beta=12.80
Iter 14000 | Total loss: 4341.5000 (MSE:0.0013, Reg:4341.4985) beta=11.90
Iter 15000 | Total loss: 3188.1860 (MSE:0.0013, Reg:3188.1848) beta=11.00
Iter 16000 | Total loss: 2158.6248 (MSE:0.0013, Reg:2158.6235) beta=10.10
Iter 17000 | Total loss: 1329.0684 (MSE:0.0014, Reg:1329.0670) beta=9.20
Iter 18000 | Total loss: 758.8760 (MSE:0.0013, Reg:758.8747) beta=8.30
Iter 19000 | Total loss: 341.8149 (MSE:0.0013, Reg:341.8136) beta=7.40
Iter 20000 | Total loss: 82.6677 (MSE:0.0013, Reg:82.6663) beta=6.50
Iter 21000 | Total loss: 2.9491 (MSE:0.0014, Reg:2.9477) beta=5.60
Iter 21165 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=5.45 ... Early stopping

encoder.layers.encoder_layer_1.self_attention.out_proj
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 768])
Iter     1 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0000 (MSE:0.0000, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0000 (MSE:0.0000, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0000 (MSE:0.0000, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 105405.9688 (MSE:0.0000, Reg:105405.9688) beta=20.00
Iter  6000 | Total loss: 2023.9840 (MSE:0.0000, Reg:2023.9840) beta=19.10
Iter  7000 | Total loss: 985.9328 (MSE:0.0000, Reg:985.9327) beta=18.20
Iter  8000 | Total loss: 635.7453 (MSE:0.0000, Reg:635.7452) beta=17.30
Iter  9000 | Total loss: 489.8235 (MSE:0.0001, Reg:489.8234) beta=16.40
Iter 10000 | Total loss: 384.3051 (MSE:0.0000, Reg:384.3050) beta=15.50
Iter 11000 | Total loss: 304.0000 (MSE:0.0000, Reg:304.0000) beta=14.60
Iter 12000 | Total loss: 221.9046 (MSE:0.0001, Reg:221.9046) beta=13.70
Iter 13000 | Total loss: 165.3424 (MSE:0.0000, Reg:165.3423) beta=12.80
Iter 14000 | Total loss: 128.8263 (MSE:0.0000, Reg:128.8263) beta=11.90
Iter 15000 | Total loss: 96.0343 (MSE:0.0000, Reg:96.0342) beta=11.00
Iter 16000 | Total loss: 65.0000 (MSE:0.0000, Reg:65.0000) beta=10.10
Iter 17000 | Total loss: 46.8798 (MSE:0.0000, Reg:46.8797) beta=9.20
Iter 18000 | Total loss: 30.5924 (MSE:0.0001, Reg:30.5924) beta=8.30
Iter 19000 | Total loss: 17.0000 (MSE:0.0000, Reg:17.0000) beta=7.40
Iter 20000 | Total loss: 6.0000 (MSE:0.0000, Reg:6.0000) beta=6.50
Iter 20665 | Total loss: 0.0000 (MSE:0.0000, Reg:0.0000) beta=5.90 ... Early stopping

encoder.layers.encoder_layer_1.mlp.linear_1
   FP_OUTPUT shape torch.Size([1024, 197, 3072])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([3072, 768])
Iter     1 | Total loss: 0.0163 (MSE:0.0163, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 468969.7500 (MSE:0.0050, Reg:468969.7500) beta=20.00
Iter  6000 | Total loss: 40582.2148 (MSE:0.0051, Reg:40582.2109) beta=19.10
Iter  7000 | Total loss: 28463.9043 (MSE:0.0052, Reg:28463.8984) beta=18.20
Iter  8000 | Total loss: 23449.1387 (MSE:0.0053, Reg:23449.1328) beta=17.30
Iter  9000 | Total loss: 19980.1562 (MSE:0.0052, Reg:19980.1504) beta=16.40
Iter 10000 | Total loss: 17008.4551 (MSE:0.0055, Reg:17008.4492) beta=15.50
Iter 11000 | Total loss: 14144.1621 (MSE:0.0050, Reg:14144.1572) beta=14.60
Iter 12000 | Total loss: 11498.5840 (MSE:0.0057, Reg:11498.5781) beta=13.70
Iter 13000 | Total loss: 9045.1611 (MSE:0.0051, Reg:9045.1562) beta=12.80
Iter 14000 | Total loss: 6667.5498 (MSE:0.0050, Reg:6667.5449) beta=11.90
Iter 15000 | Total loss: 4874.9775 (MSE:0.0051, Reg:4874.9727) beta=11.00
Iter 16000 | Total loss: 3201.1536 (MSE:0.0055, Reg:3201.1479) beta=10.10
Iter 17000 | Total loss: 1932.6656 (MSE:0.0058, Reg:1932.6598) beta=9.20
Iter 18000 | Total loss: 984.8607 (MSE:0.0057, Reg:984.8550) beta=8.30
Iter 19000 | Total loss: 335.4226 (MSE:0.0056, Reg:335.4171) beta=7.40
Iter 20000 | Total loss: 53.7093 (MSE:0.0056, Reg:53.7038) beta=6.50
Iter 20682 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=5.89 ... Early stopping

encoder.layers.encoder_layer_1.mlp.linear_2
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 3072])
Iter     1 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 222532.5625 (MSE:0.0007, Reg:222532.5625) beta=20.00
Iter  6000 | Total loss: 2619.0266 (MSE:0.0008, Reg:2619.0259) beta=19.10
Iter  7000 | Total loss: 1578.5366 (MSE:0.0008, Reg:1578.5359) beta=18.20
Iter  8000 | Total loss: 1180.7034 (MSE:0.0008, Reg:1180.7026) beta=17.30
Iter  9000 | Total loss: 957.4440 (MSE:0.0008, Reg:957.4432) beta=16.40
Iter 10000 | Total loss: 796.6362 (MSE:0.0008, Reg:796.6354) beta=15.50
Iter 11000 | Total loss: 662.5034 (MSE:0.0008, Reg:662.5026) beta=14.60
Iter 12000 | Total loss: 540.2541 (MSE:0.0008, Reg:540.2533) beta=13.70
Iter 13000 | Total loss: 415.1209 (MSE:0.0008, Reg:415.1201) beta=12.80
Iter 14000 | Total loss: 297.1395 (MSE:0.0008, Reg:297.1387) beta=11.90
Iter 15000 | Total loss: 191.8479 (MSE:0.0008, Reg:191.8471) beta=11.00
Iter 16000 | Total loss: 117.0008 (MSE:0.0008, Reg:117.0000) beta=10.10
Iter 17000 | Total loss: 75.0003 (MSE:0.0008, Reg:74.9995) beta=9.20
Iter 18000 | Total loss: 44.0007 (MSE:0.0008, Reg:43.9999) beta=8.30
Iter 19000 | Total loss: 13.0008 (MSE:0.0008, Reg:13.0000) beta=7.40
Iter 20000 | Total loss: 3.0008 (MSE:0.0008, Reg:3.0000) beta=6.50
Iter 20486 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=6.06 ... Early stopping

encoder.layers.encoder_layer_2.self_attention.in_proj
   FP_OUTPUT shape torch.Size([1024, 197, 2304])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([2304, 768])
Iter     1 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 399648.6250 (MSE:0.0036, Reg:399648.6250) beta=20.00
Iter  6000 | Total loss: 34227.5586 (MSE:0.0039, Reg:34227.5547) beta=19.10
Iter  7000 | Total loss: 22801.7422 (MSE:0.0037, Reg:22801.7383) beta=18.20
Iter  8000 | Total loss: 18148.2539 (MSE:0.0043, Reg:18148.2500) beta=17.30
Iter  9000 | Total loss: 14998.3604 (MSE:0.0037, Reg:14998.3564) beta=16.40
Iter 10000 | Total loss: 12752.9053 (MSE:0.0037, Reg:12752.9014) beta=15.50
Iter 11000 | Total loss: 10658.5898 (MSE:0.0040, Reg:10658.5859) beta=14.60
Iter 12000 | Total loss: 8683.8047 (MSE:0.0042, Reg:8683.8008) beta=13.70
Iter 13000 | Total loss: 6808.2778 (MSE:0.0040, Reg:6808.2739) beta=12.80
Iter 14000 | Total loss: 5277.3574 (MSE:0.0039, Reg:5277.3535) beta=11.90
Iter 15000 | Total loss: 3818.2695 (MSE:0.0040, Reg:3818.2656) beta=11.00
Iter 16000 | Total loss: 2566.3101 (MSE:0.0041, Reg:2566.3059) beta=10.10
Iter 17000 | Total loss: 1556.7537 (MSE:0.0041, Reg:1556.7495) beta=9.20
Iter 18000 | Total loss: 807.4242 (MSE:0.0038, Reg:807.4204) beta=8.30
Iter 19000 | Total loss: 331.4008 (MSE:0.0040, Reg:331.3969) beta=7.40
Iter 20000 | Total loss: 58.8639 (MSE:0.0039, Reg:58.8600) beta=6.50
Iter 21000 | Total loss: 1.0042 (MSE:0.0042, Reg:1.0000) beta=5.60
Iter 21243 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=5.38 ... Early stopping

encoder.layers.encoder_layer_2.self_attention.out_proj
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 768])
Iter     1 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 134913.6250 (MSE:0.0002, Reg:134913.6250) beta=20.00
Iter  6000 | Total loss: 5464.3535 (MSE:0.0003, Reg:5464.3530) beta=19.10
Iter  7000 | Total loss: 3339.9729 (MSE:0.0003, Reg:3339.9727) beta=18.20
Iter  8000 | Total loss: 2465.1470 (MSE:0.0003, Reg:2465.1467) beta=17.30
Iter  9000 | Total loss: 2001.6809 (MSE:0.0003, Reg:2001.6807) beta=16.40
Iter 10000 | Total loss: 1640.8143 (MSE:0.0002, Reg:1640.8141) beta=15.50
Iter 11000 | Total loss: 1363.9042 (MSE:0.0002, Reg:1363.9039) beta=14.60
Iter 12000 | Total loss: 1098.8862 (MSE:0.0003, Reg:1098.8860) beta=13.70
Iter 13000 | Total loss: 870.6292 (MSE:0.0003, Reg:870.6289) beta=12.80
Iter 14000 | Total loss: 676.6260 (MSE:0.0003, Reg:676.6258) beta=11.90
Iter 15000 | Total loss: 485.5459 (MSE:0.0002, Reg:485.5457) beta=11.00
Iter 16000 | Total loss: 323.3365 (MSE:0.0002, Reg:323.3363) beta=10.10
Iter 17000 | Total loss: 199.7009 (MSE:0.0003, Reg:199.7006) beta=9.20
Iter 18000 | Total loss: 107.6390 (MSE:0.0002, Reg:107.6387) beta=8.30
Iter 19000 | Total loss: 44.6600 (MSE:0.0003, Reg:44.6597) beta=7.40
Iter 20000 | Total loss: 12.0003 (MSE:0.0003, Reg:12.0000) beta=6.50
Iter 21000 | Total loss: 2.0003 (MSE:0.0003, Reg:2.0000) beta=5.60
Iter 21521 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=5.13 ... Early stopping

encoder.layers.encoder_layer_2.mlp.linear_1
   FP_OUTPUT shape torch.Size([1024, 197, 3072])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([3072, 768])
Iter     1 | Total loss: 0.0181 (MSE:0.0181, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0082 (MSE:0.0082, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0091 (MSE:0.0091, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0079 (MSE:0.0079, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 665097.2500 (MSE:0.0086, Reg:665097.2500) beta=20.00
Iter  6000 | Total loss: 51466.4141 (MSE:0.0093, Reg:51466.4062) beta=19.10
Iter  7000 | Total loss: 37529.8438 (MSE:0.0092, Reg:37529.8359) beta=18.20
Iter  8000 | Total loss: 31283.7969 (MSE:0.0087, Reg:31283.7891) beta=17.30
Iter  9000 | Total loss: 26910.2715 (MSE:0.0094, Reg:26910.2617) beta=16.40
Iter 10000 | Total loss: 23018.1992 (MSE:0.0095, Reg:23018.1895) beta=15.50
Iter 11000 | Total loss: 19332.8086 (MSE:0.0103, Reg:19332.7988) beta=14.60
Iter 12000 | Total loss: 15997.7227 (MSE:0.0085, Reg:15997.7139) beta=13.70
Iter 13000 | Total loss: 12546.1074 (MSE:0.0098, Reg:12546.0977) beta=12.80
Iter 14000 | Total loss: 9488.9424 (MSE:0.0090, Reg:9488.9336) beta=11.90
Iter 15000 | Total loss: 6621.6851 (MSE:0.0092, Reg:6621.6758) beta=11.00
Iter 16000 | Total loss: 4268.1255 (MSE:0.0104, Reg:4268.1152) beta=10.10
Iter 17000 | Total loss: 2584.3069 (MSE:0.0111, Reg:2584.2959) beta=9.20
Iter 18000 | Total loss: 1306.8944 (MSE:0.0090, Reg:1306.8854) beta=8.30
Iter 19000 | Total loss: 385.3520 (MSE:0.0094, Reg:385.3426) beta=7.40
Iter 20000 | Total loss: 21.3823 (MSE:0.0092, Reg:21.3731) beta=6.50
Iter 20481 | Total loss: 0.0096 (MSE:0.0096, Reg:0.0000) beta=6.07 ... Early stopping

encoder.layers.encoder_layer_2.mlp.linear_2
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 3072])
Iter     1 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 296548.6562 (MSE:0.0016, Reg:296548.6562) beta=20.00
Iter  6000 | Total loss: 5390.1040 (MSE:0.0016, Reg:5390.1025) beta=19.10
Iter  7000 | Total loss: 3174.2800 (MSE:0.0018, Reg:3174.2783) beta=18.20
Iter  8000 | Total loss: 2319.1609 (MSE:0.0017, Reg:2319.1592) beta=17.30
Iter  9000 | Total loss: 1827.6360 (MSE:0.0017, Reg:1827.6343) beta=16.40
Iter 10000 | Total loss: 1477.9067 (MSE:0.0018, Reg:1477.9049) beta=15.50
Iter 11000 | Total loss: 1239.6575 (MSE:0.0017, Reg:1239.6558) beta=14.60
Iter 12000 | Total loss: 990.5847 (MSE:0.0018, Reg:990.5829) beta=13.70
Iter 13000 | Total loss: 786.2787 (MSE:0.0017, Reg:786.2770) beta=12.80
Iter 14000 | Total loss: 598.0253 (MSE:0.0018, Reg:598.0234) beta=11.90
Iter 15000 | Total loss: 421.6265 (MSE:0.0017, Reg:421.6248) beta=11.00
Iter 16000 | Total loss: 266.5382 (MSE:0.0017, Reg:266.5365) beta=10.10
Iter 17000 | Total loss: 145.5653 (MSE:0.0018, Reg:145.5635) beta=9.20
Iter 18000 | Total loss: 68.3320 (MSE:0.0017, Reg:68.3303) beta=8.30
Iter 19000 | Total loss: 22.0017 (MSE:0.0017, Reg:22.0000) beta=7.40
Iter 20000 | Total loss: 8.8658 (MSE:0.0016, Reg:8.8642) beta=6.50
Iter 21000 | Total loss: 1.0021 (MSE:0.0021, Reg:1.0000) beta=5.60
Iter 21068 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=5.54 ... Early stopping

encoder.layers.encoder_layer_3.self_attention.in_proj
   FP_OUTPUT shape torch.Size([1024, 197, 2304])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([2304, 768])
Iter     1 | Total loss: 0.0093 (MSE:0.0093, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 430035.8438 (MSE:0.0065, Reg:430035.8438) beta=20.00
Iter  6000 | Total loss: 38122.0117 (MSE:0.0065, Reg:38122.0039) beta=19.10
Iter  7000 | Total loss: 24207.4629 (MSE:0.0066, Reg:24207.4570) beta=18.20
Iter  8000 | Total loss: 18437.2305 (MSE:0.0065, Reg:18437.2246) beta=17.30
Iter  9000 | Total loss: 15007.7881 (MSE:0.0065, Reg:15007.7812) beta=16.40
Iter 10000 | Total loss: 12347.4355 (MSE:0.0073, Reg:12347.4277) beta=15.50
Iter 11000 | Total loss: 10078.1631 (MSE:0.0066, Reg:10078.1562) beta=14.60
Iter 12000 | Total loss: 8065.3022 (MSE:0.0066, Reg:8065.2954) beta=13.70
Iter 13000 | Total loss: 6280.2842 (MSE:0.0073, Reg:6280.2769) beta=12.80
Iter 14000 | Total loss: 4725.1465 (MSE:0.0070, Reg:4725.1396) beta=11.90
Iter 15000 | Total loss: 3391.4856 (MSE:0.0076, Reg:3391.4780) beta=11.00
Iter 16000 | Total loss: 2325.9587 (MSE:0.0076, Reg:2325.9512) beta=10.10
Iter 17000 | Total loss: 1374.6857 (MSE:0.0075, Reg:1374.6781) beta=9.20
Iter 18000 | Total loss: 704.5375 (MSE:0.0075, Reg:704.5300) beta=8.30
Iter 19000 | Total loss: 238.3986 (MSE:0.0071, Reg:238.3914) beta=7.40
Iter 20000 | Total loss: 33.1497 (MSE:0.0079, Reg:33.1417) beta=6.50
Iter 20944 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=5.65 ... Early stopping

encoder.layers.encoder_layer_3.self_attention.out_proj
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 768])
Iter     1 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 160320.8594 (MSE:0.0004, Reg:160320.8594) beta=20.00
Iter  6000 | Total loss: 7717.0229 (MSE:0.0004, Reg:7717.0225) beta=19.10
Iter  7000 | Total loss: 4628.0742 (MSE:0.0004, Reg:4628.0737) beta=18.20
Iter  8000 | Total loss: 3404.6172 (MSE:0.0004, Reg:3404.6167) beta=17.30
Iter  9000 | Total loss: 2683.4978 (MSE:0.0004, Reg:2683.4973) beta=16.40
Iter 10000 | Total loss: 2237.4709 (MSE:0.0004, Reg:2237.4705) beta=15.50
Iter 11000 | Total loss: 1857.9384 (MSE:0.0004, Reg:1857.9380) beta=14.60
Iter 12000 | Total loss: 1502.0142 (MSE:0.0004, Reg:1502.0137) beta=13.70
Iter 13000 | Total loss: 1183.6913 (MSE:0.0004, Reg:1183.6908) beta=12.80
Iter 14000 | Total loss: 934.5030 (MSE:0.0004, Reg:934.5026) beta=11.90
Iter 15000 | Total loss: 666.1386 (MSE:0.0004, Reg:666.1382) beta=11.00
Iter 16000 | Total loss: 435.4121 (MSE:0.0004, Reg:435.4116) beta=10.10
Iter 17000 | Total loss: 250.8015 (MSE:0.0004, Reg:250.8011) beta=9.20
Iter 18000 | Total loss: 133.2959 (MSE:0.0005, Reg:133.2955) beta=8.30
Iter 19000 | Total loss: 61.0024 (MSE:0.0004, Reg:61.0020) beta=7.40
Iter 20000 | Total loss: 5.0005 (MSE:0.0005, Reg:5.0000) beta=6.50
Iter 21000 | Total loss: 1.0005 (MSE:0.0005, Reg:1.0000) beta=5.60
Iter 21197 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=5.42 ... Early stopping

encoder.layers.encoder_layer_3.mlp.linear_1
   FP_OUTPUT shape torch.Size([1024, 197, 3072])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([3072, 768])
Iter     1 | Total loss: 0.0211 (MSE:0.0211, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0130 (MSE:0.0130, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0153 (MSE:0.0153, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0135 (MSE:0.0135, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0121 (MSE:0.0121, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 470801.5625 (MSE:0.0145, Reg:470801.5625) beta=20.00
Iter  6000 | Total loss: 61786.4219 (MSE:0.0144, Reg:61786.4062) beta=19.10
Iter  7000 | Total loss: 43252.5156 (MSE:0.0149, Reg:43252.5000) beta=18.20
Iter  8000 | Total loss: 34651.6094 (MSE:0.0133, Reg:34651.5977) beta=17.30
Iter  9000 | Total loss: 28864.8066 (MSE:0.0137, Reg:28864.7930) beta=16.40
Iter 10000 | Total loss: 24050.0020 (MSE:0.0150, Reg:24049.9863) beta=15.50
Iter 11000 | Total loss: 19849.9238 (MSE:0.0140, Reg:19849.9102) beta=14.60
Iter 12000 | Total loss: 15810.4072 (MSE:0.0145, Reg:15810.3926) beta=13.70
Iter 13000 | Total loss: 12040.7793 (MSE:0.0148, Reg:12040.7646) beta=12.80
Iter 14000 | Total loss: 8797.0879 (MSE:0.0158, Reg:8797.0723) beta=11.90
Iter 15000 | Total loss: 6032.1665 (MSE:0.0132, Reg:6032.1533) beta=11.00
Iter 16000 | Total loss: 3705.3574 (MSE:0.0152, Reg:3705.3423) beta=10.10
Iter 17000 | Total loss: 1897.3497 (MSE:0.0150, Reg:1897.3347) beta=9.20
Iter 18000 | Total loss: 727.9741 (MSE:0.0151, Reg:727.9590) beta=8.30
Iter 19000 | Total loss: 125.8319 (MSE:0.0157, Reg:125.8162) beta=7.40
Iter 20000 | Total loss: 1.9682 (MSE:0.0165, Reg:1.9517) beta=6.50
Iter 20088 | Total loss: 0.0145 (MSE:0.0145, Reg:0.0000) beta=6.42 ... Early stopping

encoder.layers.encoder_layer_3.mlp.linear_2
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 3072])
Iter     1 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 265202.5625 (MSE:0.0015, Reg:265202.5625) beta=20.00
Iter  6000 | Total loss: 4704.2925 (MSE:0.0016, Reg:4704.2910) beta=19.10
Iter  7000 | Total loss: 2886.6062 (MSE:0.0016, Reg:2886.6047) beta=18.20
Iter  8000 | Total loss: 2130.7991 (MSE:0.0015, Reg:2130.7976) beta=17.30
Iter  9000 | Total loss: 1709.3451 (MSE:0.0015, Reg:1709.3436) beta=16.40
Iter 10000 | Total loss: 1433.8540 (MSE:0.0017, Reg:1433.8523) beta=15.50
Iter 11000 | Total loss: 1179.6769 (MSE:0.0014, Reg:1179.6755) beta=14.60
Iter 12000 | Total loss: 966.4218 (MSE:0.0014, Reg:966.4204) beta=13.70
Iter 13000 | Total loss: 790.3393 (MSE:0.0013, Reg:790.3380) beta=12.80
Iter 14000 | Total loss: 586.0374 (MSE:0.0014, Reg:586.0359) beta=11.90
Iter 15000 | Total loss: 436.6714 (MSE:0.0017, Reg:436.6697) beta=11.00
Iter 16000 | Total loss: 277.6991 (MSE:0.0014, Reg:277.6977) beta=10.10
Iter 17000 | Total loss: 164.9247 (MSE:0.0016, Reg:164.9231) beta=9.20
Iter 18000 | Total loss: 91.0012 (MSE:0.0015, Reg:90.9997) beta=8.30
Iter 19000 | Total loss: 32.0014 (MSE:0.0014, Reg:32.0000) beta=7.40
Iter 20000 | Total loss: 3.9808 (MSE:0.0014, Reg:3.9794) beta=6.50
Iter 21000 | Total loss: 1.0017 (MSE:0.0017, Reg:1.0000) beta=5.60
Iter 21181 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=5.44 ... Early stopping

encoder.layers.encoder_layer_4.self_attention.in_proj
   FP_OUTPUT shape torch.Size([1024, 197, 2304])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([2304, 768])
Iter     1 | Total loss: 0.0156 (MSE:0.0156, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0126 (MSE:0.0126, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0133 (MSE:0.0133, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0111 (MSE:0.0111, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0129 (MSE:0.0129, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 451332.9375 (MSE:0.0129, Reg:451332.9375) beta=20.00
Iter  6000 | Total loss: 53532.7656 (MSE:0.0121, Reg:53532.7539) beta=19.10
Iter  7000 | Total loss: 35890.3516 (MSE:0.0118, Reg:35890.3398) beta=18.20
Iter  8000 | Total loss: 27969.0801 (MSE:0.0137, Reg:27969.0664) beta=17.30
Iter  9000 | Total loss: 22769.6895 (MSE:0.0133, Reg:22769.6758) beta=16.40
Iter 10000 | Total loss: 18648.4062 (MSE:0.0143, Reg:18648.3926) beta=15.50
Iter 11000 | Total loss: 15042.0322 (MSE:0.0142, Reg:15042.0176) beta=14.60
Iter 12000 | Total loss: 11898.6084 (MSE:0.0124, Reg:11898.5957) beta=13.70
Iter 13000 | Total loss: 9063.6094 (MSE:0.0131, Reg:9063.5967) beta=12.80
Iter 14000 | Total loss: 6417.3340 (MSE:0.0116, Reg:6417.3223) beta=11.90
Iter 15000 | Total loss: 4343.5249 (MSE:0.0137, Reg:4343.5112) beta=11.00
Iter 16000 | Total loss: 2714.9426 (MSE:0.0142, Reg:2714.9285) beta=10.10
Iter 17000 | Total loss: 1531.2141 (MSE:0.0152, Reg:1531.1990) beta=9.20
Iter 18000 | Total loss: 667.6740 (MSE:0.0142, Reg:667.6598) beta=8.30
Iter 19000 | Total loss: 184.3049 (MSE:0.0125, Reg:184.2924) beta=7.40
Iter 20000 | Total loss: 9.0125 (MSE:0.0125, Reg:9.0000) beta=6.50
Iter 20697 | Total loss: 0.0133 (MSE:0.0133, Reg:0.0000) beta=5.87 ... Early stopping

encoder.layers.encoder_layer_4.self_attention.out_proj
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 768])
Iter     1 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 180106.0156 (MSE:0.0006, Reg:180106.0156) beta=20.00
Iter  6000 | Total loss: 10359.9258 (MSE:0.0007, Reg:10359.9248) beta=19.10
Iter  7000 | Total loss: 6059.8257 (MSE:0.0007, Reg:6059.8252) beta=18.20
Iter  8000 | Total loss: 4251.4370 (MSE:0.0007, Reg:4251.4365) beta=17.30
Iter  9000 | Total loss: 3238.8813 (MSE:0.0006, Reg:3238.8806) beta=16.40
Iter 10000 | Total loss: 2547.7917 (MSE:0.0006, Reg:2547.7910) beta=15.50
Iter 11000 | Total loss: 2047.3540 (MSE:0.0007, Reg:2047.3533) beta=14.60
Iter 12000 | Total loss: 1600.0879 (MSE:0.0006, Reg:1600.0873) beta=13.70
Iter 13000 | Total loss: 1250.6167 (MSE:0.0006, Reg:1250.6161) beta=12.80
Iter 14000 | Total loss: 944.2174 (MSE:0.0006, Reg:944.2167) beta=11.90
Iter 15000 | Total loss: 644.3451 (MSE:0.0006, Reg:644.3445) beta=11.00
Iter 16000 | Total loss: 399.6847 (MSE:0.0006, Reg:399.6841) beta=10.10
Iter 17000 | Total loss: 230.6696 (MSE:0.0007, Reg:230.6689) beta=9.20
Iter 18000 | Total loss: 115.9737 (MSE:0.0006, Reg:115.9731) beta=8.30
Iter 19000 | Total loss: 40.8473 (MSE:0.0006, Reg:40.8467) beta=7.40
Iter 20000 | Total loss: 8.0006 (MSE:0.0006, Reg:8.0000) beta=6.50
Iter 20669 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=5.90 ... Early stopping

encoder.layers.encoder_layer_4.mlp.linear_1
   FP_OUTPUT shape torch.Size([1024, 197, 3072])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([3072, 768])
Iter     1 | Total loss: 0.0231 (MSE:0.0231, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0199 (MSE:0.0199, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0188 (MSE:0.0188, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0158 (MSE:0.0158, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0196 (MSE:0.0196, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 512809.1875 (MSE:0.0155, Reg:512809.1875) beta=20.00
Iter  6000 | Total loss: 74292.8750 (MSE:0.0205, Reg:74292.8516) beta=19.10
Iter  7000 | Total loss: 53783.4766 (MSE:0.0177, Reg:53783.4570) beta=18.20
Iter  8000 | Total loss: 43500.4102 (MSE:0.0182, Reg:43500.3906) beta=17.30
Iter  9000 | Total loss: 36112.5195 (MSE:0.0190, Reg:36112.5000) beta=16.40
Iter 10000 | Total loss: 29961.0254 (MSE:0.0178, Reg:29961.0078) beta=15.50
Iter 11000 | Total loss: 24273.7031 (MSE:0.0173, Reg:24273.6855) beta=14.60
Iter 12000 | Total loss: 19279.2695 (MSE:0.0179, Reg:19279.2520) beta=13.70
Iter 13000 | Total loss: 14546.3398 (MSE:0.0200, Reg:14546.3203) beta=12.80
Iter 14000 | Total loss: 10258.7930 (MSE:0.0182, Reg:10258.7744) beta=11.90
Iter 15000 | Total loss: 6620.0679 (MSE:0.0181, Reg:6620.0498) beta=11.00
Iter 16000 | Total loss: 3737.8721 (MSE:0.0176, Reg:3737.8545) beta=10.10
Iter 17000 | Total loss: 1842.5645 (MSE:0.0198, Reg:1842.5447) beta=9.20
Iter 18000 | Total loss: 635.1668 (MSE:0.0189, Reg:635.1479) beta=8.30
Iter 19000 | Total loss: 88.0295 (MSE:0.0215, Reg:88.0080) beta=7.40
Iter 20000 | Total loss: 2.0199 (MSE:0.0199, Reg:2.0000) beta=6.50
Iter 20291 | Total loss: 0.0190 (MSE:0.0190, Reg:0.0000) beta=6.24 ... Early stopping

encoder.layers.encoder_layer_4.mlp.linear_2
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 3072])
Iter     1 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 241482.2500 (MSE:0.0023, Reg:241482.2500) beta=20.00
Iter  6000 | Total loss: 4535.3403 (MSE:0.0025, Reg:4535.3379) beta=19.10
Iter  7000 | Total loss: 2847.8328 (MSE:0.0028, Reg:2847.8301) beta=18.20
Iter  8000 | Total loss: 2101.7888 (MSE:0.0029, Reg:2101.7859) beta=17.30
Iter  9000 | Total loss: 1686.5104 (MSE:0.0025, Reg:1686.5078) beta=16.40
Iter 10000 | Total loss: 1401.5356 (MSE:0.0028, Reg:1401.5328) beta=15.50
Iter 11000 | Total loss: 1147.6122 (MSE:0.0028, Reg:1147.6094) beta=14.60
Iter 12000 | Total loss: 963.7108 (MSE:0.0027, Reg:963.7080) beta=13.70
Iter 13000 | Total loss: 778.0910 (MSE:0.0029, Reg:778.0881) beta=12.80
Iter 14000 | Total loss: 611.8333 (MSE:0.0031, Reg:611.8301) beta=11.90
Iter 15000 | Total loss: 450.6483 (MSE:0.0028, Reg:450.6455) beta=11.00
Iter 16000 | Total loss: 293.9500 (MSE:0.0027, Reg:293.9474) beta=10.10
Iter 17000 | Total loss: 173.1363 (MSE:0.0029, Reg:173.1334) beta=9.20
Iter 18000 | Total loss: 79.2288 (MSE:0.0025, Reg:79.2263) beta=8.30
Iter 19000 | Total loss: 27.0026 (MSE:0.0026, Reg:27.0000) beta=7.40
Iter 20000 | Total loss: 7.0027 (MSE:0.0027, Reg:7.0000) beta=6.50
Iter 20409 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=6.13 ... Early stopping

encoder.layers.encoder_layer_5.self_attention.in_proj
   FP_OUTPUT shape torch.Size([1024, 197, 2304])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([2304, 768])
Iter     1 | Total loss: 0.0185 (MSE:0.0185, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0137 (MSE:0.0137, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0120 (MSE:0.0120, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0135 (MSE:0.0135, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0162 (MSE:0.0162, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 472424.7500 (MSE:0.0125, Reg:472424.7500) beta=20.00
Iter  6000 | Total loss: 64871.0000 (MSE:0.0124, Reg:64870.9883) beta=19.10
Iter  7000 | Total loss: 44649.0195 (MSE:0.0127, Reg:44649.0078) beta=18.20
Iter  8000 | Total loss: 34795.2383 (MSE:0.0132, Reg:34795.2266) beta=17.30
Iter  9000 | Total loss: 28230.7402 (MSE:0.0160, Reg:28230.7246) beta=16.40
Iter 10000 | Total loss: 23130.0898 (MSE:0.0160, Reg:23130.0742) beta=15.50
Iter 11000 | Total loss: 18702.2109 (MSE:0.0154, Reg:18702.1953) beta=14.60
Iter 12000 | Total loss: 14669.1357 (MSE:0.0131, Reg:14669.1230) beta=13.70
Iter 13000 | Total loss: 11112.7314 (MSE:0.0137, Reg:11112.7178) beta=12.80
Iter 14000 | Total loss: 7761.7407 (MSE:0.0152, Reg:7761.7256) beta=11.90
Iter 15000 | Total loss: 5130.6406 (MSE:0.0137, Reg:5130.6270) beta=11.00
Iter 16000 | Total loss: 3101.9836 (MSE:0.0142, Reg:3101.9695) beta=10.10
Iter 17000 | Total loss: 1580.6553 (MSE:0.0158, Reg:1580.6395) beta=9.20
Iter 18000 | Total loss: 596.3158 (MSE:0.0149, Reg:596.3009) beta=8.30
Iter 19000 | Total loss: 141.5723 (MSE:0.0139, Reg:141.5584) beta=7.40
Iter 20000 | Total loss: 4.0156 (MSE:0.0156, Reg:4.0000) beta=6.50
Iter 20940 | Total loss: 0.0119 (MSE:0.0119, Reg:0.0000) beta=5.65 ... Early stopping

encoder.layers.encoder_layer_5.self_attention.out_proj
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 768])
Iter     1 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 168780.8438 (MSE:0.0009, Reg:168780.8438) beta=20.00
Iter  6000 | Total loss: 11026.7803 (MSE:0.0008, Reg:11026.7793) beta=19.10
Iter  7000 | Total loss: 6520.2744 (MSE:0.0009, Reg:6520.2734) beta=18.20
Iter  8000 | Total loss: 4508.4707 (MSE:0.0009, Reg:4508.4697) beta=17.30
Iter  9000 | Total loss: 3394.2075 (MSE:0.0008, Reg:3394.2068) beta=16.40
Iter 10000 | Total loss: 2646.8923 (MSE:0.0009, Reg:2646.8914) beta=15.50
Iter 11000 | Total loss: 2081.5737 (MSE:0.0010, Reg:2081.5728) beta=14.60
Iter 12000 | Total loss: 1634.4100 (MSE:0.0008, Reg:1634.4092) beta=13.70
Iter 13000 | Total loss: 1228.6666 (MSE:0.0008, Reg:1228.6658) beta=12.80
Iter 14000 | Total loss: 896.3148 (MSE:0.0009, Reg:896.3138) beta=11.90
Iter 15000 | Total loss: 635.2771 (MSE:0.0009, Reg:635.2762) beta=11.00
Iter 16000 | Total loss: 430.6517 (MSE:0.0009, Reg:430.6508) beta=10.10
Iter 17000 | Total loss: 242.4559 (MSE:0.0009, Reg:242.4550) beta=9.20
Iter 18000 | Total loss: 100.9841 (MSE:0.0008, Reg:100.9833) beta=8.30
Iter 19000 | Total loss: 27.9798 (MSE:0.0009, Reg:27.9789) beta=7.40
Iter 20000 | Total loss: 2.0009 (MSE:0.0009, Reg:2.0000) beta=6.50
Iter 20470 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=6.08 ... Early stopping

encoder.layers.encoder_layer_5.mlp.linear_1
   FP_OUTPUT shape torch.Size([1024, 197, 3072])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([3072, 768])
Iter     1 | Total loss: 0.0285 (MSE:0.0285, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0228 (MSE:0.0228, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0238 (MSE:0.0238, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0262 (MSE:0.0262, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0231 (MSE:0.0231, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 529800.3750 (MSE:0.0252, Reg:529800.3750) beta=20.00
Iter  6000 | Total loss: 80158.0547 (MSE:0.0253, Reg:80158.0312) beta=19.10
Iter  7000 | Total loss: 59223.7344 (MSE:0.0265, Reg:59223.7070) beta=18.20
Iter  8000 | Total loss: 48338.5938 (MSE:0.0242, Reg:48338.5703) beta=17.30
Iter  9000 | Total loss: 40246.4805 (MSE:0.0259, Reg:40246.4531) beta=16.40
Iter 10000 | Total loss: 33492.1680 (MSE:0.0251, Reg:33492.1445) beta=15.50
Iter 11000 | Total loss: 27156.6094 (MSE:0.0282, Reg:27156.5820) beta=14.60
Iter 12000 | Total loss: 21158.4492 (MSE:0.0234, Reg:21158.4258) beta=13.70
Iter 13000 | Total loss: 15731.1328 (MSE:0.0274, Reg:15731.1055) beta=12.80
Iter 14000 | Total loss: 10874.6650 (MSE:0.0252, Reg:10874.6396) beta=11.90
Iter 15000 | Total loss: 6847.5449 (MSE:0.0254, Reg:6847.5195) beta=11.00
Iter 16000 | Total loss: 3755.3721 (MSE:0.0240, Reg:3755.3481) beta=10.10
Iter 17000 | Total loss: 1624.8776 (MSE:0.0269, Reg:1624.8506) beta=9.20
Iter 18000 | Total loss: 404.2743 (MSE:0.0278, Reg:404.2464) beta=8.30
Iter 19000 | Total loss: 30.3100 (MSE:0.0249, Reg:30.2851) beta=7.40
Iter 19508 | Total loss: 0.0257 (MSE:0.0257, Reg:0.0000) beta=6.94 ... Early stopping

encoder.layers.encoder_layer_5.mlp.linear_2
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 3072])
Iter     1 | Total loss: 0.0263 (MSE:0.0263, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0229 (MSE:0.0229, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0202 (MSE:0.0202, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0219 (MSE:0.0219, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0339 (MSE:0.0339, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 234028.5000 (MSE:0.0209, Reg:234028.4844) beta=20.00
Iter  6000 | Total loss: 6415.1533 (MSE:0.0196, Reg:6415.1338) beta=19.10
Iter  7000 | Total loss: 4083.8232 (MSE:0.0273, Reg:4083.7959) beta=18.20
Iter  8000 | Total loss: 3060.7805 (MSE:0.0194, Reg:3060.7612) beta=17.30
Iter  9000 | Total loss: 2443.9507 (MSE:0.0220, Reg:2443.9287) beta=16.40
Iter 10000 | Total loss: 1954.7423 (MSE:0.0263, Reg:1954.7161) beta=15.50
Iter 11000 | Total loss: 1621.0201 (MSE:0.0240, Reg:1620.9961) beta=14.60
Iter 12000 | Total loss: 1287.5382 (MSE:0.0253, Reg:1287.5129) beta=13.70
Iter 13000 | Total loss: 970.6580 (MSE:0.0295, Reg:970.6285) beta=12.80
Iter 14000 | Total loss: 700.8934 (MSE:0.0176, Reg:700.8757) beta=11.90
Iter 15000 | Total loss: 459.6835 (MSE:0.0262, Reg:459.6573) beta=11.00
Iter 16000 | Total loss: 298.1248 (MSE:0.0221, Reg:298.1027) beta=10.10
Iter 17000 | Total loss: 155.3948 (MSE:0.0189, Reg:155.3759) beta=9.20
Iter 18000 | Total loss: 63.5713 (MSE:0.0278, Reg:63.5435) beta=8.30
Iter 19000 | Total loss: 13.0200 (MSE:0.0200, Reg:13.0000) beta=7.40
Iter 20000 | Total loss: 2.0263 (MSE:0.0263, Reg:2.0000) beta=6.50
Iter 20594 | Total loss: 0.0225 (MSE:0.0225, Reg:0.0000) beta=5.97 ... Early stopping

encoder.layers.encoder_layer_6.self_attention.in_proj
   FP_OUTPUT shape torch.Size([1024, 197, 2304])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([2304, 768])
Iter     1 | Total loss: 0.0271 (MSE:0.0271, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0201 (MSE:0.0201, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0255 (MSE:0.0255, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0207 (MSE:0.0207, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0224 (MSE:0.0224, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 507810.7500 (MSE:0.0239, Reg:507810.7188) beta=20.00
Iter  6000 | Total loss: 81762.5547 (MSE:0.0230, Reg:81762.5312) beta=19.10
Iter  7000 | Total loss: 58627.0352 (MSE:0.0222, Reg:58627.0117) beta=18.20
Iter  8000 | Total loss: 46441.9844 (MSE:0.0232, Reg:46441.9609) beta=17.30
Iter  9000 | Total loss: 38283.2148 (MSE:0.0224, Reg:38283.1914) beta=16.40
Iter 10000 | Total loss: 31452.8906 (MSE:0.0253, Reg:31452.8652) beta=15.50
Iter 11000 | Total loss: 25308.4863 (MSE:0.0218, Reg:25308.4648) beta=14.60
Iter 12000 | Total loss: 19751.9707 (MSE:0.0222, Reg:19751.9492) beta=13.70
Iter 13000 | Total loss: 14795.8906 (MSE:0.0251, Reg:14795.8652) beta=12.80
Iter 14000 | Total loss: 10274.3467 (MSE:0.0232, Reg:10274.3232) beta=11.90
Iter 15000 | Total loss: 6462.8784 (MSE:0.0231, Reg:6462.8555) beta=11.00
Iter 16000 | Total loss: 3700.9922 (MSE:0.0202, Reg:3700.9719) beta=10.10
Iter 17000 | Total loss: 1789.2145 (MSE:0.0212, Reg:1789.1934) beta=9.20
Iter 18000 | Total loss: 557.7690 (MSE:0.0233, Reg:557.7457) beta=8.30
Iter 19000 | Total loss: 87.2244 (MSE:0.0223, Reg:87.2021) beta=7.40
Iter 20000 | Total loss: 4.0198 (MSE:0.0211, Reg:3.9987) beta=6.50
Iter 20154 | Total loss: 0.0236 (MSE:0.0236, Reg:0.0000) beta=6.36 ... Early stopping

encoder.layers.encoder_layer_6.self_attention.out_proj
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 768])
Iter     1 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 164238.8750 (MSE:0.0015, Reg:164238.8750) beta=20.00
Iter  6000 | Total loss: 18267.8281 (MSE:0.0015, Reg:18267.8262) beta=19.10
Iter  7000 | Total loss: 11698.2812 (MSE:0.0018, Reg:11698.2793) beta=18.20
Iter  8000 | Total loss: 8316.8242 (MSE:0.0014, Reg:8316.8232) beta=17.30
Iter  9000 | Total loss: 6239.0962 (MSE:0.0016, Reg:6239.0947) beta=16.40
Iter 10000 | Total loss: 4770.6143 (MSE:0.0017, Reg:4770.6128) beta=15.50
Iter 11000 | Total loss: 3712.6543 (MSE:0.0017, Reg:3712.6526) beta=14.60
Iter 12000 | Total loss: 2793.8843 (MSE:0.0016, Reg:2793.8828) beta=13.70
Iter 13000 | Total loss: 2001.4955 (MSE:0.0017, Reg:2001.4938) beta=12.80
Iter 14000 | Total loss: 1398.7511 (MSE:0.0015, Reg:1398.7495) beta=11.90
Iter 15000 | Total loss: 906.0482 (MSE:0.0015, Reg:906.0466) beta=11.00
Iter 16000 | Total loss: 517.8966 (MSE:0.0020, Reg:517.8947) beta=10.10
Iter 17000 | Total loss: 238.9031 (MSE:0.0017, Reg:238.9013) beta=9.20
Iter 18000 | Total loss: 68.1465 (MSE:0.0016, Reg:68.1449) beta=8.30
Iter 19000 | Total loss: 18.0015 (MSE:0.0015, Reg:18.0000) beta=7.40
Iter 20000 | Total loss: 1.0015 (MSE:0.0015, Reg:1.0000) beta=6.50
Iter 20258 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=6.27 ... Early stopping

encoder.layers.encoder_layer_6.mlp.linear_1
   FP_OUTPUT shape torch.Size([1024, 197, 3072])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([3072, 768])
Iter     1 | Total loss: 0.0344 (MSE:0.0344, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0305 (MSE:0.0305, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0314 (MSE:0.0314, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0315 (MSE:0.0315, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0341 (MSE:0.0341, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 545439.6875 (MSE:0.0286, Reg:545439.6875) beta=20.00
Iter  6000 | Total loss: 83717.3125 (MSE:0.0314, Reg:83717.2812) beta=19.10
Iter  7000 | Total loss: 62736.3281 (MSE:0.0311, Reg:62736.2969) beta=18.20
Iter  8000 | Total loss: 51281.0078 (MSE:0.0283, Reg:51280.9805) beta=17.30
Iter  9000 | Total loss: 42998.0000 (MSE:0.0325, Reg:42997.9688) beta=16.40
Iter 10000 | Total loss: 35835.3086 (MSE:0.0308, Reg:35835.2773) beta=15.50
Iter 11000 | Total loss: 29266.1562 (MSE:0.0308, Reg:29266.1250) beta=14.60
Iter 12000 | Total loss: 22714.8828 (MSE:0.0348, Reg:22714.8477) beta=13.70
Iter 13000 | Total loss: 16703.6133 (MSE:0.0330, Reg:16703.5801) beta=12.80
Iter 14000 | Total loss: 11302.2773 (MSE:0.0327, Reg:11302.2441) beta=11.90
Iter 15000 | Total loss: 6864.5518 (MSE:0.0336, Reg:6864.5181) beta=11.00
Iter 16000 | Total loss: 3549.8357 (MSE:0.0319, Reg:3549.8037) beta=10.10
Iter 17000 | Total loss: 1305.5521 (MSE:0.0308, Reg:1305.5212) beta=9.20
Iter 18000 | Total loss: 275.5396 (MSE:0.0287, Reg:275.5109) beta=8.30
Iter 19000 | Total loss: 9.9016 (MSE:0.0322, Reg:9.8693) beta=7.40
Iter 20000 | Total loss: 0.4287 (MSE:0.0335, Reg:0.3952) beta=6.50
Iter 20005 | Total loss: 0.0332 (MSE:0.0332, Reg:0.0000) beta=6.50 ... Early stopping

encoder.layers.encoder_layer_6.mlp.linear_2
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 3072])
Iter     1 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 195281.2031 (MSE:0.0032, Reg:195281.2031) beta=20.00
Iter  6000 | Total loss: 5716.8789 (MSE:0.0030, Reg:5716.8760) beta=19.10
Iter  7000 | Total loss: 3666.4119 (MSE:0.0037, Reg:3666.4082) beta=18.20
Iter  8000 | Total loss: 2730.1873 (MSE:0.0033, Reg:2730.1838) beta=17.30
Iter  9000 | Total loss: 2166.7236 (MSE:0.0035, Reg:2166.7202) beta=16.40
Iter 10000 | Total loss: 1811.8754 (MSE:0.0034, Reg:1811.8719) beta=15.50
Iter 11000 | Total loss: 1488.3246 (MSE:0.0036, Reg:1488.3210) beta=14.60
Iter 12000 | Total loss: 1208.3064 (MSE:0.0038, Reg:1208.3026) beta=13.70
Iter 13000 | Total loss: 935.6021 (MSE:0.0038, Reg:935.5983) beta=12.80
Iter 14000 | Total loss: 695.7255 (MSE:0.0035, Reg:695.7220) beta=11.90
Iter 15000 | Total loss: 465.0194 (MSE:0.0040, Reg:465.0154) beta=11.00
Iter 16000 | Total loss: 274.0260 (MSE:0.0038, Reg:274.0222) beta=10.10
Iter 17000 | Total loss: 128.3978 (MSE:0.0033, Reg:128.3945) beta=9.20
Iter 18000 | Total loss: 43.6128 (MSE:0.0034, Reg:43.6095) beta=8.30
Iter 19000 | Total loss: 13.0032 (MSE:0.0032, Reg:13.0000) beta=7.40
Iter 20000 | Total loss: 2.0034 (MSE:0.0034, Reg:2.0000) beta=6.50
Iter 20500 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=6.05 ... Early stopping

encoder.layers.encoder_layer_7.self_attention.in_proj
   FP_OUTPUT shape torch.Size([1024, 197, 2304])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([2304, 768])
Iter     1 | Total loss: 0.0217 (MSE:0.0217, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0208 (MSE:0.0208, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0225 (MSE:0.0225, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0229 (MSE:0.0229, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0203 (MSE:0.0203, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 560152.3750 (MSE:0.0202, Reg:560152.3750) beta=20.00
Iter  6000 | Total loss: 95662.9141 (MSE:0.0207, Reg:95662.8906) beta=19.10
Iter  7000 | Total loss: 70307.0312 (MSE:0.0219, Reg:70307.0078) beta=18.20
Iter  8000 | Total loss: 56530.6172 (MSE:0.0218, Reg:56530.5938) beta=17.30
Iter  9000 | Total loss: 46557.9805 (MSE:0.0206, Reg:46557.9609) beta=16.40
Iter 10000 | Total loss: 38206.3945 (MSE:0.0202, Reg:38206.3750) beta=15.50
Iter 11000 | Total loss: 30776.1562 (MSE:0.0213, Reg:30776.1348) beta=14.60
Iter 12000 | Total loss: 24059.1797 (MSE:0.0197, Reg:24059.1602) beta=13.70
Iter 13000 | Total loss: 18010.0547 (MSE:0.0223, Reg:18010.0332) beta=12.80
Iter 14000 | Total loss: 12457.9092 (MSE:0.0225, Reg:12457.8867) beta=11.90
Iter 15000 | Total loss: 7839.1641 (MSE:0.0209, Reg:7839.1431) beta=11.00
Iter 16000 | Total loss: 4358.9595 (MSE:0.0226, Reg:4358.9370) beta=10.10
Iter 17000 | Total loss: 1977.7269 (MSE:0.0263, Reg:1977.7007) beta=9.20
Iter 18000 | Total loss: 602.0588 (MSE:0.0218, Reg:602.0371) beta=8.30
Iter 19000 | Total loss: 90.2168 (MSE:0.0219, Reg:90.1949) beta=7.40
Iter 20000 | Total loss: 4.0218 (MSE:0.0218, Reg:4.0000) beta=6.50
Iter 20340 | Total loss: 0.0225 (MSE:0.0225, Reg:0.0000) beta=6.19 ... Early stopping

encoder.layers.encoder_layer_7.self_attention.out_proj
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 768])
Iter     1 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 151366.0000 (MSE:0.0014, Reg:151366.0000) beta=20.00
Iter  6000 | Total loss: 19707.8809 (MSE:0.0014, Reg:19707.8789) beta=19.10
Iter  7000 | Total loss: 12843.0000 (MSE:0.0013, Reg:12842.9990) beta=18.20
Iter  8000 | Total loss: 9287.8594 (MSE:0.0015, Reg:9287.8574) beta=17.30
Iter  9000 | Total loss: 7036.0215 (MSE:0.0013, Reg:7036.0200) beta=16.40
Iter 10000 | Total loss: 5429.4116 (MSE:0.0012, Reg:5429.4106) beta=15.50
Iter 11000 | Total loss: 4132.4995 (MSE:0.0014, Reg:4132.4980) beta=14.60
Iter 12000 | Total loss: 3080.4822 (MSE:0.0012, Reg:3080.4810) beta=13.70
Iter 13000 | Total loss: 2249.2197 (MSE:0.0014, Reg:2249.2183) beta=12.80
Iter 14000 | Total loss: 1526.4818 (MSE:0.0014, Reg:1526.4805) beta=11.90
Iter 15000 | Total loss: 966.0073 (MSE:0.0013, Reg:966.0060) beta=11.00
Iter 16000 | Total loss: 546.7209 (MSE:0.0012, Reg:546.7197) beta=10.10
Iter 17000 | Total loss: 237.0673 (MSE:0.0013, Reg:237.0660) beta=9.20
Iter 18000 | Total loss: 78.9728 (MSE:0.0013, Reg:78.9715) beta=8.30
Iter 19000 | Total loss: 12.0013 (MSE:0.0013, Reg:12.0000) beta=7.40
Iter 19627 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=6.84 ... Early stopping

encoder.layers.encoder_layer_7.mlp.linear_1
   FP_OUTPUT shape torch.Size([1024, 197, 3072])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([3072, 768])
Iter     1 | Total loss: 0.0441 (MSE:0.0441, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0379 (MSE:0.0379, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0371 (MSE:0.0371, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0386 (MSE:0.0386, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0369 (MSE:0.0369, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 576826.3125 (MSE:0.0376, Reg:576826.2500) beta=20.00
Iter  6000 | Total loss: 95542.5547 (MSE:0.0481, Reg:95542.5078) beta=19.10
Iter  7000 | Total loss: 72901.3047 (MSE:0.0360, Reg:72901.2656) beta=18.20
Iter  8000 | Total loss: 60133.1367 (MSE:0.0373, Reg:60133.0977) beta=17.30
Iter  9000 | Total loss: 50705.8203 (MSE:0.0401, Reg:50705.7812) beta=16.40
Iter 10000 | Total loss: 42344.7539 (MSE:0.0370, Reg:42344.7188) beta=15.50
Iter 11000 | Total loss: 34443.8906 (MSE:0.0351, Reg:34443.8555) beta=14.60
Iter 12000 | Total loss: 26764.3555 (MSE:0.0387, Reg:26764.3164) beta=13.70
Iter 13000 | Total loss: 19761.8867 (MSE:0.0394, Reg:19761.8477) beta=12.80
Iter 14000 | Total loss: 13317.8428 (MSE:0.0378, Reg:13317.8047) beta=11.90
Iter 15000 | Total loss: 7783.0249 (MSE:0.0396, Reg:7782.9854) beta=11.00
Iter 16000 | Total loss: 3645.2422 (MSE:0.0418, Reg:3645.2004) beta=10.10
Iter 17000 | Total loss: 1176.1862 (MSE:0.0364, Reg:1176.1498) beta=9.20
Iter 18000 | Total loss: 170.9089 (MSE:0.0360, Reg:170.8730) beta=8.30
Iter 19000 | Total loss: 4.9810 (MSE:0.0409, Reg:4.9401) beta=7.40
Iter 19305 | Total loss: 0.0428 (MSE:0.0428, Reg:0.0000) beta=7.13 ... Early stopping

encoder.layers.encoder_layer_7.mlp.linear_2
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 3072])
Iter     1 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 178881.8438 (MSE:0.0044, Reg:178881.8438) beta=20.00
Iter  6000 | Total loss: 6610.1523 (MSE:0.0050, Reg:6610.1475) beta=19.10
Iter  7000 | Total loss: 4601.5703 (MSE:0.0048, Reg:4601.5654) beta=18.20
Iter  8000 | Total loss: 3506.4045 (MSE:0.0048, Reg:3506.3999) beta=17.30
Iter  9000 | Total loss: 2833.5605 (MSE:0.0043, Reg:2833.5562) beta=16.40
Iter 10000 | Total loss: 2348.9402 (MSE:0.0045, Reg:2348.9358) beta=15.50
Iter 11000 | Total loss: 1937.3971 (MSE:0.0046, Reg:1937.3926) beta=14.60
Iter 12000 | Total loss: 1526.8097 (MSE:0.0047, Reg:1526.8051) beta=13.70
Iter 13000 | Total loss: 1213.5894 (MSE:0.0043, Reg:1213.5851) beta=12.80
Iter 14000 | Total loss: 880.0720 (MSE:0.0049, Reg:880.0670) beta=11.90
Iter 15000 | Total loss: 593.2173 (MSE:0.0045, Reg:593.2129) beta=11.00
Iter 16000 | Total loss: 320.6305 (MSE:0.0050, Reg:320.6255) beta=10.10
Iter 17000 | Total loss: 143.1249 (MSE:0.0050, Reg:143.1199) beta=9.20
Iter 18000 | Total loss: 32.0615 (MSE:0.0044, Reg:32.0571) beta=8.30
Iter 19000 | Total loss: 3.0048 (MSE:0.0050, Reg:2.9998) beta=7.40
Iter 19378 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=7.06 ... Early stopping

encoder.layers.encoder_layer_8.self_attention.in_proj
   FP_OUTPUT shape torch.Size([1024, 197, 2304])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([2304, 768])
Iter     1 | Total loss: 0.0238 (MSE:0.0238, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0192 (MSE:0.0192, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0180 (MSE:0.0180, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0197 (MSE:0.0197, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0232 (MSE:0.0232, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 605606.8750 (MSE:0.0207, Reg:605606.8750) beta=20.00
Iter  6000 | Total loss: 113153.3906 (MSE:0.0191, Reg:113153.3750) beta=19.10
Iter  7000 | Total loss: 85183.9922 (MSE:0.0221, Reg:85183.9688) beta=18.20
Iter  8000 | Total loss: 69344.2656 (MSE:0.0229, Reg:69344.2422) beta=17.30
Iter  9000 | Total loss: 57807.9336 (MSE:0.0222, Reg:57807.9102) beta=16.40
Iter 10000 | Total loss: 47918.1172 (MSE:0.0241, Reg:47918.0938) beta=15.50
Iter 11000 | Total loss: 39138.2617 (MSE:0.0185, Reg:39138.2422) beta=14.60
Iter 12000 | Total loss: 30881.0488 (MSE:0.0224, Reg:30881.0273) beta=13.70
Iter 13000 | Total loss: 23335.5078 (MSE:0.0194, Reg:23335.4883) beta=12.80
Iter 14000 | Total loss: 16456.2344 (MSE:0.0180, Reg:16456.2168) beta=11.90
Iter 15000 | Total loss: 10503.0156 (MSE:0.0196, Reg:10502.9961) beta=11.00
Iter 16000 | Total loss: 5713.3574 (MSE:0.0236, Reg:5713.3340) beta=10.10
Iter 17000 | Total loss: 2489.9446 (MSE:0.0226, Reg:2489.9219) beta=9.20
Iter 18000 | Total loss: 757.2429 (MSE:0.0215, Reg:757.2214) beta=8.30
Iter 19000 | Total loss: 111.1476 (MSE:0.0209, Reg:111.1266) beta=7.40
Iter 20000 | Total loss: 4.0213 (MSE:0.0213, Reg:4.0000) beta=6.50
Iter 20242 | Total loss: 0.0230 (MSE:0.0230, Reg:0.0000) beta=6.28 ... Early stopping

encoder.layers.encoder_layer_8.self_attention.out_proj
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 768])
Iter     1 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 145568.1406 (MSE:0.0011, Reg:145568.1406) beta=20.00
Iter  6000 | Total loss: 22299.6992 (MSE:0.0012, Reg:22299.6973) beta=19.10
Iter  7000 | Total loss: 15226.1143 (MSE:0.0012, Reg:15226.1133) beta=18.20
Iter  8000 | Total loss: 11151.5322 (MSE:0.0012, Reg:11151.5312) beta=17.30
Iter  9000 | Total loss: 8450.8818 (MSE:0.0012, Reg:8450.8809) beta=16.40
Iter 10000 | Total loss: 6499.3213 (MSE:0.0012, Reg:6499.3203) beta=15.50
Iter 11000 | Total loss: 4945.0283 (MSE:0.0012, Reg:4945.0273) beta=14.60
Iter 12000 | Total loss: 3662.2830 (MSE:0.0011, Reg:3662.2817) beta=13.70
Iter 13000 | Total loss: 2708.7510 (MSE:0.0013, Reg:2708.7498) beta=12.80
Iter 14000 | Total loss: 1892.4407 (MSE:0.0013, Reg:1892.4395) beta=11.90
Iter 15000 | Total loss: 1116.3192 (MSE:0.0011, Reg:1116.3181) beta=11.00
Iter 16000 | Total loss: 575.5729 (MSE:0.0012, Reg:575.5717) beta=10.10
Iter 17000 | Total loss: 235.8650 (MSE:0.0013, Reg:235.8636) beta=9.20
Iter 18000 | Total loss: 63.0993 (MSE:0.0012, Reg:63.0981) beta=8.30
Iter 19000 | Total loss: 8.0013 (MSE:0.0013, Reg:8.0000) beta=7.40
Iter 19695 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=6.77 ... Early stopping

encoder.layers.encoder_layer_8.mlp.linear_1
   FP_OUTPUT shape torch.Size([1024, 197, 3072])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([3072, 768])
Iter     1 | Total loss: 0.0520 (MSE:0.0520, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0440 (MSE:0.0440, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0431 (MSE:0.0431, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0416 (MSE:0.0416, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0459 (MSE:0.0459, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 565223.0625 (MSE:0.0404, Reg:565223.0000) beta=20.00
Iter  6000 | Total loss: 97809.7422 (MSE:0.0437, Reg:97809.6953) beta=19.10
Iter  7000 | Total loss: 77031.6172 (MSE:0.0471, Reg:77031.5703) beta=18.20
Iter  8000 | Total loss: 64481.2812 (MSE:0.0396, Reg:64481.2422) beta=17.30
Iter  9000 | Total loss: 54859.2969 (MSE:0.0477, Reg:54859.2500) beta=16.40
Iter 10000 | Total loss: 46018.0117 (MSE:0.0531, Reg:46017.9570) beta=15.50
Iter 11000 | Total loss: 37795.3555 (MSE:0.0527, Reg:37795.3047) beta=14.60
Iter 12000 | Total loss: 29871.7930 (MSE:0.0486, Reg:29871.7441) beta=13.70
Iter 13000 | Total loss: 21956.2266 (MSE:0.0467, Reg:21956.1797) beta=12.80
Iter 14000 | Total loss: 14724.8281 (MSE:0.0479, Reg:14724.7803) beta=11.90
Iter 15000 | Total loss: 8599.4951 (MSE:0.0464, Reg:8599.4482) beta=11.00
Iter 16000 | Total loss: 3833.0037 (MSE:0.0495, Reg:3832.9541) beta=10.10
Iter 17000 | Total loss: 1153.1078 (MSE:0.0611, Reg:1153.0468) beta=9.20
Iter 18000 | Total loss: 169.1732 (MSE:0.0442, Reg:169.1289) beta=8.30
Iter 19000 | Total loss: 3.0020 (MSE:0.0469, Reg:2.9551) beta=7.40
Iter 19179 | Total loss: 0.0468 (MSE:0.0468, Reg:0.0000) beta=7.24 ... Early stopping

encoder.layers.encoder_layer_8.mlp.linear_2
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 3072])
Iter     1 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 146458.7188 (MSE:0.0035, Reg:146458.7188) beta=20.00
Iter  6000 | Total loss: 4092.3953 (MSE:0.0040, Reg:4092.3914) beta=19.10
Iter  7000 | Total loss: 2884.1101 (MSE:0.0036, Reg:2884.1064) beta=18.20
Iter  8000 | Total loss: 2259.7317 (MSE:0.0037, Reg:2259.7280) beta=17.30
Iter  9000 | Total loss: 1900.0972 (MSE:0.0039, Reg:1900.0933) beta=16.40
Iter 10000 | Total loss: 1608.8301 (MSE:0.0040, Reg:1608.8260) beta=15.50
Iter 11000 | Total loss: 1327.8862 (MSE:0.0042, Reg:1327.8821) beta=14.60
Iter 12000 | Total loss: 1105.0648 (MSE:0.0040, Reg:1105.0608) beta=13.70
Iter 13000 | Total loss: 872.9626 (MSE:0.0041, Reg:872.9586) beta=12.80
Iter 14000 | Total loss: 658.4491 (MSE:0.0041, Reg:658.4450) beta=11.90
Iter 15000 | Total loss: 431.8434 (MSE:0.0039, Reg:431.8394) beta=11.00
Iter 16000 | Total loss: 273.8196 (MSE:0.0040, Reg:273.8156) beta=10.10
Iter 17000 | Total loss: 128.1081 (MSE:0.0036, Reg:128.1045) beta=9.20
Iter 18000 | Total loss: 46.2168 (MSE:0.0039, Reg:46.2130) beta=8.30
Iter 19000 | Total loss: 4.0039 (MSE:0.0039, Reg:4.0000) beta=7.40
Iter 19803 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=6.68 ... Early stopping

encoder.layers.encoder_layer_9.self_attention.in_proj
   FP_OUTPUT shape torch.Size([1024, 197, 2304])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([2304, 768])
Iter     1 | Total loss: 0.0134 (MSE:0.0134, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0125 (MSE:0.0125, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0121 (MSE:0.0121, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0115 (MSE:0.0115, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0116 (MSE:0.0116, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 591035.5000 (MSE:0.0124, Reg:591035.5000) beta=20.00
Iter  6000 | Total loss: 107280.5547 (MSE:0.0140, Reg:107280.5391) beta=19.10
Iter  7000 | Total loss: 83254.8750 (MSE:0.0140, Reg:83254.8594) beta=18.20
Iter  8000 | Total loss: 68892.4922 (MSE:0.0126, Reg:68892.4766) beta=17.30
Iter  9000 | Total loss: 58375.1602 (MSE:0.0137, Reg:58375.1484) beta=16.40
Iter 10000 | Total loss: 49445.6680 (MSE:0.0122, Reg:49445.6562) beta=15.50
Iter 11000 | Total loss: 41475.3516 (MSE:0.0140, Reg:41475.3359) beta=14.60
Iter 12000 | Total loss: 33795.7578 (MSE:0.0125, Reg:33795.7461) beta=13.70
Iter 13000 | Total loss: 26441.2480 (MSE:0.0132, Reg:26441.2344) beta=12.80
Iter 14000 | Total loss: 19514.7812 (MSE:0.0148, Reg:19514.7656) beta=11.90
Iter 15000 | Total loss: 13010.1455 (MSE:0.0114, Reg:13010.1338) beta=11.00
Iter 16000 | Total loss: 7791.8042 (MSE:0.0132, Reg:7791.7910) beta=10.10
Iter 17000 | Total loss: 3886.0862 (MSE:0.0130, Reg:3886.0732) beta=9.20
Iter 18000 | Total loss: 1400.0077 (MSE:0.0155, Reg:1399.9922) beta=8.30
Iter 19000 | Total loss: 300.3827 (MSE:0.0148, Reg:300.3679) beta=7.40
Iter 20000 | Total loss: 17.0793 (MSE:0.0121, Reg:17.0672) beta=6.50
Iter 20797 | Total loss: 0.0131 (MSE:0.0131, Reg:0.0000) beta=5.78 ... Early stopping

encoder.layers.encoder_layer_9.self_attention.out_proj
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 768])
Iter     1 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 139665.4531 (MSE:0.0004, Reg:139665.4531) beta=20.00
Iter  6000 | Total loss: 17268.7344 (MSE:0.0005, Reg:17268.7344) beta=19.10
Iter  7000 | Total loss: 10739.2402 (MSE:0.0004, Reg:10739.2402) beta=18.20
Iter  8000 | Total loss: 7318.6157 (MSE:0.0004, Reg:7318.6152) beta=17.30
Iter  9000 | Total loss: 5306.3555 (MSE:0.0005, Reg:5306.3550) beta=16.40
Iter 10000 | Total loss: 4020.9355 (MSE:0.0004, Reg:4020.9351) beta=15.50
Iter 11000 | Total loss: 3089.2532 (MSE:0.0005, Reg:3089.2527) beta=14.60
Iter 12000 | Total loss: 2377.4541 (MSE:0.0005, Reg:2377.4536) beta=13.70
Iter 13000 | Total loss: 1827.7938 (MSE:0.0004, Reg:1827.7935) beta=12.80
Iter 14000 | Total loss: 1286.2355 (MSE:0.0004, Reg:1286.2350) beta=11.90
Iter 15000 | Total loss: 842.8201 (MSE:0.0005, Reg:842.8196) beta=11.00
Iter 16000 | Total loss: 502.4438 (MSE:0.0004, Reg:502.4434) beta=10.10
Iter 17000 | Total loss: 268.8025 (MSE:0.0004, Reg:268.8020) beta=9.20
Iter 18000 | Total loss: 102.4905 (MSE:0.0004, Reg:102.4900) beta=8.30
Iter 19000 | Total loss: 8.0004 (MSE:0.0004, Reg:8.0000) beta=7.40
Iter 19868 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=6.62 ... Early stopping

encoder.layers.encoder_layer_9.mlp.linear_1
   FP_OUTPUT shape torch.Size([1024, 197, 3072])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([3072, 768])
Iter     1 | Total loss: 0.0457 (MSE:0.0457, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0323 (MSE:0.0323, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0327 (MSE:0.0327, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0349 (MSE:0.0349, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0337 (MSE:0.0337, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 609243.8750 (MSE:0.0302, Reg:609243.8750) beta=20.00
Iter  6000 | Total loss: 104783.4844 (MSE:0.0385, Reg:104783.4453) beta=19.10
Iter  7000 | Total loss: 83700.0234 (MSE:0.0402, Reg:83699.9844) beta=18.20
Iter  8000 | Total loss: 71459.8516 (MSE:0.0328, Reg:71459.8203) beta=17.30
Iter  9000 | Total loss: 61729.4375 (MSE:0.0418, Reg:61729.3945) beta=16.40
Iter 10000 | Total loss: 52618.7383 (MSE:0.0341, Reg:52618.7031) beta=15.50
Iter 11000 | Total loss: 43537.7070 (MSE:0.0356, Reg:43537.6719) beta=14.60
Iter 12000 | Total loss: 34693.5625 (MSE:0.0354, Reg:34693.5273) beta=13.70
Iter 13000 | Total loss: 26132.8242 (MSE:0.0354, Reg:26132.7891) beta=12.80
Iter 14000 | Total loss: 17773.7559 (MSE:0.0342, Reg:17773.7207) beta=11.90
Iter 15000 | Total loss: 10507.3340 (MSE:0.0330, Reg:10507.3008) beta=11.00
Iter 16000 | Total loss: 4963.8647 (MSE:0.0366, Reg:4963.8281) beta=10.10
Iter 17000 | Total loss: 1647.7343 (MSE:0.0356, Reg:1647.6987) beta=9.20
Iter 18000 | Total loss: 264.8434 (MSE:0.0356, Reg:264.8078) beta=8.30
Iter 19000 | Total loss: 6.8963 (MSE:0.0352, Reg:6.8611) beta=7.40
Iter 19646 | Total loss: 0.0353 (MSE:0.0353, Reg:0.0000) beta=6.82 ... Early stopping

encoder.layers.encoder_layer_9.mlp.linear_2
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 3072])
Iter     1 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 145742.2188 (MSE:0.0032, Reg:145742.2188) beta=20.00
Iter  6000 | Total loss: 2521.9814 (MSE:0.0032, Reg:2521.9783) beta=19.10
Iter  7000 | Total loss: 1721.8260 (MSE:0.0036, Reg:1721.8225) beta=18.20
Iter  8000 | Total loss: 1363.8733 (MSE:0.0033, Reg:1363.8700) beta=17.30
Iter  9000 | Total loss: 1156.7264 (MSE:0.0033, Reg:1156.7231) beta=16.40
Iter 10000 | Total loss: 987.6812 (MSE:0.0034, Reg:987.6777) beta=15.50
Iter 11000 | Total loss: 821.0499 (MSE:0.0039, Reg:821.0459) beta=14.60
Iter 12000 | Total loss: 671.5880 (MSE:0.0033, Reg:671.5846) beta=13.70
Iter 13000 | Total loss: 524.7109 (MSE:0.0036, Reg:524.7073) beta=12.80
Iter 14000 | Total loss: 406.3003 (MSE:0.0035, Reg:406.2968) beta=11.90
Iter 15000 | Total loss: 282.3427 (MSE:0.0038, Reg:282.3389) beta=11.00
Iter 16000 | Total loss: 175.3849 (MSE:0.0034, Reg:175.3815) beta=10.10
Iter 17000 | Total loss: 89.0034 (MSE:0.0034, Reg:89.0000) beta=9.20
Iter 18000 | Total loss: 23.7392 (MSE:0.0036, Reg:23.7356) beta=8.30
Iter 19000 | Total loss: 5.0037 (MSE:0.0037, Reg:5.0000) beta=7.40
Iter 19825 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=6.66 ... Early stopping

encoder.layers.encoder_layer_10.self_attention.in_proj
   FP_OUTPUT shape torch.Size([1024, 197, 2304])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([2304, 768])
Iter     1 | Total loss: 0.0140 (MSE:0.0140, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0135 (MSE:0.0135, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0128 (MSE:0.0128, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0125 (MSE:0.0125, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0130 (MSE:0.0130, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 504687.6875 (MSE:0.0136, Reg:504687.6875) beta=20.00
Iter  6000 | Total loss: 75118.8906 (MSE:0.0144, Reg:75118.8750) beta=19.10
Iter  7000 | Total loss: 58375.7461 (MSE:0.0134, Reg:58375.7344) beta=18.20
Iter  8000 | Total loss: 48665.3984 (MSE:0.0126, Reg:48665.3867) beta=17.30
Iter  9000 | Total loss: 41586.2969 (MSE:0.0152, Reg:41586.2812) beta=16.40
Iter 10000 | Total loss: 35833.7930 (MSE:0.0135, Reg:35833.7812) beta=15.50
Iter 11000 | Total loss: 30378.5020 (MSE:0.0143, Reg:30378.4883) beta=14.60
Iter 12000 | Total loss: 25199.1621 (MSE:0.0144, Reg:25199.1484) beta=13.70
Iter 13000 | Total loss: 20273.9648 (MSE:0.0127, Reg:20273.9531) beta=12.80
Iter 14000 | Total loss: 15304.3867 (MSE:0.0116, Reg:15304.3750) beta=11.90
Iter 15000 | Total loss: 10606.8486 (MSE:0.0158, Reg:10606.8330) beta=11.00
Iter 16000 | Total loss: 6291.2627 (MSE:0.0146, Reg:6291.2480) beta=10.10
Iter 17000 | Total loss: 3238.6414 (MSE:0.0126, Reg:3238.6289) beta=9.20
Iter 18000 | Total loss: 1192.9631 (MSE:0.0128, Reg:1192.9503) beta=8.30
Iter 19000 | Total loss: 282.6569 (MSE:0.0142, Reg:282.6428) beta=7.40
Iter 20000 | Total loss: 17.4432 (MSE:0.0131, Reg:17.4301) beta=6.50
Iter 20362 | Total loss: 0.0131 (MSE:0.0131, Reg:0.0000) beta=6.17 ... Early stopping

encoder.layers.encoder_layer_10.self_attention.out_proj
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 768])
Iter     1 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 120830.8906 (MSE:0.0004, Reg:120830.8906) beta=20.00
Iter  6000 | Total loss: 9286.6289 (MSE:0.0004, Reg:9286.6289) beta=19.10
Iter  7000 | Total loss: 5216.6528 (MSE:0.0005, Reg:5216.6523) beta=18.20
Iter  8000 | Total loss: 3432.9810 (MSE:0.0005, Reg:3432.9805) beta=17.30
Iter  9000 | Total loss: 2467.9824 (MSE:0.0004, Reg:2467.9819) beta=16.40
Iter 10000 | Total loss: 1822.2532 (MSE:0.0004, Reg:1822.2527) beta=15.50
Iter 11000 | Total loss: 1403.9349 (MSE:0.0004, Reg:1403.9344) beta=14.60
Iter 12000 | Total loss: 1088.7554 (MSE:0.0004, Reg:1088.7549) beta=13.70
Iter 13000 | Total loss: 852.5607 (MSE:0.0005, Reg:852.5602) beta=12.80
Iter 14000 | Total loss: 602.6414 (MSE:0.0004, Reg:602.6410) beta=11.90
Iter 15000 | Total loss: 393.9995 (MSE:0.0005, Reg:393.9990) beta=11.00
Iter 16000 | Total loss: 252.3272 (MSE:0.0004, Reg:252.3268) beta=10.10
Iter 17000 | Total loss: 124.6729 (MSE:0.0004, Reg:124.6725) beta=9.20
Iter 18000 | Total loss: 47.5389 (MSE:0.0004, Reg:47.5384) beta=8.30
Iter 19000 | Total loss: 7.0004 (MSE:0.0004, Reg:7.0000) beta=7.40
Iter 19824 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=6.66 ... Early stopping

encoder.layers.encoder_layer_10.mlp.linear_1
   FP_OUTPUT shape torch.Size([1024, 197, 3072])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([3072, 768])
Iter     1 | Total loss: 0.0388 (MSE:0.0388, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0353 (MSE:0.0353, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0314 (MSE:0.0314, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0339 (MSE:0.0339, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0366 (MSE:0.0366, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 535768.3750 (MSE:0.0347, Reg:535768.3125) beta=20.00
Iter  6000 | Total loss: 88056.2422 (MSE:0.0370, Reg:88056.2031) beta=19.10
Iter  7000 | Total loss: 70961.5547 (MSE:0.0368, Reg:70961.5156) beta=18.20
Iter  8000 | Total loss: 60437.6328 (MSE:0.0354, Reg:60437.5977) beta=17.30
Iter  9000 | Total loss: 52271.0078 (MSE:0.0378, Reg:52270.9688) beta=16.40
Iter 10000 | Total loss: 44730.4336 (MSE:0.0326, Reg:44730.4023) beta=15.50
Iter 11000 | Total loss: 37430.5625 (MSE:0.0322, Reg:37430.5312) beta=14.60
Iter 12000 | Total loss: 30060.0020 (MSE:0.0382, Reg:30059.9629) beta=13.70
Iter 13000 | Total loss: 22529.5000 (MSE:0.0360, Reg:22529.4648) beta=12.80
Iter 14000 | Total loss: 15522.5322 (MSE:0.0376, Reg:15522.4941) beta=11.90
Iter 15000 | Total loss: 9279.2998 (MSE:0.0354, Reg:9279.2646) beta=11.00
Iter 16000 | Total loss: 4631.8125 (MSE:0.0390, Reg:4631.7734) beta=10.10
Iter 17000 | Total loss: 1678.2494 (MSE:0.0365, Reg:1678.2129) beta=9.20
Iter 18000 | Total loss: 287.4042 (MSE:0.0403, Reg:287.3640) beta=8.30
Iter 19000 | Total loss: 18.0347 (MSE:0.0350, Reg:17.9997) beta=7.40
Iter 19633 | Total loss: 0.0374 (MSE:0.0374, Reg:0.0000) beta=6.83 ... Early stopping

encoder.layers.encoder_layer_10.mlp.linear_2
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 3072])
Iter     1 | Total loss: 0.0259 (MSE:0.0259, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0186 (MSE:0.0186, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0210 (MSE:0.0210, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0213 (MSE:0.0213, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0188 (MSE:0.0188, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 112304.6953 (MSE:0.0238, Reg:112304.6719) beta=20.00
Iter  6000 | Total loss: 3702.1902 (MSE:0.0223, Reg:3702.1680) beta=19.10
Iter  7000 | Total loss: 2437.5310 (MSE:0.0192, Reg:2437.5117) beta=18.20
Iter  8000 | Total loss: 1887.2010 (MSE:0.0200, Reg:1887.1810) beta=17.30
Iter  9000 | Total loss: 1528.6627 (MSE:0.0218, Reg:1528.6409) beta=16.40
Iter 10000 | Total loss: 1249.5779 (MSE:0.0202, Reg:1249.5576) beta=15.50
Iter 11000 | Total loss: 1019.1956 (MSE:0.0178, Reg:1019.1778) beta=14.60
Iter 12000 | Total loss: 831.3835 (MSE:0.0179, Reg:831.3656) beta=13.70
Iter 13000 | Total loss: 624.7827 (MSE:0.0276, Reg:624.7551) beta=12.80
Iter 14000 | Total loss: 390.9550 (MSE:0.0252, Reg:390.9298) beta=11.90
Iter 15000 | Total loss: 216.2403 (MSE:0.0212, Reg:216.2190) beta=11.00
Iter 16000 | Total loss: 104.9831 (MSE:0.0228, Reg:104.9603) beta=10.10
Iter 17000 | Total loss: 50.2823 (MSE:0.0216, Reg:50.2607) beta=9.20
Iter 18000 | Total loss: 12.0230 (MSE:0.0230, Reg:12.0000) beta=8.30
Iter 19000 | Total loss: 1.0190 (MSE:0.0190, Reg:1.0000) beta=7.40
Iter 19401 | Total loss: 0.0192 (MSE:0.0192, Reg:0.0000) beta=7.04 ... Early stopping

encoder.layers.encoder_layer_11.self_attention.in_proj
   FP_OUTPUT shape torch.Size([1024, 197, 2304])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([2304, 768])
Iter     1 | Total loss: 0.0218 (MSE:0.0218, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0195 (MSE:0.0195, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0224 (MSE:0.0224, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0211 (MSE:0.0211, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0198 (MSE:0.0198, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 346781.6562 (MSE:0.0211, Reg:346781.6250) beta=20.00
Iter  6000 | Total loss: 39475.4727 (MSE:0.0261, Reg:39475.4453) beta=19.10
Iter  7000 | Total loss: 29782.5039 (MSE:0.0215, Reg:29782.4824) beta=18.20
Iter  8000 | Total loss: 24341.3184 (MSE:0.0198, Reg:24341.2988) beta=17.30
Iter  9000 | Total loss: 20578.9785 (MSE:0.0217, Reg:20578.9570) beta=16.40
Iter 10000 | Total loss: 17401.6191 (MSE:0.0202, Reg:17401.5996) beta=15.50
Iter 11000 | Total loss: 14706.8877 (MSE:0.0207, Reg:14706.8672) beta=14.60
Iter 12000 | Total loss: 12157.9277 (MSE:0.0236, Reg:12157.9043) beta=13.70
Iter 13000 | Total loss: 9566.8271 (MSE:0.0221, Reg:9566.8047) beta=12.80
Iter 14000 | Total loss: 7135.6831 (MSE:0.0234, Reg:7135.6597) beta=11.90
Iter 15000 | Total loss: 4882.5483 (MSE:0.0179, Reg:4882.5303) beta=11.00
Iter 16000 | Total loss: 3063.5481 (MSE:0.0192, Reg:3063.5288) beta=10.10
Iter 17000 | Total loss: 1629.1685 (MSE:0.0220, Reg:1629.1465) beta=9.20
Iter 18000 | Total loss: 624.7963 (MSE:0.0214, Reg:624.7749) beta=8.30
Iter 19000 | Total loss: 144.8478 (MSE:0.0228, Reg:144.8250) beta=7.40
Iter 20000 | Total loss: 8.9350 (MSE:0.0239, Reg:8.9112) beta=6.50
Iter 21000 | Total loss: 1.0199 (MSE:0.0199, Reg:1.0000) beta=5.60
Iter 21138 | Total loss: 0.0212 (MSE:0.0212, Reg:0.0000) beta=5.48 ... Early stopping

encoder.layers.encoder_layer_11.self_attention.out_proj
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 768])
Iter     1 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 86336.2344 (MSE:0.0016, Reg:86336.2344) beta=20.00
Iter  6000 | Total loss: 2871.5410 (MSE:0.0014, Reg:2871.5396) beta=19.10
Iter  7000 | Total loss: 1461.8759 (MSE:0.0016, Reg:1461.8743) beta=18.20
Iter  8000 | Total loss: 917.8616 (MSE:0.0015, Reg:917.8602) beta=17.30
Iter  9000 | Total loss: 651.4050 (MSE:0.0013, Reg:651.4037) beta=16.40
Iter 10000 | Total loss: 463.1895 (MSE:0.0016, Reg:463.1879) beta=15.50
Iter 11000 | Total loss: 321.7561 (MSE:0.0019, Reg:321.7542) beta=14.60
Iter 12000 | Total loss: 230.0011 (MSE:0.0012, Reg:229.9999) beta=13.70
Iter 13000 | Total loss: 183.0090 (MSE:0.0015, Reg:183.0074) beta=12.80
Iter 14000 | Total loss: 127.0016 (MSE:0.0016, Reg:127.0000) beta=11.90
Iter 15000 | Total loss: 83.0017 (MSE:0.0017, Reg:83.0000) beta=11.00
Iter 16000 | Total loss: 46.0017 (MSE:0.0017, Reg:46.0000) beta=10.10
Iter 17000 | Total loss: 20.9945 (MSE:0.0018, Reg:20.9927) beta=9.20
Iter 18000 | Total loss: 9.0016 (MSE:0.0016, Reg:9.0000) beta=8.30
Iter 19000 | Total loss: 1.9720 (MSE:0.0016, Reg:1.9704) beta=7.40
Iter 19586 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=6.87 ... Early stopping

encoder.layers.encoder_layer_11.mlp.linear_1
   FP_OUTPUT shape torch.Size([1024, 197, 3072])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([3072, 768])
Iter     1 | Total loss: 0.0413 (MSE:0.0413, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0346 (MSE:0.0346, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0371 (MSE:0.0371, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0412 (MSE:0.0412, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0380 (MSE:0.0380, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 409715.7188 (MSE:0.0376, Reg:409715.6875) beta=20.00
Iter  6000 | Total loss: 52380.0664 (MSE:0.0334, Reg:52380.0312) beta=19.10
Iter  7000 | Total loss: 40961.6523 (MSE:0.0414, Reg:40961.6094) beta=18.20
Iter  8000 | Total loss: 34115.4258 (MSE:0.0422, Reg:34115.3828) beta=17.30
Iter  9000 | Total loss: 28846.0859 (MSE:0.0422, Reg:28846.0430) beta=16.40
Iter 10000 | Total loss: 24047.9336 (MSE:0.0357, Reg:24047.8984) beta=15.50
Iter 11000 | Total loss: 19503.3086 (MSE:0.0387, Reg:19503.2695) beta=14.60
Iter 12000 | Total loss: 15177.4160 (MSE:0.0361, Reg:15177.3799) beta=13.70
Iter 13000 | Total loss: 11006.8730 (MSE:0.0367, Reg:11006.8359) beta=12.80
Iter 14000 | Total loss: 7239.5532 (MSE:0.0363, Reg:7239.5171) beta=11.90
Iter 15000 | Total loss: 4065.7678 (MSE:0.0446, Reg:4065.7231) beta=11.00
Iter 16000 | Total loss: 2001.9509 (MSE:0.0329, Reg:2001.9181) beta=10.10
Iter 17000 | Total loss: 794.3851 (MSE:0.0340, Reg:794.3511) beta=9.20
Iter 18000 | Total loss: 165.5881 (MSE:0.0365, Reg:165.5516) beta=8.30
Iter 19000 | Total loss: 7.4428 (MSE:0.0325, Reg:7.4103) beta=7.40
Iter 19287 | Total loss: 0.0378 (MSE:0.0378, Reg:0.0000) beta=7.14 ... Early stopping

encoder.layers.encoder_layer_11.mlp.linear_2
   FP_OUTPUT shape torch.Size([1024, 197, 768])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([768, 3072])
Iter     1 | Total loss: 0.0202 (MSE:0.0202, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0087 (MSE:0.0087, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0082 (MSE:0.0082, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0085 (MSE:0.0085, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 195470.3281 (MSE:0.0079, Reg:195470.3125) beta=20.00
Iter  6000 | Total loss: 477.5063 (MSE:0.0088, Reg:477.4974) beta=19.10
Iter  7000 | Total loss: 314.3093 (MSE:0.0078, Reg:314.3015) beta=18.20
Iter  8000 | Total loss: 222.9615 (MSE:0.0088, Reg:222.9527) beta=17.30
Iter  9000 | Total loss: 156.9998 (MSE:0.0080, Reg:156.9918) beta=16.40
Iter 10000 | Total loss: 128.0088 (MSE:0.0088, Reg:128.0000) beta=15.50
Iter 11000 | Total loss: 96.0094 (MSE:0.0094, Reg:96.0000) beta=14.60
Iter 12000 | Total loss: 73.0087 (MSE:0.0087, Reg:73.0000) beta=13.70
Iter 13000 | Total loss: 61.0079 (MSE:0.0079, Reg:61.0000) beta=12.80
Iter 14000 | Total loss: 39.0073 (MSE:0.0082, Reg:38.9991) beta=11.90
Iter 15000 | Total loss: 26.0089 (MSE:0.0089, Reg:26.0000) beta=11.00
Iter 16000 | Total loss: 16.9277 (MSE:0.0083, Reg:16.9194) beta=10.10
Iter 17000 | Total loss: 13.0096 (MSE:0.0096, Reg:13.0000) beta=9.20
Iter 18000 | Total loss: 7.7717 (MSE:0.0085, Reg:7.7632) beta=8.30
Iter 19000 | Total loss: 4.0087 (MSE:0.0087, Reg:4.0000) beta=7.40
Iter 20000 | Total loss: 2.0097 (MSE:0.0097, Reg:2.0000) beta=6.50
Iter 20119 | Total loss: 0.0088 (MSE:0.0088, Reg:0.0000) beta=6.39 ... Early stopping

heads.head
   FP_OUTPUT shape torch.Size([1024, 1000])
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 25000 torch.Size([1000, 768])
Iter     1 | Total loss: 0.0319 (MSE:0.0319, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0226 (MSE:0.0226, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0272 (MSE:0.0272, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0227 (MSE:0.0227, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0227 (MSE:0.0227, Reg:0.0000) beta=0.00
Iter  5000 | Total loss: 106676.6328 (MSE:0.0242, Reg:106676.6094) beta=20.00
Iter  6000 | Total loss: 21685.2754 (MSE:0.0211, Reg:21685.2539) beta=19.10
Iter  7000 | Total loss: 14997.8115 (MSE:0.0244, Reg:14997.7871) beta=18.20
Iter  8000 | Total loss: 10530.2627 (MSE:0.0190, Reg:10530.2441) beta=17.30
Iter  9000 | Total loss: 7123.4941 (MSE:0.0232, Reg:7123.4707) beta=16.40
Iter 10000 | Total loss: 4678.2256 (MSE:0.0227, Reg:4678.2031) beta=15.50
Iter 11000 | Total loss: 2852.0149 (MSE:0.0271, Reg:2851.9878) beta=14.60
Iter 12000 | Total loss: 1518.1693 (MSE:0.0265, Reg:1518.1428) beta=13.70
Iter 13000 | Total loss: 803.6193 (MSE:0.0196, Reg:803.5997) beta=12.80
Iter 14000 | Total loss: 302.1566 (MSE:0.0249, Reg:302.1317) beta=11.90
Iter 15000 | Total loss: 76.3302 (MSE:0.0233, Reg:76.3069) beta=11.00
Iter 16000 | Total loss: 16.9937 (MSE:0.0179, Reg:16.9757) beta=10.10
Iter 16989 | Total loss: 0.0254 (MSE:0.0254, Reg:0.0000) beta=9.21 ... Early stopping

,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    Quantized model Evaluation accuracy on 50000 images, 80.536%
Total time: 6548.06 sec
