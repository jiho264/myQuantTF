
- main_args params:
    - arch: ViT_B_16
    - batch_size: 128
    - num_samples: 1024

- weight params:
    - scheme: AbsMaxQuantizer
    - bit_width: 8
    - per_channel: True
    - AdaRound: PerEncoder

- activation params:
    - scheme: MovAvgAbsMaxQuantizer
    - bit_width: 8
    - per_channel: False
    - momentum: 0.95
    - batches: 16

- softmax params:
    - bit_width: 16

- layer_norm params:
    - bit_width: 8

- gelu params:
    - bit_width: 8

- Identity addition : INT16 (The input of each LayerNorm)

- Activation of Softmax(Q@K/d_K) (attn_map) : UINT8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 8
IntSoftMax 16
Int Activation Unsigned 8 for softmax_act
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
Int Activation 8
    Initiated the V
IntGELU bit: 8
Int Activation 8
Int Activation 8
    Initiated the V
Int Activation 16 for idAdd for identity add
Int Activation 8
    Initiated the V
Activation calibration is done.

[1/14] conv_proj
 INPUT_FP : torch.Size([1024, 3, 224, 224])
 OUTPUT_FP : torch.Size([1024, 768, 14, 14])
    V   : , torch.Size([768, 3, 16, 16])
    s_a : act_quant, torch.Size([])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 5000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
) 5000
Iter     1 | Total loss: 0.0149 (MSE:0.0149, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0142 (MSE:0.0142, Reg:0.0000) beta=20.00

Iter  1000 | Total loss: 0.0142 (MSE:0.0142, Reg:0.0000) beta=20.00
    Early stopped

    Set the rounding value

[2/14] encoder.layers.0
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    s_a : ln_1_act, torch.Size([])
    V   : self_attention.in_proj, torch.Size([2304, 768])
    s_a : self_attention.in_proj.act_quant, torch.Size([])
    s_a : self_attention.qk_act, torch.Size([])
    s_a : self_attention.softmax_act, torch.Size([])
    s_a : self_attention.attnout_act, torch.Size([])
    V   : self_attention.out_proj, torch.Size([768, 768])
    s_a : self_attention.out_proj.act_quant, torch.Size([])
    s_a : idAdd_1, torch.Size([])
    s_a : ln_2_act, torch.Size([])
    V   : mlp.linear_1, torch.Size([3072, 768])
    s_a : mlp.linear_1.act_quant, torch.Size([])
    s_a : mlp.gelu_act, torch.Size([])
    V   : mlp.linear_2, torch.Size([768, 3072])
    s_a : mlp.linear_2.act_quant, torch.Size([])
    s_a : idAdd_2, torch.Size([])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 5000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
) 5000
Iter     1 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=20.00

Iter  1000 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=20.00
    Early stopped

    Set the rounding value
    Set the rounding value
    Set the rounding value
    Set the rounding value

[3/14] encoder.layers.1
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    s_a : ln_1_act, torch.Size([])
    V   : self_attention.in_proj, torch.Size([2304, 768])
    s_a : self_attention.in_proj.act_quant, torch.Size([])
    s_a : self_attention.qk_act, torch.Size([])
    s_a : self_attention.softmax_act, torch.Size([])
    s_a : self_attention.attnout_act, torch.Size([])
    V   : self_attention.out_proj, torch.Size([768, 768])
    s_a : self_attention.out_proj.act_quant, torch.Size([])
    s_a : idAdd_1, torch.Size([])
    s_a : ln_2_act, torch.Size([])
    V   : mlp.linear_1, torch.Size([3072, 768])
    s_a : mlp.linear_1.act_quant, torch.Size([])
    s_a : mlp.gelu_act, torch.Size([])
    V   : mlp.linear_2, torch.Size([768, 3072])
    s_a : mlp.linear_2.act_quant, torch.Size([])
    s_a : idAdd_2, torch.Size([])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 5000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
) 5000
Iter     1 | Total loss: 0.0092 (MSE:0.0092, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0093 (MSE:0.0093, Reg:0.0000) beta=20.00

Iter  1000 | Total loss: 0.0093 (MSE:0.0093, Reg:0.0000) beta=20.00
    Early stopped

    Set the rounding value
    Set the rounding value
    Set the rounding value
    Set the rounding value

[4/14] encoder.layers.2
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    s_a : ln_1_act, torch.Size([])
    V   : self_attention.in_proj, torch.Size([2304, 768])
    s_a : self_attention.in_proj.act_quant, torch.Size([])
    s_a : self_attention.qk_act, torch.Size([])
    s_a : self_attention.softmax_act, torch.Size([])
    s_a : self_attention.attnout_act, torch.Size([])
    V   : self_attention.out_proj, torch.Size([768, 768])
    s_a : self_attention.out_proj.act_quant, torch.Size([])
    s_a : idAdd_1, torch.Size([])
    s_a : ln_2_act, torch.Size([])
    V   : mlp.linear_1, torch.Size([3072, 768])
    s_a : mlp.linear_1.act_quant, torch.Size([])
    s_a : mlp.gelu_act, torch.Size([])
    V   : mlp.linear_2, torch.Size([768, 3072])
    s_a : mlp.linear_2.act_quant, torch.Size([])
    s_a : idAdd_2, torch.Size([])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 5000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
) 5000
Iter     1 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=20.00

Iter  1000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=20.00
    Early stopped

    Set the rounding value
    Set the rounding value
    Set the rounding value
    Set the rounding value

[5/14] encoder.layers.3
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    s_a : ln_1_act, torch.Size([])
    V   : self_attention.in_proj, torch.Size([2304, 768])
    s_a : self_attention.in_proj.act_quant, torch.Size([])
    s_a : self_attention.qk_act, torch.Size([])
    s_a : self_attention.softmax_act, torch.Size([])
    s_a : self_attention.attnout_act, torch.Size([])
    V   : self_attention.out_proj, torch.Size([768, 768])
    s_a : self_attention.out_proj.act_quant, torch.Size([])
    s_a : idAdd_1, torch.Size([])
    s_a : ln_2_act, torch.Size([])
    V   : mlp.linear_1, torch.Size([3072, 768])
    s_a : mlp.linear_1.act_quant, torch.Size([])
    s_a : mlp.gelu_act, torch.Size([])
    V   : mlp.linear_2, torch.Size([768, 3072])
    s_a : mlp.linear_2.act_quant, torch.Size([])
    s_a : idAdd_2, torch.Size([])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 5000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
) 5000
Iter     1 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0082 (MSE:0.0082, Reg:0.0000) beta=20.00

Iter  1000 | Total loss: 0.0082 (MSE:0.0082, Reg:0.0000) beta=20.00
    Early stopped

    Set the rounding value
    Set the rounding value
    Set the rounding value
    Set the rounding value

[6/14] encoder.layers.4
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    s_a : ln_1_act, torch.Size([])
    V   : self_attention.in_proj, torch.Size([2304, 768])
    s_a : self_attention.in_proj.act_quant, torch.Size([])
    s_a : self_attention.qk_act, torch.Size([])
    s_a : self_attention.softmax_act, torch.Size([])
    s_a : self_attention.attnout_act, torch.Size([])
    V   : self_attention.out_proj, torch.Size([768, 768])
    s_a : self_attention.out_proj.act_quant, torch.Size([])
    s_a : idAdd_1, torch.Size([])
    s_a : ln_2_act, torch.Size([])
    V   : mlp.linear_1, torch.Size([3072, 768])
    s_a : mlp.linear_1.act_quant, torch.Size([])
    s_a : mlp.gelu_act, torch.Size([])
    V   : mlp.linear_2, torch.Size([768, 3072])
    s_a : mlp.linear_2.act_quant, torch.Size([])
    s_a : idAdd_2, torch.Size([])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 5000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
) 5000
Iter     1 | Total loss: 0.0079 (MSE:0.0079, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0077 (MSE:0.0077, Reg:0.0000) beta=20.00

Iter  1000 | Total loss: 0.0077 (MSE:0.0077, Reg:0.0000) beta=20.00
    Early stopped

    Set the rounding value
    Set the rounding value
    Set the rounding value
    Set the rounding value

[7/14] encoder.layers.5
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    s_a : ln_1_act, torch.Size([])
    V   : self_attention.in_proj, torch.Size([2304, 768])
    s_a : self_attention.in_proj.act_quant, torch.Size([])
    s_a : self_attention.qk_act, torch.Size([])
    s_a : self_attention.softmax_act, torch.Size([])
    s_a : self_attention.attnout_act, torch.Size([])
    V   : self_attention.out_proj, torch.Size([768, 768])
    s_a : self_attention.out_proj.act_quant, torch.Size([])
    s_a : idAdd_1, torch.Size([])
    s_a : ln_2_act, torch.Size([])
    V   : mlp.linear_1, torch.Size([3072, 768])
    s_a : mlp.linear_1.act_quant, torch.Size([])
    s_a : mlp.gelu_act, torch.Size([])
    V   : mlp.linear_2, torch.Size([768, 3072])
    s_a : mlp.linear_2.act_quant, torch.Size([])
    s_a : idAdd_2, torch.Size([])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 5000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
) 5000
Iter     1 | Total loss: 0.0361 (MSE:0.0361, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0356 (MSE:0.0356, Reg:0.0000) beta=20.00

Iter  1000 | Total loss: 0.0356 (MSE:0.0356, Reg:0.0000) beta=20.00
    Early stopped

    Set the rounding value
    Set the rounding value
    Set the rounding value
    Set the rounding value

[8/14] encoder.layers.6
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    s_a : ln_1_act, torch.Size([])
    V   : self_attention.in_proj, torch.Size([2304, 768])
    s_a : self_attention.in_proj.act_quant, torch.Size([])
    s_a : self_attention.qk_act, torch.Size([])
    s_a : self_attention.softmax_act, torch.Size([])
    s_a : self_attention.attnout_act, torch.Size([])
    V   : self_attention.out_proj, torch.Size([768, 768])
    s_a : self_attention.out_proj.act_quant, torch.Size([])
    s_a : idAdd_1, torch.Size([])
    s_a : ln_2_act, torch.Size([])
    V   : mlp.linear_1, torch.Size([3072, 768])
    s_a : mlp.linear_1.act_quant, torch.Size([])
    s_a : mlp.gelu_act, torch.Size([])
    V   : mlp.linear_2, torch.Size([768, 3072])
    s_a : mlp.linear_2.act_quant, torch.Size([])
    s_a : idAdd_2, torch.Size([])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 5000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
) 5000
Iter     1 | Total loss: 0.0106 (MSE:0.0106, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0107 (MSE:0.0107, Reg:0.0000) beta=20.00

Iter  1000 | Total loss: 0.0107 (MSE:0.0107, Reg:0.0000) beta=20.00
    Early stopped

    Set the rounding value
    Set the rounding value
    Set the rounding value
    Set the rounding value

[9/14] encoder.layers.7
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    s_a : ln_1_act, torch.Size([])
    V   : self_attention.in_proj, torch.Size([2304, 768])
    s_a : self_attention.in_proj.act_quant, torch.Size([])
    s_a : self_attention.qk_act, torch.Size([])
    s_a : self_attention.softmax_act, torch.Size([])
    s_a : self_attention.attnout_act, torch.Size([])
    V   : self_attention.out_proj, torch.Size([768, 768])
    s_a : self_attention.out_proj.act_quant, torch.Size([])
    s_a : idAdd_1, torch.Size([])
    s_a : ln_2_act, torch.Size([])
    V   : mlp.linear_1, torch.Size([3072, 768])
    s_a : mlp.linear_1.act_quant, torch.Size([])
    s_a : mlp.gelu_act, torch.Size([])
    V   : mlp.linear_2, torch.Size([768, 3072])
    s_a : mlp.linear_2.act_quant, torch.Size([])
    s_a : idAdd_2, torch.Size([])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 5000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
) 5000
Iter     1 | Total loss: 0.0125 (MSE:0.0125, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0125 (MSE:0.0125, Reg:0.0000) beta=20.00

Iter  1000 | Total loss: 0.0125 (MSE:0.0125, Reg:0.0000) beta=20.00
    Early stopped

    Set the rounding value
    Set the rounding value
    Set the rounding value
    Set the rounding value

[10/14] encoder.layers.8
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    s_a : ln_1_act, torch.Size([])
    V   : self_attention.in_proj, torch.Size([2304, 768])
    s_a : self_attention.in_proj.act_quant, torch.Size([])
    s_a : self_attention.qk_act, torch.Size([])
    s_a : self_attention.softmax_act, torch.Size([])
    s_a : self_attention.attnout_act, torch.Size([])
    V   : self_attention.out_proj, torch.Size([768, 768])
    s_a : self_attention.out_proj.act_quant, torch.Size([])
    s_a : idAdd_1, torch.Size([])
    s_a : ln_2_act, torch.Size([])
    V   : mlp.linear_1, torch.Size([3072, 768])
    s_a : mlp.linear_1.act_quant, torch.Size([])
    s_a : mlp.gelu_act, torch.Size([])
    V   : mlp.linear_2, torch.Size([768, 3072])
    s_a : mlp.linear_2.act_quant, torch.Size([])
    s_a : idAdd_2, torch.Size([])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 5000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
) 5000
Iter     1 | Total loss: 0.0165 (MSE:0.0165, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0166 (MSE:0.0166, Reg:0.0000) beta=20.00

Iter  1000 | Total loss: 0.0166 (MSE:0.0166, Reg:0.0000) beta=20.00
    Early stopped

    Set the rounding value
    Set the rounding value
    Set the rounding value
    Set the rounding value

[11/14] encoder.layers.9
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    s_a : ln_1_act, torch.Size([])
    V   : self_attention.in_proj, torch.Size([2304, 768])
    s_a : self_attention.in_proj.act_quant, torch.Size([])
    s_a : self_attention.qk_act, torch.Size([])
    s_a : self_attention.softmax_act, torch.Size([])
    s_a : self_attention.attnout_act, torch.Size([])
    V   : self_attention.out_proj, torch.Size([768, 768])
    s_a : self_attention.out_proj.act_quant, torch.Size([])
    s_a : idAdd_1, torch.Size([])
    s_a : ln_2_act, torch.Size([])
    V   : mlp.linear_1, torch.Size([3072, 768])
    s_a : mlp.linear_1.act_quant, torch.Size([])
    s_a : mlp.gelu_act, torch.Size([])
    V   : mlp.linear_2, torch.Size([768, 3072])
    s_a : mlp.linear_2.act_quant, torch.Size([])
    s_a : idAdd_2, torch.Size([])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 5000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
) 5000
Iter     1 | Total loss: 0.0224 (MSE:0.0224, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0227 (MSE:0.0227, Reg:0.0000) beta=20.00

Iter  1000 | Total loss: 0.0227 (MSE:0.0227, Reg:0.0000) beta=20.00
    Early stopped

    Set the rounding value
    Set the rounding value
    Set the rounding value
    Set the rounding value

[12/14] encoder.layers.10
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    s_a : ln_1_act, torch.Size([])
    V   : self_attention.in_proj, torch.Size([2304, 768])
    s_a : self_attention.in_proj.act_quant, torch.Size([])
    s_a : self_attention.qk_act, torch.Size([])
    s_a : self_attention.softmax_act, torch.Size([])
    s_a : self_attention.attnout_act, torch.Size([])
    V   : self_attention.out_proj, torch.Size([768, 768])
    s_a : self_attention.out_proj.act_quant, torch.Size([])
    s_a : idAdd_1, torch.Size([])
    s_a : ln_2_act, torch.Size([])
    V   : mlp.linear_1, torch.Size([3072, 768])
    s_a : mlp.linear_1.act_quant, torch.Size([])
    s_a : mlp.gelu_act, torch.Size([])
    V   : mlp.linear_2, torch.Size([768, 3072])
    s_a : mlp.linear_2.act_quant, torch.Size([])
    s_a : idAdd_2, torch.Size([])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 5000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
) 5000
Iter     1 | Total loss: 0.0558 (MSE:0.0558, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0667 (MSE:0.0667, Reg:0.0000) beta=20.00

Iter  1000 | Total loss: 0.0667 (MSE:0.0667, Reg:0.0000) beta=20.00
    Early stopped

    Set the rounding value
    Set the rounding value
    Set the rounding value
    Set the rounding value

[13/14] encoder.layers.11
 INPUT_FP : torch.Size([1024, 197, 768])
 OUTPUT_FP : torch.Size([1024, 197, 768])
    s_a : ln_1_act, torch.Size([])
    V   : self_attention.in_proj, torch.Size([2304, 768])
    s_a : self_attention.in_proj.act_quant, torch.Size([])
    s_a : self_attention.qk_act, torch.Size([])
    s_a : self_attention.softmax_act, torch.Size([])
    s_a : self_attention.attnout_act, torch.Size([])
    V   : self_attention.out_proj, torch.Size([768, 768])
    s_a : self_attention.out_proj.act_quant, torch.Size([])
    s_a : idAdd_1, torch.Size([])
    s_a : ln_2_act, torch.Size([])
    V   : mlp.linear_1, torch.Size([3072, 768])
    s_a : mlp.linear_1.act_quant, torch.Size([])
    s_a : mlp.gelu_act, torch.Size([])
    V   : mlp.linear_2, torch.Size([768, 3072])
    s_a : mlp.linear_2.act_quant, torch.Size([])
    s_a : idAdd_2, torch.Size([])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 5000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
) 5000
Iter     1 | Total loss: 0.0334 (MSE:0.0334, Reg:0.0000) beta=0.00

Iter  1000 | Total loss: 0.0323 (MSE:0.0323, Reg:0.0000) beta=20.00

Iter  1000 | Total loss: 0.0323 (MSE:0.0323, Reg:0.0000) beta=20.00
    Early stopped

    Set the rounding value
    Set the rounding value
    Set the rounding value
    Set the rounding value

[14/14] heads.0
 INPUT_FP : torch.Size([1024, 768])
 OUTPUT_FP : torch.Size([1024, 1000])
    V   : , torch.Size([1000, 768])
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
) 5000
Iter     1 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00

    main()
  File "/home/lee/Desktop/myQuantTF/main_TFquant.py", line 254, in main
    run_AdaRound(model, train_loader, scheme)
  File "/home/lee/Desktop/myQuantTF/main_TFquant.py", line 183, in run_AdaRound
    _adaround_for_a_module(model, module, cali_data, batch_size, lr, n_iter)
  File "/home/lee/Desktop/myQuantTF/main_TFquant.py", line 85, in _adaround_for_a_module
    a_hat, _ = module.forward(batch_input_fp, S_X_INPUT)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lee/Desktop/myQuantTF/utils/quant_modules.py", line 321, in forward
    assert (
AssertionError: -133.0 76.0
